import pandas as pd
import numpy as np
from scipy. stats import truncnorm
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import Optional, Tuple, List, Dict
import time
import os
import re

# ----------- 0.  CONSTANTS AND CONFIGURATION -----------

# æ€§èƒ½ä¼˜åŒ–ï¼šæœ€å¤§AOæå‰å¤©æ•°çš„é»˜è®¤å€¼ï¼ˆä»é…ç½®ä¸­åŠ¨æ€è·å–ï¼Œæ­¤ä¸ºåå¤‡å€¼ï¼‰
DEFAULT_MAX_ADVANCE_DAYS = 10

# å¹¶è¡Œè®¡ç®—çš„é»˜è®¤å¼€å…³ä¸å¹¶å‘åº¦ï¼ˆè°¨æ…ä½¿ç”¨ï¼Œé»˜è®¤å…³é—­ä»¥ç¡®ä¿ä¸æ—§ç‰ˆè¾“å‡ºä¸€è‡´ï¼‰
# - use_parallel_ao_consumeï¼šæ˜¯å¦å¯ç”¨ AO æ¶ˆè€—çš„å¹¶è¡Œåˆ†ç»„è®¡ç®—ï¼ˆæŒ‰ç‰©æ–™-åœ°ç‚¹æ‹†åˆ†ï¼Œè¿›ç¨‹æ± ï¼‰
# - use_parallel_file_loadï¼šæ˜¯å¦å¯ç”¨å†å²è®¢å•æ–‡ä»¶çš„å¹¶è¡Œè¯»å–ï¼ˆçº¿ç¨‹æ± ï¼Œé€‚åˆI/Oï¼‰
# - parallel_max_workersï¼šå¹¶å‘å·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨ï¼šCPUæ ¸å¿ƒæ•°ï¼‰
DEFAULT_USE_PARALLEL_AO_CONSUME = True
DEFAULT_USE_PARALLEL_FILE_LOAD = True
DEFAULT_PARALLEL_MAX_WORKERS: Optional[int] = None
DEFAULT_ERROR_LOG_PATH: Optional[str] = None  # å¼‚å¸¸æ—¥å¿—è¾“å‡ºè·¯å¾„ï¼ˆtxtï¼‰ï¼Œä¸ºç©ºåˆ™ä¸å†™ç›˜
DEFAULT_USE_PARALLEL_NORMAL_CONSUME: Optional[bool] = None  # Normalå¹¶è¡Œå¼€å…³ï¼ˆNoneè¡¨ç¤ºç»§æ‰¿AOå¹¶è¡Œå¼€å…³ï¼‰

# ç®€æ˜“å¼‚å¸¸æ—¥å¿—è®°å½•å·¥å…·ï¼ˆä¸­æ–‡ä¿¡æ¯ï¼‰
def _append_error_log(message: str):
    """
    å°†å¼‚å¸¸ä¿¡æ¯è¿½åŠ å†™å…¥åˆ°txtæ–‡ä»¶ã€‚
    ä½¿ç”¨è¯´æ˜ï¼ˆä¸­æ–‡ï¼‰ï¼š
    - é»˜è®¤ä¸å†™ç›˜ï¼Œéœ€åœ¨é›†æˆå…¥å£é‡Œè®¾ç½® `DEFAULT_ERROR_LOG_PATH` ä¸ºæŸä¸ªæ–‡ä»¶è·¯å¾„ã€‚
    - æ—¥å¿—å†…å®¹ä¸ºç®€å•æ–‡æœ¬ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿå®šä½é—®é¢˜ï¼ˆå“ªä¸ªæ¨¡å—ã€å“ªä¸ªML/æ–‡ä»¶ã€å¼‚å¸¸ç±»å‹ä¸ä¿¡æ¯ï¼‰ã€‚
    """
    try:
        path = globals().get('DEFAULT_ERROR_LOG_PATH', None)
        if not path:
            return
        # åˆ›å»ºç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'a', encoding='utf-8') as f:
            f.write(str(message).rstrip('\n') + '\n')
    except Exception:
        # æ—¥å¿—å†™å…¥å¤±è´¥æ—¶é™é»˜ï¼Œé¿å…å½±å“ä¸»æµç¨‹
        pass

# ----------- 0. STRING NORMALIZATION FUNCTIONS -----------

def _normalize_location(location_str) -> str:
    """
    è§„èŒƒåŒ–åœ°ç‚¹ï¼ˆlocationï¼‰å­—ç¬¦ä¸²ï¼š
    - å°†æ•°å€¼æˆ–å­—ç¬¦ä¸²å½¢å¼çš„åœ°ç‚¹ç¼–å·ç»Ÿä¸€ä¸º4ä½ã€å·¦ä¾§è¡¥é›¶çš„å­—ç¬¦ä¸²ï¼ˆå¦‚"7"â†’"0007"ï¼‰
    - å¯¹ None/NaN è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œé¿å…åç»­åˆå¹¶é”®å‡ºç°éé¢„æœŸç±»å‹
    é‡è¦ï¼šæœ¬å‡½æ•°ç”¨äºä¿éšœæ‰€æœ‰ä¸åœ°ç‚¹ç›¸å…³çš„é”®åœ¨æ•°æ®å¤„ç†ä¸­çš„ä¸€è‡´æ€§ï¼Œé˜²æ­¢å› ç±»å‹æˆ–ä½æ•°ä¸åŒå¯¼è‡´çš„é‡å¤é”®æˆ–åŒ¹é…å¤±è´¥ã€‚
    """
    # Handle None and pandas NA
    if location_str is None or pd.isna(location_str):
        return ""
    try:
        return str(int(location_str)).zfill(4)
    except (ValueError, TypeError):
        return str(location_str).zfill(4)

def _normalize_material(material_str) -> str:
    """
    è§„èŒƒåŒ–ç‰©æ–™ï¼ˆmaterialï¼‰å­—ç¬¦ä¸²ï¼š
    - å°†è¾“å…¥ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²ï¼›å¯¹ None/NaN è¿”å›ç©ºå­—ç¬¦ä¸²
    ç”¨é€”ï¼šç¡®ä¿åˆå¹¶ä¸åˆ†ç»„æ—¶çš„é”®ä¸€è‡´ï¼Œé¿å…ç±»å‹å·®å¼‚é€ æˆçš„å¯¹é½é—®é¢˜ã€‚
    """
    # Handle None and pandas NA
    if material_str is None or pd.isna(material_str):
        return ""
    return str(material_str)

def _normalize_identifiers(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç»Ÿä¸€è§„èŒƒåŒ–æ ‡è¯†ç¬¦åˆ—ï¼ˆmaterial/location/sending/receiving/sourcing/dps_locationï¼‰ï¼š
    - å…¨éƒ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œç¼ºå¤±å€¼å¡«å……ä¸ºç©ºå­—ç¬¦ä¸²
    - `location` ä¸ `dps_location` ä½¿ç”¨å‘é‡åŒ– `zfill(4)` ä¿è¯4ä½ç¼–å·
    ç›®çš„ï¼šåœ¨æ•´ä¸ªæ¨¡å—ä¸­ä¿æŒé”®ä¸€è‡´æ€§ï¼Œå‡å°‘åˆå¹¶æ—¶çš„é‡å¤é”®ä¸é”™é…ã€‚
    ç‰¹æ®Šå¤„ç†ï¼šé‡‡ç”¨å‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œï¼Œé¿å…é€è¡Œ `apply` å¸¦æ¥çš„æ€§èƒ½æŸè€—ã€‚
    """
    if df.empty:
        return df
    
    # Define identifier columns that need string conversion
    identifier_cols = ['material', 'location', 'sending', 'receiving', 'sourcing', 'dps_location']
    
    df = df.copy()
    for col in identifier_cols:
        if col in df.columns:
            # Convert to string and handle NaN values
            df[col] = df[col].astype('string')
            # Apply specific normalization for location (vectorized)
            if col in ['location', 'dps_location']:
                # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œæ›¿ä»£apply
                df[col] = df[col].str.zfill(4)
            # Apply specific normalization for material
            elif col == 'material':
                df[col] = df[col].fillna("")
            # For other identifier columns, ensure they are properly formatted strings
            else:
                df[col] = df[col].fillna("")
    
    return df

# ----------- 1. LOAD CONFIG (Enhanced) -----------
def load_config(filename, sheet_mapping=None):
    """
    ä» Excel æ–‡ä»¶åŠ è½½å„é…ç½®é¡µä¸º DataFrame å­—å…¸ï¼š
    - å¯¹å·²å­˜åœ¨çš„ sheet è¿›è¡Œè§£æå¹¶è°ƒç”¨ `_normalize_identifiers` ä¿è¯é”®è§„èŒƒ
    - å¯¹ä¸å­˜åœ¨çš„ sheet ä½¿ç”¨é»˜è®¤ç©ºè¡¨æˆ– None å¡«å……
    å¼‚å¸¸å¤„ç†ï¼šè¯»å–å¤±è´¥æ—¶æŠ›å‡º RuntimeErrorï¼Œä¾¿äºä¸Šå±‚æ•è·å¹¶æç¤ºã€‚
    """
    if sheet_mapping is None:
        sheet_mapping = {
            'DemandForecast': ('demand_forecast', None),
            'ForecastError': ('forecast_error', None),
            'OrderCalendar': ('order_calendar', None),
            'AOConfig': ('ao_config', pd.DataFrame()),
            'SupplyChoiceConfig': ('supply_choice', pd.DataFrame()),
            'InitialInventory': ('initial_inventory', None),
            'DPSConfig': ('dps_config', pd.DataFrame()),
            'ProductionPlan': ('production_plan', pd.DataFrame()),
            'DeliveryPlan': ('delivery_plan', pd.DataFrame()),
        }

    try:
        xl = pd.ExcelFile(filename)
        loaded_sheets = {}
        for sheet_name, (key, default) in sheet_mapping.items():
            if sheet_name in xl.sheet_names:
                df = xl.parse(sheet_name)
                # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
                loaded_sheets[key] = _normalize_identifiers(df)
            else:
                loaded_sheets[key] = default
        return loaded_sheets
    except Exception as e:
        raise RuntimeError(f"Failed to load config from {filename}: {e}")

# ----------- 2.  DPS SPLIT -----------
def apply_dps(df, dps_cfg):
    """
    æŒ‰ DPS é…ç½®è¿›è¡Œåœ°ç‚¹æ‹†åˆ†ï¼š
    - è¾“å…¥ä¸ºå‘¨åº¦é¢„æµ‹ `df` ä¸ `dps_cfg`ï¼ˆå« `dps_location` ä¸ `dps_percent`ï¼‰
    - é€»è¾‘ï¼šå…ˆåœ¨ MaterialLocationWeekï¼ˆç‰©æ–™-åœ°ç‚¹-å‘¨ï¼‰ç²’åº¦èšåˆï¼Œå†æŒ‰ç™¾åˆ†æ¯”åˆ†å‰²ä¸ºâ€œä¿ç•™é‡â€å’Œâ€œæ‹†åˆ†é‡â€ï¼Œæ‹†åˆ†é‡çš„åœ°ç‚¹æ”¹ä¸º `dps_location`
    - è¾“å‡ºé‡æ–°åœ¨ MaterialLocationWeekï¼ˆç‰©æ–™-åœ°ç‚¹-å‘¨ï¼‰ç²’åº¦æ±‡æ€»ï¼Œæ•°é‡è½¬ä¸ºæ•´æ•°
    ç‰¹æ®Šå¤„ç†ï¼š
    - ç¼ºå¤± `dps_percent` è§†ä¸º 0ï¼Œä¸æ‹†åˆ†
    - ä½¿ç”¨å‘é‡åŒ–è¿ç®—ä¸åˆå¹¶ï¼Œé¿å…é€è¡Œè¿­ä»£ï¼Œæé«˜æ€§èƒ½
    - è¿”å›å‰ç»Ÿä¸€è§„èŒƒåŒ–æ ‡è¯†ç¬¦ï¼Œå‡å°‘åç»­é”®åŒ¹é…é—®é¢˜
    """
    if dps_cfg.empty:
        return df.copy()
    t0 = time.perf_counter()
    df_g = df.groupby(['material','location','week'], as_index=False)['quantity'].sum()
    cols = ['material','location','dps_location','dps_percent']
    m = df_g.merge(dps_cfg[cols], on=['material','location'], how='left')
    m['dps_percent'] = m['dps_percent'].fillna(0.0)
    m['split_qty'] = np.round(m['quantity'] * m['dps_percent']).astype(int)
    m['remain_qty'] = (m['quantity'] - m['split_qty']).astype(int)
    remain = m[['material','location','week','remain_qty']].rename(columns={'remain_qty':'quantity'})
    split = m[['material','dps_location','week','split_qty']].rename(columns={'dps_location':'location','split_qty':'quantity'})
    out = pd.concat([remain, split], ignore_index=True)
    out = out.groupby(['material','location','week'], as_index=False)['quantity'].sum()
    out['quantity'] = out['quantity'].astype(int)
    print(f"[M1] DPSæ‹†åˆ†å®Œæˆï¼Œæ¡ç›®: {len(out)}ï¼Œè€—æ—¶: {time.perf_counter()-t0:.3f}s")
    return _normalize_identifiers(out)

# ----------- 3. SUPPLY CHOICE -----------
def apply_supply_choice(df, supply_cfg):
    """
    åº”ç”¨ä¾›åº”é€‰æ‹©ï¼ˆSupply Choiceï¼‰å¯¹å‘¨åº¦é¢„æµ‹è¿›è¡Œæ•°é‡è°ƒæ•´ï¼š
    - åœ¨ MaterialLocationWeekï¼ˆç‰©æ–™-åœ°ç‚¹-å‘¨ï¼‰ç²’åº¦åˆå¹¶ `adjust_quantity` å¹¶è¿›è¡Œå‘é‡åŒ–åŠ æ€»
    - ç¼ºå¤±è°ƒæ•´é‡æŒ‰ 0 å¤„ç†
    ç›®çš„ï¼šåœ¨å‘¨åº¦é˜¶æ®µå®Œæˆæ‰€æœ‰æ•°é‡ä¿®æ­£ï¼Œç¡®ä¿åç»­æ—¥åº¦æ‹†åˆ†ä¸è®¢å•ç”Ÿæˆçš„åŸºçº¿æ­£ç¡®ã€‚
    """
    if supply_cfg.empty:
        return df.copy()
    t0 = time.perf_counter()
    df_g = df.groupby(['material','location','week'], as_index=False)['quantity'].sum()
    sup_g = supply_cfg.groupby(['material','location','week'], as_index=False)['adjust_quantity'].sum()
    m = df_g.merge(sup_g, on=['material','location','week'], how='left')
    m['quantity'] = (m['quantity'] + m['adjust_quantity'].fillna(0)).astype(int)
    out = m[['material','location','week','quantity']]
    print(f"[M1] SupplyChoiceè°ƒæ•´å®Œæˆï¼Œæ¡ç›®: {len(out)}ï¼Œè€—æ—¶: {time.perf_counter()-t0:.3f}s")
    return _normalize_identifiers(out)

# ----------- 4. SPLIT WEEKLY FORECAST TO DAILY (INTEGER, NO ERROR) -----------
def expand_forecast_to_days_integer_split(demand_weekly, start_date, num_weeks, simulation_end_date=None):
    """
    å°†å‘¨åº¦é¢„æµ‹å‡åŒ€æ‹†åˆ†ä¸º7å¤©çš„æ—¥åº¦é¢„æµ‹ï¼ˆæ•´æ•°åˆ†é…ï¼‰ï¼š
    - æ¯å‘¨æ•°é‡æŒ‰ `base_qty = quantity // 7` åˆ†é…ï¼Œä½™æ•° `remainder = quantity % 7` çš„å‰ `remainder` å¤©å„åŠ  1
    - ä»…ç”Ÿæˆè‡³ `simulation_end_date`ï¼ˆå¦‚æä¾›ï¼‰
    - è¾“å‡ºä¿ç•™ `original_quantity` ä¾¿äºè¿½æº¯æ‹†åˆ†å‰çš„æ•°é‡
    æ€§èƒ½ä¼˜åŒ–ï¼šä»…è¿›è¡Œ 7 æ¬¡å¤åˆ¶å¹¶å‘é‡åŒ–è®¡ç®—æ¯æ—¥æ•°é‡ï¼Œé¿å…å¯¹æ¯æ¡è®°å½•é€æ—¥å¾ªç¯ã€‚
    """
    if demand_weekly.empty:
        return pd.DataFrame(columns=['date', 'material', 'location', 'week', 'demand_type', 'quantity', 'original_quantity'])
    
    # âœ… å‘é‡åŒ–è®¡ç®—
    start_date = pd.to_datetime(start_date)
    demand_weekly = demand_weekly.copy()
    
    # âœ… é¢„è®¡ç®—æ¯å‘¨çš„èµ·å§‹æ—¥æœŸ
    demand_weekly['week_start'] = start_date + pd.to_timedelta((demand_weekly['week'] - 1) * 7, unit='D')
    
    # âœ… è®¡ç®—æ¯æ—¥åŸºç¡€æ•°é‡å’Œä½™æ•°
    demand_weekly['base_qty'] = (demand_weekly['quantity'] // 7).astype(int)
    demand_weekly['remainder'] = (demand_weekly['quantity'] % 7).astype(int)
    
    # âœ… ç”Ÿæˆ7å¤©çš„æ•°æ®ï¼ˆåªå¾ªç¯7æ¬¡ï¼Œè€Œä¸æ˜¯N*7æ¬¡ï¼‰
    t0 = time.perf_counter()
    days = []
    for day_offset in range(7):
        day_df = demand_weekly.copy()
        day_df['date'] = day_df['week_start'] + pd.Timedelta(days=day_offset)
        # å‰remainderå¤©å¤šåˆ†é…1ä¸ªå•ä½
        day_df['quantity'] = day_df['base_qty'] + (day_offset < day_df['remainder']).astype(int)
        days.append(day_df[['date', 'material', 'location', 'week', 'quantity']])
    
    result_df = pd.concat(days, ignore_index=True)
    
    # è¿‡æ»¤ç»“æŸæ—¥æœŸ
    if simulation_end_date is not None:
        result_df = result_df[result_df['date'] <= pd.to_datetime(simulation_end_date)]
    
    result_df['demand_type'] = 'normal'
    result_df['original_quantity'] = result_df['quantity']
    result_df['quantity'] = result_df['quantity'].astype(int)
    print(f"[M1] å‘¨åº¦â†’æ—¥åº¦æ‹†åˆ†å®Œæˆï¼Œç”Ÿæˆå¤©æ•°: {len(result_df)}ï¼Œè€—æ—¶: {time.perf_counter()-t0:.3f}s")
    # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
    return _normalize_identifiers(result_df)

# ----------- 5. DAILY ORDER GENERATION -----------
def generate_daily_orders(sim_date, original_forecast, current_forecast, ao_config, order_calendar, forecast_error):
    """
    ç”Ÿæˆå•æ—¥è®¢å•ï¼ˆå« AO ä¸ Normalï¼‰ï¼Œå¹¶æ¶ˆè€—é¢„æµ‹ï¼š
        - ä»…åœ¨è®¢å•æ—¥ç”Ÿæˆï¼›éè®¢å•æ—¥ç›´æ¥è¿”å›ç©ºè®¢å•ä¸åŸé¢„æµ‹
        - åœ¨ç‰©æ–™-åœ°ç‚¹ï¼ˆMLï¼‰ç²’åº¦è®¡ç®— 7 å¤©å¹³å‡éœ€æ±‚ï¼ˆé»˜è®¤çª—å£ä¸º7å¤©ï¼›è‹¥7å¤©å†…æ— æ•°æ®åˆ™å›é€€è‡³1å¤©ï¼Œå³å½“æ—¥çª—å£ï¼‰
    - AOï¼šæŒ‰å»é‡åçš„ AO é…ç½®ï¼ˆä»…ç§»é™¤å®Œå…¨é‡å¤è¡Œï¼Œä¸åˆå¹¶ MLï¼‰è®¡ç®— `ao_daily_avg`ï¼Œå¹¶åŸºäºç™¾åˆ†æ¯”è¯¯å·®ç”Ÿæˆæ•°é‡ï¼›æ—¥æœŸä¸º `sim_date + advance_days`
    - Normalï¼šåŒä¸€ ML æ±‡æ€» AO ç™¾åˆ†æ¯”åè®¡ç®— `normal_daily_avg = avg*(1-ao%)`ï¼Œè¯¯å·®ä¸å½“å¤©ä¸‹å•ç”Ÿæˆ
    - è®¢å•æ±‡æ€»åè¿›è¡Œâ€œæ¶ˆè€—â€ï¼š
      â€¢ AO ä¼˜å…ˆï¼Œå›ºå®šçª—å£åç§»é¡ºåº [0, -1, -2, 1, 2, 3] è¿›è¡Œè´ªå©ªæ‰£å‡ï¼Œä¿è¯ç¡®å®šæ€§
      â€¢ Normal ä»…åœ¨å½“æ—¥æ‰£å‡
        é‡‡æ ·é¢—ç²’åº¦ï¼š
        - AO é‡‡æ ·åœ¨â€œæ¯æ¡ AO é…ç½®è¡Œâ€é¢—ç²’åº¦ï¼ˆmaterial-location-advance_daysï¼‰å‘é‡åŒ–ç”Ÿæˆæ•°é‡
        - Normal é‡‡æ ·åœ¨â€œæ¯ä¸ª ML å½“æ—¥â€é¢—ç²’åº¦ï¼ˆmaterial-location å½“æ—¥ä¸€è¡Œï¼‰å‘é‡åŒ–ç”Ÿæˆæ•°é‡
        - é€šè¿‡æ•´åˆ— `np.random.normal(mean_vector, std_vector)` ä¸€æ¬¡æ€§ç”Ÿæˆï¼Œå†è£å‰ªä¸ºéè´Ÿæ•´æ•°
    ç‰¹æ®Šå¤„ç†ä¸ä¿éšœï¼š
    - é¢„æµ‹åˆå¹¶ä¸è®¢å•ç”Ÿæˆå‡åœ¨ ML ç²’åº¦ï¼Œé¿å…å‘¨æˆ–æ›´ç»†ç²’åº¦å¯¼è‡´çš„é‡å¤é”®
    - è¯¯å·®ç”Ÿæˆé‡‡ç”¨æ­£æ€å¹¶éæˆªæ–­æ­£æ€ï¼Œç»“æœå‘ä¸Šå–æ•´å¹¶è£å‰ªä¸ºéè´Ÿæ•´æ•°
    - ç»Ÿä¸€è§„èŒƒæ ‡è¯†ç¬¦ï¼Œç¡®ä¿åç»­åº“å­˜ä¸å‘è´§ç¯èŠ‚çš„é”®ä¸€è‡´
    è¿”å›ï¼š`orders_df`ï¼ˆå½“æ—¥ç”Ÿæˆçš„æ‰€æœ‰è®¢å•ï¼‰ä¸ `consumed_forecast`ï¼ˆæ‰£å‡åçš„é¢„æµ‹è§†å›¾ï¼‰
    """
    
    # Check if today is an order day
    is_order_day = not order_calendar[order_calendar['date'] == sim_date].empty
    if not is_order_day:
        return pd.DataFrame(), current_forecast
    
    orders = []
    t0 = time.perf_counter()
    # é¢„æµ‹è§†å›¾æŒ‰é”®èšåˆï¼Œä¿éšœå”¯ä¸€æ€§
    current_forecast = current_forecast.groupby(['material','location','date'], as_index=False)['quantity'].sum()
    consumed_forecast = current_forecast.copy()
    
    # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è¿‡æ»¤7å¤©çª—å£çš„æ•°æ®ï¼ˆé»˜è®¤çª—å£æ”¹ä¸º7å¤©ï¼‰
    forecast_window_days = 7
    end_date = sim_date + pd.Timedelta(days=forecast_window_days)
    
    windowed_forecast = original_forecast[
        (original_forecast['date'] >= sim_date) &
        (original_forecast['date'] < end_date)
    ].copy()
    
    # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šé¢„åˆ†ç»„è®¡ç®—å¹³å‡éœ€æ±‚ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
    if not windowed_forecast.empty:
        ml_avg_demand = windowed_forecast.groupby(['material','location'], as_index=False)['quantity'].mean()
        ml_avg_demand.columns = ['material', 'location', 'avg_daily_demand']
    else:
        # å¦‚æœ7å¤©çª—å£å†…æ²¡æœ‰æ•°æ®ï¼Œå›é€€è‡³1å¤©çª—å£ï¼ˆä»…å½“å¤©ï¼‰
        short_end_date = sim_date + pd.Timedelta(days=1)
        windowed_forecast_short = original_forecast[
            (original_forecast['date'] >= sim_date) &
            (original_forecast['date'] < short_end_date)
        ].copy()
        
        if not windowed_forecast_short. empty:
            ml_avg_demand = windowed_forecast_short.groupby(['material', 'location'], as_index=False)['quantity'].mean()
            ml_avg_demand.columns = ['material', 'location', 'avg_daily_demand']
        else:
            ml_avg_demand = pd.DataFrame(columns=['material', 'location', 'avg_daily_demand'])
    
    if ml_avg_demand.empty:
        return pd.DataFrame(), consumed_forecast
    print(f"[M1] å¹³å‡éœ€æ±‚è®¡ç®—å®Œæˆï¼ŒMLæ•°: {len(ml_avg_demand)}ï¼Œè€—æ—¶: {time.perf_counter()-t0:.3f}s")
    
    # âœ… å‘é‡åŒ–ï¼šåœ¨ç‰©æ–™-åœ°ç‚¹ç²’åº¦ç”Ÿæˆ AO ä¸ Normal è®¢å•
    # AO é…ç½®å»é‡ï¼šä»…ç§»é™¤å®Œå…¨é‡å¤çš„è¡Œï¼ˆä¸æŒ‰ ML æŠ˜å ï¼‰
    ao_cols = ['material','location','advance_days','ao_percent']
    ao_cfg_selected = ao_config[ao_cols].drop_duplicates() if not ao_config.empty else ao_config[ao_cols]
    ao_lines = ml_avg_demand.merge(ao_cfg_selected, on=['material','location'], how='left')
    ao_lines = ao_lines.dropna(subset=['ao_percent'])
    fe = forecast_error.groupby(['material','location','order_type'], as_index=False)['error_std_percent'].max()
    t1 = time.perf_counter()
    if not ao_lines.empty:
        ao_lines['ao_daily_avg'] = ao_lines['avg_daily_demand'] * ao_lines['ao_percent']
        fe_ao = fe[fe['order_type'] == 'AO'][['material','location','error_std_percent']]
        ao_e = ao_lines.merge(fe_ao, on=['material','location'], how='left')
        ao_abs_std = ao_e['ao_daily_avg'] * ao_e['error_std_percent'].fillna(0)
        ao_qty = np.maximum(0, np.round(np.random.normal(ao_e['ao_daily_avg'], ao_abs_std))).astype(int)
        ao_dates = sim_date + pd.to_timedelta(ao_e['advance_days'].astype(int), unit='D')
        ao_orders_df = pd.DataFrame({
            'date': ao_dates,
            'material': ao_e['material'].astype(str),
            'location': ao_e['location'].astype(str),
            'demand_type': 'AO',
            'quantity': ao_qty,
            'simulation_date': sim_date,
            'advance_days': ao_e['advance_days'].astype(int)
        })
    else:
        ao_orders_df = pd.DataFrame(columns=['date','material','location','demand_type','quantity','simulation_date','advance_days'])

    # Normal è®¢å•è®¡ç®—ä½¿ç”¨å»é‡åçš„ AO ç™¾åˆ†æ¯”ä¹‹å’Œï¼ˆåŒä¸€ ML ä¸åŒ advance_days ä¼šç´¯åŠ ï¼‰
    total_ao = ao_cfg_selected.groupby(['material','location'], as_index=False)['ao_percent'].sum()
    normal = ml_avg_demand.merge(total_ao, on=['material','location'], how='left')
    normal['ao_percent'] = normal['ao_percent'].fillna(0).clip(0,1)
    normal['normal_daily_avg'] = normal['avg_daily_demand'] * (1 - normal['ao_percent'])
    normal = normal[normal['normal_daily_avg'] > 0]
    t2 = time.perf_counter()
    if not normal.empty:
        fe_n = fe[fe['order_type'] == 'normal'][['material','location','error_std_percent']]
        n_e = normal.merge(fe_n, on=['material','location'], how='left')
        n_abs_std = n_e['normal_daily_avg'] * n_e['error_std_percent'].fillna(0)
        normal_qty = np.maximum(0, np.round(np.random.normal(n_e['normal_daily_avg'], n_abs_std))).astype(int)
        normal_orders_df = pd.DataFrame({
            'date': pd.Series([sim_date] * len(n_e)),
            'material': n_e['material'].astype(str),
            'location': n_e['location'].astype(str),
            'demand_type': 'normal',
            'quantity': normal_qty,
            'simulation_date': pd.Series([sim_date] * len(n_e)),
            'advance_days': 0
        })
    else:
        normal_orders_df = pd.DataFrame(columns=['date','material','location','demand_type','quantity','simulation_date','advance_days'])

    orders_df = pd.concat([ao_orders_df, normal_orders_df], ignore_index=True)
    if not orders_df.empty:
        orders_df = orders_df.groupby(['date','material','location','demand_type','simulation_date','advance_days'], as_index=False)['quantity'].sum()
        orders_df['quantity'] = orders_df['quantity'].astype(int)
        orders_df = _normalize_identifiers(orders_df)
    print(f"[M1] è®¢å•ç”Ÿæˆå®Œæˆ (AOè€—æ—¶: {time.perf_counter()-t1:.3f}s, Normalè€—æ—¶: {time.perf_counter()-t2:.3f}s, æ€»è€—æ—¶: {time.perf_counter()-t0:.3f}s)ï¼Œè®¢å•æ•°: {len(orders_df)}")

    # æ¶ˆè€—ï¼šå…ˆ AO å normalï¼Œè´ªå©ªä¼˜å…ˆé¡ºåº
    ao_consume = orders_df[orders_df['demand_type'] == 'AO'].copy() if not orders_df.empty else pd.DataFrame(columns=orders_df.columns)
    t3 = time.perf_counter()
    if not ao_consume.empty:
        ao_consume = ao_consume.sort_values(by=['date','advance_days','quantity','simulation_date'])
        offsets = np.array([0, -1, -2, 1, 2, 3], dtype=int)

        # å¹¶è¡ŒAOæ¶ˆè€—ï¼šæŒ‰(ç‰©æ–™,åœ°ç‚¹)åˆ†ç»„ä»¥é¿å…å…±äº«å†™å…¥å†²çªï¼ˆé»˜è®¤å…³é—­ï¼‰
        # é…ç½®ä½¿ç”¨æ–¹å¼ï¼šåœ¨è°ƒç”¨æœ¬å‡½æ•°å‰å°† `config_dict['M1_ParallelConfig']` è®¾ç½®å¦‚ä¸‹é”®ï¼š
        # - use_parallel_ao_consume(bool)ï¼šæ˜¯å¦å¯ç”¨AOæ¶ˆè€—å¹¶è¡Œï¼Œé»˜è®¤Falseï¼›å¼€å¯åè¾“å‡ºåº”ä¸ä¸²è¡Œç‰ˆæœ¬ä¸€è‡´ï¼ˆåŒä¸€MLå†…ä»ä¿æŒç¡®å®šæ€§é¡ºåºï¼‰
        # - parallel_max_workers(int|None)ï¼šå¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼›Noneè¡¨ç¤ºä½¿ç”¨os.cpu_count()
        use_parallel_cfg = globals().get('DEFAULT_USE_PARALLEL_AO_CONSUME', False)
        max_workers_cfg = globals().get('DEFAULT_PARALLEL_MAX_WORKERS', None)

        if use_parallel_cfg:
            # ç®€åŒ–ä¸­æ–‡è¯´æ˜ï¼š
            # - é£é™©æç¤ºï¼šå¦‚æœä¸åŒåˆ†ç»„ä¹‹é—´å­˜åœ¨å…±äº«æ—¥æœŸè¡Œï¼Œåˆå¹¶è¡¥ä¸æ—¶éœ€ç¡®ä¿ä¸äº§ç”Ÿè´Ÿæ•°ï¼›æœ¬å®ç°å¯¹æ¯ä¸ªMLç‹¬ç«‹å¤„ç†åå†å®‰å…¨åˆå¹¶ã€‚

            def _consume_ao_for_ml(args: Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, str, str]) -> pd.DataFrame:
                # å­è¿›ç¨‹çº¯å‡½æ•°ï¼šåœ¨å•ä¸€(ç‰©æ–™,åœ°ç‚¹)ä¸‹æ¶ˆè´¹AOï¼Œè¿”å›(date, material, location, new_quantity)è¡¥ä¸
                ml_orders, ml_forecast, offsets_local, mat, loc = args
                if ml_orders.empty or ml_forecast.empty:
                    return pd.DataFrame(columns=['material','location','date','new_quantity'])
                # å±€éƒ¨çª—å£è§†å›¾
                ml_forecast = ml_forecast.copy()
                for r in ml_orders.itertuples():
                    if r.quantity <= 0:
                        continue
                    remaining = int(r.quantity)
                    for od in offsets_local:
                        if remaining <= 0:
                            break
                        d = pd.to_datetime(r.date) + pd.to_timedelta(int(od), unit='D')
                        idxs = ml_forecast.index[ml_forecast['date'] == d]
                        if len(idxs) == 0:
                            continue
                        idx = idxs[0]
                        avail = int(ml_forecast.at[idx, 'quantity'])
                        take = min(avail, remaining)
                        ml_forecast.at[idx, 'quantity'] = avail - take
                        remaining -= take
                # è¾“å‡ºè¡¥ä¸
                out = ml_forecast[['date','quantity']].copy()
                out['material'] = mat
                out['location'] = loc
                out = out.rename(columns={'quantity':'new_quantity'})
                return out[['material','location','date','new_quantity']]

            # ç»„è£…ä»»åŠ¡ï¼ˆæ¯ä¸ªMLä¸€ä¸ªä»»åŠ¡ï¼‰
            tasks: List[Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, str, str]] = []
            for (mat, loc), grp in ao_consume.groupby(['material','location']):
                ml_mask = (consumed_forecast['material'] == mat) & (consumed_forecast['location'] == loc)
                ml_forecast = consumed_forecast.loc[ml_mask, ['date','quantity']].copy()
                if ml_forecast.empty:
                    continue
                tasks.append((grp[['date','quantity','advance_days','simulation_date']], ml_forecast, offsets, mat, loc))

            patches: List[pd.DataFrame] = []
            with ProcessPoolExecutor(max_workers=max_workers_cfg) as ex:
                futures = [ex.submit(_consume_ao_for_ml, t) for t in tasks]
                for f in as_completed(futures):
                    try:
                        res = f.result()
                        if res is not None and not res.empty:
                            patches.append(res)
                    except Exception:
                        # å¹¶è¡Œå­ä»»åŠ¡å¼‚å¸¸æ—¶è®°å½•æ—¥å¿—å¹¶å¿½ç•¥ï¼Œä¿æŒç¨³å¥ï¼ˆè¾“å‡ºä¸ä¸²è¡Œå¯èƒ½ä¸åŒï¼›å»ºè®®å…³é—­å¹¶è¡Œï¼‰
                        _append_error_log('[AOå¹¶è¡Œ] å­ä»»åŠ¡å¼‚å¸¸ï¼šæŸä¸ªç‰©æ–™-åœ°ç‚¹çš„AOæ¶ˆè€—æœªåº”ç”¨ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æˆ–å…³é—­å¹¶è¡Œ')

            if patches:
                patch_df = pd.concat(patches, ignore_index=True)
                # å°†è¡¥ä¸å®‰å…¨åº”ç”¨åˆ°consumed_forecastï¼ˆé˜²æ­¢è´Ÿæ•°ï¼Œä¼˜å…ˆä½¿ç”¨è¡¥ä¸çš„æ–°å€¼ï¼‰
                key_cols = ['material','location','date']
                cf = consumed_forecast.merge(
                    patch_df, on=key_cols, how='left'
                )
                cf['quantity'] = np.where(
                    cf['new_quantity'].notna(),
                    np.maximum(0, cf['new_quantity'].astype(int)),
                    cf['quantity'].astype(int)
                )
                consumed_forecast = cf[['material','location','date','quantity']]
            # å¹¶è¡Œè·¯å¾„è®¡æ—¶æ‰“å°ï¼ˆä¸ä¸²è¡Œä¸€è‡´çš„å¯è¯»æ€§ï¼‰
            print(f"[M1] AOæ¶ˆè€—å®Œæˆï¼Œè€—æ—¶: {time.perf_counter()-t3:.3f}s")
            # è‹¥æ— è¡¥ä¸æˆ–å¹¶è¡Œå¤±è´¥ï¼Œä¿æŒåŸconsumed_forecastä¸å˜
        else:
            # ä¸²è¡Œè·¯å¾„ï¼ˆä¿æŒæ—§é€»è¾‘ï¼Œç¡®ä¿è¾“å‡ºå®Œå…¨ä¸€è‡´ï¼‰
            for r in ao_consume.itertuples():
                if r.quantity <= 0:
                    continue
                target_dates = pd.to_datetime(r.date) + pd.to_timedelta(offsets, unit='D')
                ml_mask = (consumed_forecast['material'] == r.material) & (consumed_forecast['location'] == r.location)
                window_mask = ml_mask & consumed_forecast['date'].isin(target_dates)
                window = consumed_forecast.loc[window_mask, ['date','quantity']].copy()
                remaining = int(r.quantity)
                for od in offsets:
                    if remaining <= 0:
                        break
                    d = pd.to_datetime(r.date) + pd.to_timedelta(int(od), unit='D')
                    idxs = window.index[window['date'] == d]
                    if len(idxs) == 0:
                        continue
                    idx = idxs[0]
                    avail = int(window.at[idx, 'quantity'])
                    take = min(avail, remaining)
                    window.at[idx, 'quantity'] = avail - take
                    remaining -= take
                for _, w in window.iterrows():
                    consumed_forecast.loc[ml_mask & (consumed_forecast['date'] == w['date']), 'quantity'] = int(w['quantity'])
            print(f"[M1] AOæ¶ˆè€—å®Œæˆï¼Œè€—æ—¶: {time.perf_counter()-t3:.3f}s")

    normal_consume = orders_df[orders_df['demand_type'] == 'normal'].copy() if not orders_df.empty else pd.DataFrame(columns=orders_df.columns)
    t4 = time.perf_counter()
    if not normal_consume.empty:
        # ä¸ AO ç›¸åŒçš„è´ªå©ªçª—å£é¡ºåºï¼Œç¡®ä¿ç¡®å®šæ€§ï¼š[0, -1, -2, 1, 2, 3]
        offsets_n = np.array([0, -1, -2, 1, 2, 3], dtype=int)

        # ä½¿ç”¨ç‹¬ç«‹çš„ Normal å¹¶è¡Œé…ç½®å¼€å…³ï¼›ä¸ºç©ºæ—¶ç»§æ‰¿ AO å¼€å…³
        inherit_flag = globals().get('DEFAULT_USE_PARALLEL_NORMAL_CONSUME', None)
        use_parallel_normal = (inherit_flag if inherit_flag is not None else globals().get('DEFAULT_USE_PARALLEL_AO_CONSUME', False))
        max_workers_cfg = globals().get('DEFAULT_PARALLEL_MAX_WORKERS', None)

        if use_parallel_normal:
            # å­è¿›ç¨‹å‡½æ•°ï¼šåœ¨å•ä¸€(ç‰©æ–™,åœ°ç‚¹)ä¸‹æ¶ˆè´¹Normalè®¢å•ï¼Œè¿”å›(date, material, location, new_quantity)è¡¥ä¸
            def _consume_normal_for_ml(args: Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, str, str]) -> pd.DataFrame:
                ml_orders, ml_forecast, offsets_local, mat, loc = args
                if ml_orders.empty or ml_forecast.empty:
                    return pd.DataFrame(columns=['material','location','date','new_quantity'])
                ml_forecast = ml_forecast.copy()
                # Normalè®¢å•åœ¨è¯¥çª—å£å†…è´ªå©ªæ‰£å‡ï¼ˆä¸AOä¸€è‡´ï¼‰
                for r in ml_orders.itertuples():
                    if r.quantity <= 0:
                        continue
                    remaining = int(r.quantity)
                    for od in offsets_local:
                        if remaining <= 0:
                            break
                        d = pd.to_datetime(r.date) + pd.to_timedelta(int(od), unit='D')
                        idxs = ml_forecast.index[ml_forecast['date'] == d]
                        if len(idxs) == 0:
                            continue
                        idx = idxs[0]
                        avail = int(ml_forecast.at[idx, 'quantity'])
                        take = min(avail, remaining)
                        ml_forecast.at[idx, 'quantity'] = avail - take
                        remaining -= take
                out = ml_forecast[['date','quantity']].copy()
                out['material'] = mat
                out['location'] = loc
                out = out.rename(columns={'quantity':'new_quantity'})
                return out[['material','location','date','new_quantity']]

            # ç»„è£…ä»»åŠ¡ï¼šæ¯ä¸ª(ç‰©æ–™,åœ°ç‚¹)ä¸ºä¸€ä¸ªä»»åŠ¡
            tasks_n: List[Tuple[pd.DataFrame, pd.DataFrame, np.ndarray, str, str]] = []
            for (mat, loc), grp in normal_consume.groupby(['material','location']):
                ml_mask = (consumed_forecast['material'] == mat) & (consumed_forecast['location'] == loc)
                ml_forecast = consumed_forecast.loc[ml_mask, ['date','quantity']].copy()
                if ml_forecast.empty:
                    continue
                tasks_n.append((grp[['date','quantity','simulation_date']], ml_forecast, offsets_n, mat, loc))

            patches_n: List[pd.DataFrame] = []
            with ProcessPoolExecutor(max_workers=max_workers_cfg) as ex:
                futures = [ex.submit(_consume_normal_for_ml, t) for t in tasks_n]
                for f in as_completed(futures):
                    try:
                        res = f.result()
                        if res is not None and not res.empty:
                            patches_n.append(res)
                    except Exception:
                        _append_error_log('[Normalå¹¶è¡Œ] å­ä»»åŠ¡å¼‚å¸¸ï¼šæŸä¸ªç‰©æ–™-åœ°ç‚¹çš„Normalæ¶ˆè€—æœªåº”ç”¨ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æˆ–å…³é—­å¹¶è¡Œ')

            if patches_n:
                patch_df_n = pd.concat(patches_n, ignore_index=True)
                key_cols = ['material','location','date']
                cf_n = consumed_forecast.merge(patch_df_n, on=key_cols, how='left')
                cf_n['quantity'] = np.where(
                    cf_n['new_quantity'].notna(),
                    np.maximum(0, cf_n['new_quantity'].astype(int)),
                    cf_n['quantity'].astype(int)
                )
                consumed_forecast = cf_n[['material','location','date','quantity']]
            print(f"[M1] Normalæ¶ˆè€—å®Œæˆï¼ˆå¹¶è¡Œï¼‰ï¼Œè€—æ—¶: {time.perf_counter()-t4:.3f}s")
        else:
            # ä¸²è¡Œè·¯å¾„ï¼šä¸ AO ä¸€è‡´çš„è´ªå©ªçª—å£é¡ºåº
            normal_consume = normal_consume.sort_values(by=['date','quantity','simulation_date'])
            for r in normal_consume.itertuples():
                if r.quantity <= 0:
                    continue
                target_dates = pd.to_datetime(r.date) + pd.to_timedelta(offsets_n, unit='D')
                ml_mask = (consumed_forecast['material'] == r.material) & (consumed_forecast['location'] == r.location)
                window_mask = ml_mask & consumed_forecast['date'].isin(target_dates)
                window = consumed_forecast.loc[window_mask, ['date','quantity']].copy()
                remaining = int(r.quantity)
                for od in offsets_n:
                    if remaining <= 0:
                        break
                    d = pd.to_datetime(r.date) + pd.to_timedelta(int(od), unit='D')
                    idxs = window.index[window['date'] == d]
                    if len(idxs) == 0:
                        continue
                    idx = idxs[0]
                    avail = int(window.at[idx, 'quantity'])
                    take = min(avail, remaining)
                    window.at[idx, 'quantity'] = avail - take
                    remaining -= take
                for _, w in window.iterrows():
                    consumed_forecast.loc[ml_mask & (consumed_forecast['date'] == w['date']), 'quantity'] = int(w['quantity'])
            print(f"[M1] Normalæ¶ˆè€—å®Œæˆï¼ˆä¸²è¡Œï¼‰ï¼Œè€—æ—¶: {time.perf_counter()-t4:.3f}s")
    
    return orders_df, consumed_forecast


def generate_quantity_with_percent_error(mean_qty, material, location, order_type, forecast_error):
    """
    æ ¹æ®ç™¾åˆ†æ¯”è¯¯å·®ç”Ÿæˆå¸¦å™ªå£°çš„è®¢å•æ•°é‡ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰ï¼š
    - ä¼˜å…ˆè¯»å– `forecast_error` ä¸­æŒ‡å®š `order_type` çš„ `error_std_percent`ï¼Œè®¡ç®—ç»å¯¹æ ‡å‡†å·®
    - è‹¥ç¼ºå¤±åˆ™å›é€€è‡³æ—§ç‰ˆ `error_std`ï¼ˆç»å¯¹è¯¯å·®ï¼‰
    - ä½¿ç”¨æˆªæ–­æ­£æ€ï¼ˆä¸‹é™0ï¼‰ç”Ÿæˆå€¼å¹¶å››èˆäº”å…¥ä¸ºæ•´æ•°
    æ³¨ï¼šæ­¤å‡½æ•°ä¸ºé€æ¡è°ƒç”¨ç‰ˆæœ¬ï¼Œå½“å‰ä¸»è·¯å¾„ä½¿ç”¨å‘é‡åŒ–æ­£æ€é‡‡æ ·ï¼›ä¿ç•™è¯¥å‡½æ•°ç”¨äºå…¼å®¹ä¸å•ç‚¹ç”Ÿæˆåœºæ™¯ã€‚
    """
    
    # Get error percentage for this material-location-order_type
    mask = (
        (forecast_error['material'] == material) & 
        (forecast_error['location'] == location) & 
        (forecast_error['order_type'] == order_type)
    )
    error_config = forecast_error[mask]
    
    if error_config. empty:
        # Fallback to old error_std format if order_type not found
        mask_old = (
            (forecast_error['material'] == material) & 
            (forecast_error['location'] == location)
        )
        error_config_old = forecast_error[mask_old]
        if not error_config_old.empty and 'error_std' in error_config_old.columns:
            # Use absolute error for backward compatibility
            error_std = float(error_config_old['error_std']. iloc[0])
            if error_std > 0:
                error = np.random.normal(0, error_std)
                return max(0, int(round(mean_qty + error)))
        return max(0, int(round(mean_qty)))
    
    # Use percentage-based error
    if 'error_std_percent' in error_config.columns:
        error_percent = float(error_config['error_std_percent'].iloc[0])
    else:
        error_percent = 0.0
    
    # Calculate absolute standard deviation from percentage
    abs_std = mean_qty * error_percent
    
    if abs_std <= 0:
        return max(0, int(round(mean_qty)))
    
    # Generate truncated normal (>= 0)
    lower_bound = 0
    a = (lower_bound - mean_qty) / abs_std
    value = truncnorm. rvs(a, np.inf, loc=mean_qty, scale=abs_std)
    
    return max(0, int(round(value)))


def consume_forecast_ao_logic(forecast_df, material, location, order_date, consume_qty):
    """
    AO é¢„æµ‹æ¶ˆè€—ï¼ˆç¤ºä¾‹/å…¼å®¹å‡½æ•°ï¼‰ï¼š
    - å›ºå®šçª—å£ï¼šè®¢å•æ—¥å½“å¤©ã€å‰2å¤©ã€å3å¤©ï¼ˆé¡ºåºä¸º [0, -1, -2, 1, 2, 3]ï¼‰
    - è´ªå©ªæ‰£å‡ï¼Œä¸”ä¸äº§ç”Ÿè´Ÿæ•°
    è¯´æ˜ï¼šä¸»è·¯å¾„çš„ AO æ¶ˆè€—åœ¨ `generate_daily_orders` å†…å®Œæˆï¼Œæ­¤å‡½æ•°ä¿ç•™ç”¨äºå…¼å®¹æˆ–å•ç‹¬è°ƒç”¨ã€‚
    """
    if consume_qty <= 0:
        return forecast_df
    
    # Consumption window: [order_date-2, order_date-1, order_date, order_date+1, order_date+2, order_date+3]
    offsets = [0, -1, -2, 1, 2, 3]
    consumption_dates = [order_date + pd.Timedelta(days=offset) for offset in offsets]
    
    result_forecast = forecast_df.copy()
    remaining_consume = consume_qty
    
    for date in consumption_dates:
        if remaining_consume <= 0:
            break
        
        mask = (
            (result_forecast['material'] == material) & 
            (result_forecast['location'] == location) & 
            (result_forecast['date'] == date)
        )
        matching_rows = result_forecast[mask]
        
        if not matching_rows.empty:
            idx = matching_rows.index[0]
            available_qty = int(result_forecast.at[idx, 'quantity'])
            actual_consume = min(available_qty, remaining_consume)
            
            # Update forecast (cannot go below 0)
            new_qty = max(0, available_qty - actual_consume)
            result_forecast.at[idx, 'quantity'] = new_qty
            remaining_consume -= actual_consume
    
    return result_forecast


def consume_forecast_normal_logic(forecast_df, material, location, order_date, consume_qty):
    """
    Normal é¢„æµ‹æ¶ˆè€—ï¼ˆç¤ºä¾‹/å…¼å®¹å‡½æ•°ï¼‰ï¼š
    - ä»…è®¢å•å½“æ—¥è¿›è¡Œæ‰£å‡ï¼Œä¸”ä¸äº§ç”Ÿè´Ÿæ•°
    è¯´æ˜ï¼šä¸»è·¯å¾„çš„ Normal æ¶ˆè€—åœ¨ `generate_daily_orders` å†…å®Œæˆï¼Œæ­¤å‡½æ•°ä¿ç•™ç”¨äºå…¼å®¹æˆ–å•ç‹¬è°ƒç”¨ã€‚
    """
    if consume_qty <= 0:
        return forecast_df
    
    result_forecast = forecast_df.copy()
    
    mask = (
        (result_forecast['material'] == material) & 
        (result_forecast['location'] == location) & 
        (result_forecast['date'] == order_date)
    )
    matching_rows = result_forecast[mask]
    
    if not matching_rows.empty:
        idx = matching_rows.index[0]
        available_qty = int(result_forecast.at[idx, 'quantity'])
        actual_consume = min(available_qty, consume_qty)
        
        # Update forecast (cannot go below 0)
        new_qty = max(0, available_qty - actual_consume)
        result_forecast.at[idx, 'quantity'] = new_qty
    
    return result_forecast


# ----------- 8. SIMULATE SHIPMENT FOR SINGLE DAY -----------
def simulate_shipment_for_single_day(
    simulation_date, order_log, current_inventory, material_list, location_list,
    production_plan=None, delivery_plan=None
):
    """
    è®¡ç®—å•æ—¥çš„å‘è´§ï¼ˆshipmentï¼‰ä¸ç¼ºè´§ï¼ˆcutï¼‰ï¼š
    - è¾“å…¥ï¼šè®¢å•æ—¥å¿—ï¼ˆæŒ‰æ—¥èšåˆï¼‰ã€å½“å‰å¯ç”¨åº“å­˜ï¼ˆå­—å…¸å½¢å¼ï¼‰ã€å¯é€‰çš„å½“å¤©ç”Ÿäº§/è°ƒè¿ï¼ˆå½“å‰å®ç°ä¸å åŠ ï¼Œé¿å…åŒè®¡ï¼‰
    - é€»è¾‘ï¼š
      â€¢ å½“æ—¥è®¢å•åœ¨ ML ç²’åº¦èšåˆä¸º `qty_ordered`
      â€¢ ä¸åº“å­˜åˆå¹¶å¾—åˆ° `qty_avail`ï¼Œå‘è´§é‡ä¸ºäºŒè€…æœ€å°å€¼ï¼Œcut ä¸ºå·®å€¼
    - è¾“å‡ºï¼šä¸¤ä¸ª DataFrameï¼ˆshipment ä¸ cutï¼‰ï¼Œå‡è§„èŒƒåŒ–æ ‡è¯†ç¬¦
    ç‰¹åˆ«è¯´æ˜ï¼šå½“å‰åº“å­˜å·²ç”± orchestrator è®¡ç®—ä¸ºâ€œæœŸåˆ + å½“æ—¥ GRâ€ï¼Œæ­¤å¤„ä¸å†å åŠ ç”Ÿäº§/è°ƒè¿ï¼Œä»¥å…é‡å¤è®¡å…¥ã€‚
    """
    # Pre-filter by date once before loops for better performance
    prod_today = None
    if production_plan is not None and not production_plan.empty:
        prod_today = production_plan[production_plan['available_date'] == simulation_date]
    
    deliv_today = None
    if delivery_plan is not None and not delivery_plan.empty:
        deliv_today = delivery_plan[delivery_plan['actual_delivery_date'] == simulation_date]
    
    # å½“å‰åº“å­˜å·²ç”± orchestrator è®¡ç®—ï¼ˆæœŸåˆ+å½“æ—¥GRï¼‰
    inv_df = pd.DataFrame([
        {'material': k[0], 'location': k[1], 'qty_avail': v}
        for k, v in current_inventory.items()
    ])
    if inv_df.empty:
        inv_df = pd.DataFrame(columns=['material','location','qty_avail'])

    todays_orders = order_log[order_log['date'] == simulation_date] if not order_log.empty else pd.DataFrame(columns=order_log.columns)
    ord_g = todays_orders.groupby(['material','location'], as_index=False)['quantity'].sum().rename(columns={'quantity':'qty_ordered'}) if not todays_orders.empty else pd.DataFrame(columns=['material','location','qty_ordered'])
    merged = ord_g.merge(inv_df, on=['material','location'], how='left')
    merged['qty_avail'] = merged['qty_avail'].fillna(0).astype(int)
    merged['qty_ordered'] = merged['qty_ordered'].fillna(0).astype(int)
    merged['shipped'] = np.minimum(merged['qty_ordered'], merged['qty_avail']).astype(int)
    merged['cut'] = (merged['qty_ordered'] - merged['shipped']).astype(int)
    shipment_df = pd.DataFrame({
        'date': simulation_date,
        'material': merged['material'].astype(str),
        'location': merged['location'].astype(str),
        'quantity': merged['shipped'].astype(int)
    })
    cut_df = pd.DataFrame({
        'date': simulation_date,
        'material': merged['material'].astype(str),
        'location': merged['location'].astype(str),
        'quantity': merged['cut'].astype(int)
    })
    shipment_df = _normalize_identifiers(shipment_df)
    cut_df = _normalize_identifiers(cut_df)
    
    return (
        shipment_df,
        cut_df,
        current_inventory  # è¿”å›å¯ç”¨åº“å­˜
    )


# ----------- 14. é›†æˆæ¨¡å¼æ”¯æŒ -----------

def _load_previous_orders(m1_output_dir: str, current_date: pd.Timestamp, max_advance_days: int = DEFAULT_MAX_ADVANCE_DAYS,
                          use_parallel: Optional[bool] = None,
                          max_workers: Optional[int] = None) -> pd.DataFrame:
    """
    åŠ è½½è¿‘æœŸå†å²è®¢å•ï¼ˆé›†æˆæ¨¡å¼ä¼˜åŒ–ï¼‰ï¼š
    - åªè¯»å– `current_date - (max_advance_days+1)` åˆ° `current_date` ä¹‹é—´çš„ `module1_output_YYYYMMDD.xlsx`
    - åªæå– `OrderLog` å·¥ä½œè¡¨ï¼Œå¹¶ç»Ÿä¸€æ—¥æœŸç±»å‹ï¼›è¿‡æ»¤åˆ°æœŸåœ¨ `current_date` åŠä¹‹åçš„è®¢å•
    - ç›®çš„ï¼šæ§åˆ¶å†å²è¯»å–èŒƒå›´ï¼Œé¿å…éšç€ä»¿çœŸæ¨è¿›å¯¼è‡´ I/O å’Œå†…å­˜æ¶ˆè€—å¿«é€Ÿå¢é•¿
    å®¹é”™ï¼šé‡åˆ°æ–‡ä»¶/è§£æé”™è¯¯æ—¶è·³è¿‡è¯¥æ–‡ä»¶ï¼Œæ•´ä½“è¿”å›åˆå¹¶åçš„ç»“æœæˆ–ç©ºè¡¨ã€‚
    """
    try:
        if not os.path.isdir(m1_output_dir):
            return pd.DataFrame()
        
        pattern = re.compile(r"module1_output_(\d{8})\.xlsx$")
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šè®¡ç®—éœ€è¦è¯»å–çš„æœ€æ—©æ—¥æœŸï¼ˆå½“å‰æ—¥æœŸ - max_advance_days - 1ï¼‰
        # åªè¯»å–è¿™ä¸ªæ—¶é—´çª—å£å†…çš„æ–‡ä»¶ï¼Œé¿å…éšç€ä»¿çœŸæ¨è¿›è€Œè¯»å–è¶Šæ¥è¶Šå¤šçš„å†å²æ–‡ä»¶
        # åŠ 1æ˜¯ä¸ºäº†ç¡®ä¿è¦†ç›–æ‰€æœ‰å¯èƒ½è¿˜æœªåˆ°æœŸçš„è®¢å•
        earliest_relevant_date = current_date - pd.Timedelta(days=max_advance_days + 1)
        
        # é‡‡é›†å€™é€‰æ–‡ä»¶åˆ—è¡¨
        candidates = []
        for fname in os.listdir(m1_output_dir):
            m = pattern.match(fname)
            if not m:
                continue
            fdate = pd.to_datetime(m.group(1))
            if fdate.normalize() >= current_date.normalize():
                continue
            if fdate.normalize() < earliest_relevant_date.normalize():
                continue
            candidates.append(os.path.join(m1_output_dir, fname))

        # å¹¶è¡Œè¯»å–ï¼ˆçº¿ç¨‹æ± ï¼Œé€‚åˆI/Oï¼›é»˜è®¤å¼€å¯ï¼‰
        if use_parallel is None:
            use_parallel = globals().get('DEFAULT_USE_PARALLEL_FILE_LOAD', True)
        if max_workers is None:
            max_workers = globals().get('DEFAULT_PARALLEL_MAX_WORKERS', None)

        def _read_orderlog(path: str) -> Optional[pd.DataFrame]:
            try:
                xl = pd.ExcelFile(path)
                if 'OrderLog' not in xl.sheet_names:
                    return None
                df = xl.parse('OrderLog')
                if df is None or df.empty:
                    return None
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'])
                if 'simulation_date' in df.columns:
                    df['simulation_date'] = pd.to_datetime(df['simulation_date'])
                return df
            except Exception:
                _append_error_log(f"[å†å²æ–‡ä»¶å¹¶è¡Œ] è¯»å–å¤±è´¥ï¼š{path}")
                return None

        rows = []
        t0 = time.perf_counter()
        if use_parallel and candidates:
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = [ex.submit(_read_orderlog, p) for p in candidates]
                for f in as_completed(futures):
                    df = f.result()
                    if df is not None and not df.empty:
                        rows.append(df)
        else:
            for p in candidates:
                df = _read_orderlog(p)
                if df is not None and not df.empty:
                    rows.append(df)
        print(f"[M1] å†å²è®¢å•è¯»å–å®Œæˆï¼Œæ–‡ä»¶æ•°: {len(candidates)}ï¼Œåˆå¹¶æ¡ç›®: {sum(len(r) for r in rows) if rows else 0}ï¼Œè€—æ—¶: {time.perf_counter()-t0:.3f}s")
        return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()
    except Exception:
        return pd.DataFrame()


def run_daily_order_generation(
    config_dict: dict,
    simulation_date: pd.Timestamp,
    output_dir: str,
    orchestrator: object = None
) -> dict:
    """
    é›†æˆæ¨¡å¼ä¸»å…¥å£ï¼šç”ŸæˆæŒ‡å®šæ—¥æœŸçš„è®¢å•ä¸å‘è´§ï¼Œå¹¶è¾“å‡ºä¾›éœ€æ—¥å¿—ã€‚
    æ ¸å¿ƒæµç¨‹ï¼š
    1) è¯»å–å¹¶æ ¡éªŒ M1_* é…ç½®ï¼ˆé¢„æµ‹ã€è¯¯å·®ã€è®¢å•æ—¥å†ã€AOã€DPSã€Supply Choiceï¼‰
    2) è‹¥é¢„æµ‹ä¸ºå‘¨åº¦ï¼šå…ˆæ‰§è¡Œ DPS â†’ Supply Choiceï¼Œå†æŒ‰ orchestrator çš„å…¨å±€èµ·å§‹æ—¥æœŸåšæ•´æ•°æ—¥æ‹†åˆ†
    3) ç”Ÿæˆå½“æ—¥è®¢å•ï¼ˆAO+Normalï¼Œå«ç¡®å®šæ€§æ¶ˆè€—é€»è¾‘ï¼‰
    4) è¯»å–å†å²æœªåˆ°æœŸè®¢å•ï¼ˆèŒƒå›´å—æœ€å¤§ `advance_days` é™åˆ¶ï¼‰ï¼Œä¸å½“æ—¥è®¢å•åˆå¹¶
    5) è°ƒç”¨åº“å­˜æ ¡éªŒç”Ÿæˆ shipment/cutï¼›æ„å»ºä¾›éœ€æ—¥å¿—å¹¶å†™å…¥ Excel
    å…³é”®çº¦æŸä¸ç‰¹æ®Šå¤„ç†ï¼š
    - èµ·å§‹æ—¥æœŸå¿…é¡»æ¥æºäº `orchestrator.start_date`ï¼Œç¡®ä¿å…¨å±€ä¸€è‡´
    - å†å²è®¢å•ä»…åŠ è½½è¿‘çª—å£ï¼Œä¸”è¿›è¡Œå»é‡ï¼ˆåŒ…å« `quantity` åœ¨å†…çš„å…¨é”®å»é‡ï¼‰
    - è¾“å‡ºå‰ç»Ÿä¸€è§„èŒƒåŒ–æ ‡è¯†ç¬¦ï¼Œä¿éšœåç»­æ¨¡å—çš„é”®ä¸€è‡´æ€§
    è¿”å›ï¼šåŒ…å«è®¢å•ã€å‘è´§ã€ç¼ºè´§ã€ä¾›éœ€æ—¥å¿—ä¸è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸ã€‚
    """
    # print(f"ğŸ”„ Module1 è¿è¡Œäºé›†æˆæ¨¡å¼ - {simulation_date.strftime('%Y-%m-%d')}")
    
    try:
        # 1) è¯»å–é›†æˆé…ç½®
        demand_forecast = config_dict. get('M1_DemandForecast', pd.DataFrame())
        forecast_error = config_dict.get('M1_ForecastError', pd.DataFrame())
        order_calendar = config_dict.get('M1_OrderCalendar', pd.DataFrame())
        ao_config = config_dict.get('M1_AOConfig', pd.DataFrame())
        dps_cfg = config_dict.get('M1_DPSConfig', pd.DataFrame())
        supply_choice_cfg = config_dict.get('M1_SupplyChoiceConfig', pd.DataFrame())
        # 2) åŸºæœ¬æ ¡éªŒï¼ˆå¿…é¡»ï¼‰
        if demand_forecast. empty:
            raise ValueError("ç¼ºå°‘å¿…éœ€çš„é…ç½®æ•°æ®ï¼šM1_DemandForecast")
        if order_calendar.empty:
            raise ValueError("ç¼ºå°‘å¿…éœ€çš„é…ç½®æ•°æ®ï¼šM1_OrderCalendar")
        if ao_config.empty:
            raise ValueError("ç¼ºå°‘å¿…éœ€çš„é…ç½®æ•°æ®ï¼šM1_AOConfig")
        if forecast_error.empty:
            raise ValueError("ç¼ºå°‘å¿…éœ€çš„é…ç½®æ•°æ®ï¼šM1_ForecastError")

        # 3) è®¢å•æ—¥å†è§„èŒƒåŒ–
        # print(f"  ğŸ“… è®¢å•æ—¥å†éªŒè¯: {len(order_calendar)}ä¸ªæ—¥æœŸ")
        order_calendar['date'] = pd.to_datetime(order_calendar['date'])
        # print(f"  ğŸ“… è®¢å•æ—¥å†æ—¥æœŸèŒƒå›´: {order_calendar['date'].min()} åˆ° {order_calendar['date']. max()}")
        # print(f"  ğŸ“… å½“å‰ä»¿çœŸæ—¥æœŸ: {simulation_date}")
        is_order_day = not order_calendar[order_calendar['date'] == simulation_date].empty
        # print(f"  ğŸ“… å½“å‰æ—¥æœŸæ˜¯å¦ä¸ºè®¢å•æ—¥: {'æ˜¯' if is_order_day else 'å¦'}")

        # â€”â€” å°†å‘¨åº¦é¢„æµ‹è½¬æ¢ä¸ºæ—¥åº¦é¢„æµ‹ï¼ˆå…ˆåš DPS â†’ Supply Choiceï¼‰ï¼Œä¸”èµ·å§‹æ—¥æœŸå¿…é¡»ä¸å…¨å±€ä¸€è‡´ â€”â€” 
        # å¼ºåˆ¶è¦æ±‚ orchestrator å­˜åœ¨ä¸”æä¾› start_date
        if orchestrator is None or not hasattr(orchestrator, 'start_date'):
            raise ValueError("orchestrator. start_date å¿…é¡»æä¾›ï¼Œä¸” Module1 çš„èµ·å§‹æ—¥æœŸå¿…é¡»ä¸å…¨å±€ä¸€è‡´")

        # è¯»å– M1_* é…ç½®ï¼ˆè‹¥æœªæä¾›åˆ™ç”¨ç©ºè¡¨ï¼‰
        dps_config = config_dict.get('M1_DPSConfig', pd.DataFrame())
        supply_choice = config_dict.get('M1_SupplyChoiceConfig', pd.DataFrame())

        if 'week' in demand_forecast.columns:
            # å…ˆåš DPS â†’ Supply Choice
            demand_forecast = apply_dps(demand_forecast, dps_config if dps_config is not None else pd.DataFrame())
            demand_forecast = apply_supply_choice(demand_forecast, supply_choice if supply_choice is not None else pd.DataFrame())

            # èµ·å§‹æ—¥æœŸä¸¥æ ¼æ¥è‡ª orchestratorï¼ˆæ— ä»»ä½•å…œåº•ï¼‰
            sim_start = pd.to_datetime(orchestrator. start_date). normalize()

            max_week = int(demand_forecast['week'].max()) if not demand_forecast.empty else 1

            daily_demand_forecast = expand_forecast_to_days_integer_split(
                demand_forecast, sim_start, max_week
            )
            # print(f"  ğŸ“Š å‘¨åº¦é¢„æµ‹è½¬æ¢(å·²è¿‡ DPS/SC): {max_week}å‘¨ -> {len(daily_demand_forecast)}å¤©")
            # print(f"  ğŸ“… é¢„æµ‹æ—¥æœŸèŒƒå›´: {daily_demand_forecast['date'].min()} åˆ° {daily_demand_forecast['date'].max()}")
        else:
            # å·²ç»æ˜¯æ—¥åº¦æ•°æ®ï¼šé€šå¸¸ä¸å†å¯¹æ—¥åº¦æ•°æ®åº”ç”¨ DPS/SCï¼ˆæŒ‰ä½ å½“å‰å®šä¹‰ï¼‰
            daily_demand_forecast = demand_forecast. copy()
            # print(f"  ğŸ“Š ä½¿ç”¨ç°æœ‰æ—¥åº¦é¢„æµ‹(è·³è¿‡ DPS/SC): {len(daily_demand_forecast)}å¤©")

        # 6) ç”Ÿæˆå½“æ—¥è®¢å•ï¼ˆconsumption ä¿æŒåŸé€»è¾‘ï¼‰
        # æ³¨æ„ï¼šæ ‡è¯†ç¬¦å­—æ®µå·²åœ¨main_integration.pyä¸­ç»Ÿä¸€æ ‡å‡†åŒ–ï¼Œæ— éœ€é‡å¤å¤„ç†
        t0 = time.perf_counter()
        today_orders_df, consumed_forecast = generate_daily_orders(
            simulation_date, daily_demand_forecast, daily_demand_forecast, 
            ao_config, order_calendar, forecast_error
        )
        print(f"[M1] å½“æ—¥è®¢å•ç”Ÿæˆå®Œæˆï¼Œè®¢å•æ•°: {len(today_orders_df)}ï¼Œè€—æ—¶: {time.perf_counter()-t0:.3f}s")

        # 7) åˆå¹¶å†å²æœªåˆ°æœŸè®¢å• â†’ å½“æ—¥ç‰ˆæœ¬è®¢å•è§†å›¾
        # æ€§èƒ½ä¼˜åŒ–ï¼šä»ao_configä¸­è·å–æœ€å¤§advance_daysï¼Œç”¨äºä¼˜åŒ–å†å²è®¢å•åŠ è½½èŒƒå›´
        if not ao_config.empty and 'advance_days' in ao_config. columns:
            max_val = ao_config['advance_days'].max(skipna=True)
            max_advance_days = int(max_val) if pd.notna(max_val) else DEFAULT_MAX_ADVANCE_DAYS
        else:
            max_advance_days = DEFAULT_MAX_ADVANCE_DAYS
        
        t1 = time.perf_counter()
        previous_orders_all = _load_previous_orders(output_dir, simulation_date, max_advance_days)
        print(f"[M1] å†å²è®¢å•åˆå¹¶å‰è¿‡æ»¤å®Œæˆï¼Œè€—æ—¶: {time.perf_counter()-t1:.3f}s")
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šåœ¨å»é‡ä¹‹å‰å…ˆè¿‡æ»¤æœªæ¥è®¢å•ï¼Œå‡å°‘å¤„ç†çš„æ•°æ®é‡
        if not previous_orders_all.empty and 'date' in previous_orders_all.columns:
            previous_orders_all['date'] = pd.to_datetime(previous_orders_all['date'])
            previous_orders_all = previous_orders_all[previous_orders_all['date'] >= simulation_date]. copy()
        
        if not previous_orders_all.empty:
            dedup_keys = [
                c for c in ['date','material','location','demand_type','simulation_date','advance_days','quantity']
                if c in previous_orders_all.columns
            ]
            if dedup_keys:
                previous_orders_all = previous_orders_all.drop_duplicates(subset=dedup_keys)

        previous_orders_future = previous_orders_all.copy() if not previous_orders_all. empty else pd.DataFrame()

        orders_df = (
            pd.concat([previous_orders_future, today_orders_df], ignore_index=True)
            if (today_orders_df is not None and not today_orders_df.empty)
            else previous_orders_future. copy()
        )

        if not orders_df.empty:
            if 'quantity' in orders_df.columns:
                orders_df['quantity'] = orders_df['quantity'].astype(int)
            if 'simulation_date' not in orders_df.columns:
                orders_df['simulation_date'] = orders_df['date']
            # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
            orders_df = _normalize_identifiers(orders_df)

        # 8) å‘è´§ï¼ˆä¾èµ– orchestrator åº“å­˜ï¼‰
        if orchestrator is not None:
            t2 = time.perf_counter()
            shipment_df, cut_df = generate_shipment_with_inventory_check(
                orders_df, simulation_date, orchestrator,
                daily_demand_forecast, forecast_error
            )
            print(f"[M1] å‘è´§ä¸ç¼ºè´§è®¡ç®—å®Œæˆï¼Œshipment: {len(shipment_df)}ï¼Œcut: {len(cut_df)}ï¼Œè€—æ—¶: {time.perf_counter()-t2:.3f}s")
        else:
            print("  âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰Orchestratorï¼Œæ— æ³•ç”ŸæˆåŸºäºåº“å­˜çš„shipment")
            shipment_df, cut_df = pd.DataFrame(), pd.DataFrame()

        # 9) ä¾›éœ€æ—¥å¿—ï¼ˆé›†æˆè§„èŒƒï¼‰
        t3 = time.perf_counter()
        supply_demand_df = generate_supply_demand_log_for_integration(
            daily_demand_forecast, consumed_forecast, simulation_date
        )
        print(f"[M1] ä¾›éœ€æ—¥å¿—ç”Ÿæˆå®Œæˆï¼Œæ¡ç›®: {len(supply_demand_df)}ï¼Œè€—æ—¶: {time.perf_counter()-t3:.3f}s")

        # 10) è½ç›˜
        output_file = f"{output_dir}/module1_output_{simulation_date.strftime('%Y%m%d')}.xlsx"
        # è‡ªåŠ¨è®¾ç½®å¼‚å¸¸æ—¥å¿—ä¿å­˜è·¯å¾„åˆ°ä¸Module1è¾“å‡ºç›¸åŒçš„ç›®å½•ï¼Œæ— éœ€ç”¨æˆ·é¢å¤–é…ç½®
        try:
            globals()['DEFAULT_ERROR_LOG_PATH'] = os.path.join(
                output_dir,
                f"module1_parallel_errors_{simulation_date.strftime('%Y%m%d')}.txt"
            )
        except Exception:
            # å¦‚æœè®¾ç½®å¤±è´¥ï¼Œå¿½ç•¥ï¼Œä¸å½±å“ä¸»æµç¨‹
            pass
        save_module1_output_with_supply_demand(orders_df, shipment_df, supply_demand_df, output_file, cut_df)

        # print(f"âœ… Module1 å®Œæˆ - ç”Ÿæˆ {len(orders_df)} ä¸ªè®¢å•, {len(shipment_df)} ä¸ªå‘è´§, {len(cut_df)} ä¸ªcut")
        return {
            'orders_df': orders_df,
            'shipment_df': shipment_df,
            'cut_df': cut_df,
            'supply_demand_df': supply_demand_df,
            'output_file': output_file
        }

    except Exception as e:
        print(f"âŒ Module1 é›†æˆæ¨¡å¼å¤±è´¥: {e}")
        import traceback; traceback.print_exc()
        return {
            'orders_df': pd.DataFrame(),
            'shipment_df': pd.DataFrame(),
            'cut_df': pd.DataFrame(),
            'supply_demand_df': pd.DataFrame(),
            'output_file': None
        }


def generate_supply_demand_log_for_integration(
    demand_forecast: pd.DataFrame, 
    consumed_forecast: pd. DataFrame, 
    simulation_date: pd.Timestamp
) -> pd.DataFrame:
    """
    ç”Ÿæˆé›†æˆæ¨¡å¼çš„ä¾›éœ€æ—¥å¿—ï¼ˆSupplyDemandLogï¼‰ï¼š
    - ä»…è¾“å‡ºä»¿çœŸæ—¥æœŸä¹‹åã€æœªæ¥ 90 å¤©å†…çš„éœ€æ±‚ï¼ˆdemand_element="forecast"ï¼‰
    - ä½¿ç”¨ `consumed_forecast` ä½œä¸ºæ¥æºï¼Œåæ˜ è®¢å•æ¶ˆè€—åçš„æœ€æ–°éœ€æ±‚è§†å›¾
    - ç»Ÿä¸€è§„èŒƒæ ‡è¯†ç¬¦ï¼Œé¿å…åç»­æ¨¡å—çš„é”®ä¸ä¸€è‡´
    è¿”å›ï¼šåŒ…å« `date/material/location/quantity/demand_element` çš„ DataFrameã€‚
    """
    # å¤„ç†ç©ºDataFrame
    if consumed_forecast.empty or 'date' not in consumed_forecast.columns:
        return pd.DataFrame(columns=['date', 'material', 'location', 'quantity', 'demand_element'])
    
    # æ€§èƒ½ä¼˜åŒ–ï¼šåªç”Ÿæˆæœªæ¥90å¤©çš„éœ€æ±‚æ•°æ®ï¼Œå‡å°‘æ•°æ®é‡
    # 90å¤©ï¼ˆçº¦3ä¸ªæœˆï¼‰ï¼Œè¶³å¤Ÿæ»¡è¶³ä¸šåŠ¡éœ€æ±‚
    future_cutoff_date = simulation_date + pd.Timedelta(days=90)
    
    # ç”Ÿæˆæœªæ¥éœ€æ±‚æ•°æ®ï¼ˆä»¿çœŸæ—¥æœŸä¹‹åçš„90å¤©å†…ï¼‰
    future_demand = consumed_forecast[
        (pd.to_datetime(consumed_forecast['date']) > simulation_date) &
        (pd.to_datetime(consumed_forecast['date']) <= future_cutoff_date)
    ].copy()
    
    if future_demand.empty:
        # å¦‚æœæ²¡æœ‰æœªæ¥éœ€æ±‚ï¼Œè¿”å›ç©ºçš„DataFrameä½†åŒ…å«æ­£ç¡®çš„åˆ—å
        return pd.DataFrame(columns=['date', 'material', 'location', 'quantity', 'demand_element'])
    
    # æ·»åŠ demand_elementå­—æ®µï¼ˆéµå¾ªé¡¹ç›®è§„èŒƒï¼‰
    future_demand['demand_element'] = 'forecast'
    
    # ç¡®ä¿åŒ…å«å¿…è¦çš„åˆ—
    supply_demand_log = future_demand[[
        'date', 'material', 'location', 'quantity', 'demand_element'
    ]].copy()
    
    # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
    return _normalize_identifiers(supply_demand_log)

def save_module1_output_with_supply_demand(
    orders_df: pd.DataFrame, 
    shipment_df: pd. DataFrame, 
    supply_demand_df: pd.DataFrame,
    output_file: str,
    cut_df: pd. DataFrame = None
):
    """
    å°† Module1 çš„ä¸»è¾“å‡ºå†™å…¥ Excelï¼š
    - å·¥ä½œè¡¨ï¼šOrderLogã€ShipmentLogã€CutLogï¼ˆå§‹ç»ˆå†™å‡ºï¼‰ã€SupplyDemandLogã€Summary
    - ä½¿ç”¨ `_ensure_cols` ä¿è¯åˆ—å®Œæ•´ï¼Œè°ƒç”¨ `_normalize_identifiers` ä¿æŒé”®è§„èŒƒ
    å®¹é”™ï¼šæ•´ä½“å†™å…¥å¼‚å¸¸æ—¶ä»…æ‰“å°è­¦å‘Šï¼Œé˜²æ­¢ä¸­æ–­ä¸»æµç¨‹ã€‚
    """
    # ğŸ†• ç»Ÿä¸€åˆ—å¤´ä¿éšœå‡½æ•°
    def _ensure_cols(df: pd.DataFrame, cols: list) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame(columns=cols)
        # ç¼ºåˆ—è¡¥åˆ—
        for c in cols:
            if c not in df.columns:
                df[c] = pd.Series(dtype='object')
        return df[cols]
    try:
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            orders_df = _ensure_cols(orders_df, ['date','material','location','demand_type','quantity','simulation_date','advance_days'])
            shipment_df = _ensure_cols(shipment_df, ['date','material','location','quantity','demand_type','order_id'])
            cut_df = _ensure_cols(cut_df, ['date','material','location','quantity'])
            supply_demand_df = _ensure_cols(supply_demand_df, ['date','material','location','quantity','demand_element'])
            _normalize_identifiers(orders_df).to_excel(writer, sheet_name='OrderLog', index=False)
            _normalize_identifiers(shipment_df).to_excel(writer, sheet_name='ShipmentLog', index=False)
            _normalize_identifiers(cut_df).to_excel(writer, sheet_name='CutLog', index=False)  # å§‹ç»ˆå†™
            _normalize_identifiers(supply_demand_df).to_excel(writer, sheet_name='SupplyDemandLog', index=False)
            summary_data = pd.DataFrame([{
                'Total_Orders': len(orders_df),
                'Total_Shipments': len(shipment_df),
                'Total_Cuts': len(cut_df),
                'Total_SupplyDemand': len(supply_demand_df),
                'Date': orders_df['date'].iloc[0] if not orders_df.empty else 'N/A'
            }])
            summary_data.to_excel(writer, sheet_name='Summary', index=False)
    except Exception as e:
        print(f"âš ï¸  Module1 è¾“å‡ºä¿å­˜å¤±è´¥: {e}")

def _build_available_inventory_from_orchestrator(orchestrator, simulation_date: pd.Timestamp) -> dict:
    """
    æ„å»ºå½“æ—¥å¯ç”¨åº“å­˜ï¼ˆML å­—å…¸ï¼‰ï¼š
    - å¯ç”¨åº“å­˜ = æœŸåˆåº“å­˜ + å½“æ—¥ç”Ÿäº§å…¥åº“ï¼ˆlocationï¼‰+ å½“æ—¥è°ƒè¿å…¥åº“ï¼ˆreceivingï¼‰
    - ä¸‰è§†å›¾ç»Ÿä¸€åˆ° ML ç²’åº¦å¹¶æ±‡æ€»ï¼›æ˜¾å¼æ•°å€¼è½¬æ¢é¿å… `fillna` çš„ downcasting é¢„è­¦
    - è¿”å›å­—å…¸ `{(material, location): qty}` ä¾›å‘è´§ç¯èŠ‚ä½¿ç”¨
    ç‰¹åˆ«è¯´æ˜ï¼šåœ°ç‚¹åˆ—åç§°åœ¨ä¸åŒè§†å›¾ä¸­ä¸åŒï¼ˆproduction: locationï¼›delivery: receivingï¼‰ï¼Œæ­¤å¤„å·²ç»Ÿä¸€å¤„ç†ã€‚
    """
    date_str = simulation_date.strftime('%Y-%m-%d')

    # æœŸåˆ
    beg_df = orchestrator.get_beginning_inventory_view(date_str)
    # å½“æ—¥ GR
    prod_df = orchestrator.get_production_gr_view(date_str)
    delv_df = orchestrator.get_delivery_gr_view(date_str)

    # ç»Ÿä¸€å¹¶èšåˆä¸º ML ç²’åº¦
    def _to_ml(df, loc_col):
        if df is None or df.empty:
            return pd.DataFrame(columns=['material','location','quantity'])
        o = df[['material', loc_col, 'quantity']].copy()
        o['material'] = o['material'].astype(str)
        o['location'] = o[loc_col].astype(str).str.zfill(4)
        return o.groupby(['material','location'], as_index=False)['quantity'].sum()

    beg = _to_ml(beg_df, 'location')
    prod = _to_ml(prod_df, 'location')
    delv = _to_ml(delv_df, 'receiving')
    inv_df = beg.merge(prod, on=['material','location'], how='outer', suffixes=('_beg','_prod'))
    inv_df = inv_df.merge(delv, on=['material','location'], how='outer')
    # æ˜¾å¼æ•°å€¼è½¬æ¢ä»¥é¿å… fillna çš„æœªæ¥ downcasting å˜æ›´
    for col in ['quantity_beg','quantity_prod','quantity']:
        if col in inv_df.columns:
            inv_df[col] = pd.to_numeric(inv_df[col], errors='coerce')
    inv_df[['quantity_beg','quantity_prod','quantity']] = inv_df[['quantity_beg','quantity_prod','quantity']].fillna(0)
    # è®¡ç®—æ€»å¯ç”¨åº“å­˜
    qty = inv_df.get('quantity_beg', 0) + inv_df.get('quantity_prod', 0) + inv_df.get('quantity', 0)
    inv_df['qty'] = pd.to_numeric(qty, errors='coerce').fillna(0).astype(int)
    inv_df = inv_df[['material','location','qty']]
    return {(r.material, r.location): int(r.qty) for r in inv_df.itertuples(index=False)}

def generate_shipment_with_inventory_check(
    orders_df: pd. DataFrame, 
    simulation_date: pd.Timestamp, 
    orchestrator: object,
    demand_forecast: pd.DataFrame = None,
    forecast_error: pd.DataFrame = None
) -> tuple:
    """
    åŸºäºçœŸå®å¯ç”¨åº“å­˜ï¼ˆæœŸåˆ+å½“æ—¥ GRï¼‰ç”Ÿæˆå½“æ—¥å‘è´§ä¸ç¼ºè´§ï¼š
    - è¿‡æ»¤å½“æ—¥åˆ°æœŸè®¢å•ï¼›è§„èŒƒåŒ–ç‰©æ–™/åœ°ç‚¹ä»¥åŒ¹é…åº“å­˜é”®
    - é€šè¿‡ `_build_available_inventory_from_orchestrator` è·å–å½“æ—¥ ML åº“å­˜
    - è°ƒç”¨ `simulate_shipment_for_single_day` è®¡ç®— shipment/cutï¼Œå¹¶ä¸º shipment ç”Ÿæˆ `order_id`
    - è¿”å›ä¸¤ä¸ª DataFrameï¼š`shipment_df`ï¼ˆæ–°å¢ `demand_type='customer'` ä¸ `order_id`ï¼‰ä¸ `cut_df`
    è¯´æ˜ï¼šä¸å åŠ  production_plan/delivery_planï¼Œé¿å…ä¸ orchestrator çš„ GR é‡å¤è®¡å…¥ã€‚
    """
    if orders_df.empty:
        return pd.DataFrame(), pd.DataFrame()
    
    # å½“æ—¥åˆ°æœŸè®¢å•
    today_orders = orders_df[
        pd.to_datetime(orders_df['date']) == simulation_date. normalize()
    ].copy()
    if today_orders.empty:
        return pd.DataFrame(), pd.DataFrame()
    
    # ç¡®ä¿ä¸åº“å­˜é”®ä¸€è‡´çš„ç‰©æ–™æ•°æ®ç±»å‹
    today_orders['material'] = today_orders['material']. astype(str)
    
    # âœ… å¯ç”¨åº“å­˜ = æœŸåˆ + å½“æ—¥ Production GR + å½“æ—¥ Delivery GR
    current_inventory = _build_available_inventory_from_orchestrator(orchestrator, simulation_date)
    
    # Normalize material and location lists to match inventory keys
    materials = [_normalize_material(m) for m in today_orders['material'].unique().tolist()]
    locations = [_normalize_location(l) for l in today_orders['location'].unique().tolist()]
    
    # Normalize order_log material and location to match inventory keys
    order_log = today_orders.copy()
    order_log['material'] = order_log['material'].apply(_normalize_material)
    order_log['location'] = order_log['location'].apply(_normalize_location)
    
    # æ³¨æ„ï¼šæ­¤å¤„ä¸å†å åŠ  production_plan / delivery_planï¼Œé¿å…åŒè®¡
    shipment_df, cut_df, _ = simulate_shipment_for_single_day(
        simulation_date=simulation_date,
        order_log=order_log,
        current_inventory=current_inventory,
        material_list=materials,
        location_list=locations,
        production_plan=None,
        delivery_plan=None
    )
    
    if not shipment_df.empty:
        shipment_df['demand_type'] = 'customer'
        # Vectorized order_id generation instead of apply
        date_str = simulation_date.strftime('%Y%m%d')
        shipment_df['order_id'] = 'ORD_' + date_str + '_' + shipment_df.index.astype(str)
    
    # print(f"  ğŸ“¦ åŸºäº[æœŸåˆ+å½“æ—¥GR]ç”Ÿæˆ: {len(shipment_df)} ä¸ªshipment, {len(cut_df)} ä¸ªcut")
    return shipment_df, cut_df