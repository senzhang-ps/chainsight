# main_integration.py
# ä¸»é›†æˆæ‰§è¡Œè„šæœ¬ - åè°ƒæ‰€æœ‰æ¨¡å—é€šè¿‡Orchestratorè¿è¡Œ
#
# æ‰§è¡Œé¡ºåº: M1 â†’ M4 â†’ M5 â†’ M6 â†’ M3
# æ¯æ—¥å¤„ç†æ¨¡å¼ï¼Œç¡®ä¿æ¨¡å—é—´æ•°æ®æµç¯ç¯ç›¸æ‰£

import pandas as pd
import numpy as np
from pathlib import Path
import sys
from datetime import datetime
import os
from pandas.errors import EmptyDataError, ParserError
import logging

import orchestrator

logger = logging.getLogger(__name__)

# å¯¼å…¥æ‰€æœ‰æ¨¡å—
from orchestrator import create_orchestrator
from validation_manager import ValidationManager
from time_manager import SimulationTimeManager, initialize_time_manager
from config_validator import run_pre_simulation_validation
from inventory_balance_checker import InventoryBalanceChecker
from summary_report_generator import SummaryReportGenerator
from performance_profiler import PerformanceProfiler
import module1
import module3
import module4
import module5
import module6


# ========================= æ–­ç‚¹ç»­è·‘åŠŸèƒ½ =========================

def detect_last_complete_date(output_base_dir: str, start_date: str, end_date: str) -> str:
    """
    æ£€æµ‹æœ€åä¸€ä¸ªå®Œæ•´å¤„ç†çš„æ—¥æœŸ
    Args:
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        start_date: åŸå§‹å¼€å§‹æ—¥æœŸ
        end_date: åŸå§‹ç»“æŸæ—¥æœŸ
        
    Returns:
        str: æœ€åå®Œæ•´å¤„ç†çš„æ—¥æœŸ(YYYY-MM-DD)ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    print(f"ğŸ” æ£€æµ‹ä¸­æ–­ç‚¹...")
    
    output_dir = Path(output_base_dir)
    orchestrator_dir = output_dir / "orchestrator"
    
    if not orchestrator_dir.exists():
        print(f"  ğŸ“ è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œå°†ä»å¤´å¼€å§‹: {orchestrator_dir}")
        return None
    
    # ç”Ÿæˆæ—¥æœŸèŒƒå›´
    date_range = pd.date_range(start_date, end_date, freq='D')
    
    last_complete_date = None
    
    for current_date in date_range:
        date_str = current_date.strftime('%Y%m%d')
        
        # æ£€æŸ¥å…³é”®çŠ¶æ€æ–‡ä»¶æ˜¯å¦éƒ½å­˜åœ¨
        # é¡¹ç›®çº¦å®šçš„å®Œæ•´æ€§è§†å›¾ï¼ˆè§ .github/copilot-instructions.md Â§6ï¼‰
        # æ–°å¢ daily_logs ä½œä¸ºå¿…é¡»å­˜åœ¨çš„ daily summary æ—¥å¿—
        required_files = [
            f"unrestricted_inventory_{date_str}.csv",
            f"open_deployment_{date_str}.csv",
            f"planning_intransit_{date_str}.csv",
            f"space_quota_{date_str}.csv",
            f"delivery_gr_{date_str}.csv",
            f"production_gr_{date_str}.csv",
            f"shipment_log_{date_str}.csv",
            f"delivery_shipment_log_{date_str}.csv",
            f"inventory_change_log_{date_str}.csv",
            f"daily_logs_{date_str}.csv"
        ]
        
        # è½»é‡éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶å­˜åœ¨å¹¶èƒ½è¢«è¯»å–ï¼ˆåªè¯»è¡¨å¤´ä»¥é¿å…åŠ è½½å¤§å‹æ–‡ä»¶ï¼›è¡¨å¤´ä¸ºç©ºä¹Ÿè§†ä¸ºå¯æ¥å—ï¼‰
        all_files_exist = True
        for file_name in required_files:
            file_path = orchestrator_dir / file_name

            try:
                if not file_path.exists():
                    logger.warning("ç¼ºå¤±æ–‡ä»¶: %s", file_path)
                    all_files_exist = False
                    break

                # ä½¿ç”¨ nrows=0 åªè¯»è¡¨å¤´ï¼ˆå³ä¾¿æ²¡æœ‰æ•°æ®ä¹Ÿä¸ä¼šå°è¯•è¯»å–è¡Œï¼‰
                # è¿™æ ·"åªæœ‰è¡¨å¤´"æˆ–"æ— æ•°æ®"ä¸ä¼šä¸­æ­¢ç»­è·‘åˆ¤æ–­
                pd.read_csv(file_path, nrows=0, encoding="utf-8")

            except EmptyDataError:
                # æœ‰äº›æ—¥æœŸæ–‡ä»¶åªæœ‰è¡¨å¤´æˆ–æ— æ•°æ®ï¼Œè¿™æ˜¯å…è®¸çš„ï¼ˆè®°å½•ä½†ä¸è§†ä¸ºè‡´å‘½ï¼‰
                logger.info("CSV åªæœ‰è¡¨å¤´æˆ–æ— æ•°æ®ï¼ˆEmptyDataErrorï¼Œä½†å¯æ¥å—ï¼‰: %s", file_path)
                # ç»§ç»­æ£€æŸ¥ä¸‹ä¸€ä¸ªæ–‡ä»¶
                continue
            except (UnicodeDecodeError, ParserError) as e:
                logger.warning("CSV è§£ç /è§£æå¤±è´¥: %s -> %s", file_path, e)
                all_files_exist = False
                break
            except OSError as e:
                logger.error("æ–‡ä»¶è®¿é—®é”™è¯¯: %s -> %s", file_path, e)
                all_files_exist = False
                break
            except Exception as e:
                logger.exception("æœªçŸ¥é”™è¯¯è¯»å–æ–‡ä»¶ %s", file_path)
                all_files_exist = False
                break
                
        if all_files_exist:
            # éªŒè¯æ–‡ä»¶ä¸ä¸ºç©º
            try:
                inventory_file = orchestrator_dir / f"unrestricted_inventory_{date_str}.csv"
                df = pd.read_csv(inventory_file)
                if len(df) >= 0:  # å…è®¸ç©ºåº“å­˜ï¼Œä½†æ–‡ä»¶æ ¼å¼è¦æ­£ç¡®
                    last_complete_date = current_date.strftime('%Y-%m-%d')
                    print(f"  âœ… å‘ç°å®Œæ•´æ—¥æœŸ: {last_complete_date}")
                else:
                    break
            except Exception as e:
                print(f"  âš ï¸  æ—¥æœŸ {current_date.strftime('%Y-%m-%d')} æ–‡ä»¶æŸå: {e}")
                break
        else:
            print(f"  âŒ æ—¥æœŸ {current_date.strftime('%Y-%m-%d')} æ–‡ä»¶ä¸å®Œæ•´")
            break
    
    if last_complete_date:
        print(f"  ğŸ¯ æ£€æµ‹åˆ°æœ€åå®Œæ•´æ—¥æœŸ: {last_complete_date}")
    else:
        print(f"  ğŸ“ æœªå‘ç°å®Œæ•´æ—¥æœŸï¼Œå°†ä»å¤´å¼€å§‹")
        
    return last_complete_date

def restore_orchestrator_state(orchestrator, restore_date: str, output_base_dir: str):
    """
    ä»æŒ‡å®šæ—¥æœŸçš„çŠ¶æ€æ–‡ä»¶æ¢å¤OrchestratorçŠ¶æ€
    
    Args:
        orchestrator: Orchestratorå®ä¾‹
        restore_date: æ¢å¤æ—¥æœŸ (YYYY-MM-DD)
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
    """
    print(f"ğŸ”„ ä»æ—¥æœŸ {restore_date} æ¢å¤OrchestratorçŠ¶æ€...")
    
    output_dir = Path(output_base_dir)
    orchestrator_dir = output_dir / "orchestrator"
    date_str = pd.to_datetime(restore_date).strftime('%Y%m%d')
    
    # å¯è°ƒæ•´çš„æ—¥å¿—å›æº¯å¤©æ•°ï¼ˆé»˜è®¤14å¤©ï¼‰
    log_lookback_days = 14
    
    try:
        # 1. æ¢å¤æ— é™åˆ¶åº“å­˜
        inventory_file = orchestrator_dir / f"unrestricted_inventory_{date_str}.csv"
        if inventory_file.exists():
            try:
                inventory_df = pd.read_csv(inventory_file, dtype=object)
            except EmptyDataError:
                inventory_df = pd.DataFrame()
            inventory_df = _normalize_identifiers(inventory_df) if isinstance(inventory_df, pd.DataFrame) and not inventory_df.empty else pd.DataFrame()
            # é‡å»ºåº“å­˜å­—å…¸
            orchestrator.unrestricted_inventory = {}
            for _, row in inventory_df.iterrows():
                mat = str(row.get('material', '')).strip()
                loc = str(row.get('location', '')).strip()
                key = (mat, loc)
                try:
                    qty = float(row.get('quantity', 0)) if pd.notna(row.get('quantity', 0)) else 0.0
                except Exception:
                    try:
                        qty = float(str(row.get('quantity', 0)).strip())
                    except Exception:
                        qty = 0.0
                orchestrator.unrestricted_inventory[key] = qty
            print(f"  âœ… æ¢å¤åº“å­˜è®°å½•: {len(inventory_df)} æ¡")
        else:
            orchestrator.unrestricted_inventory = {}
        
        # 2. æ¢å¤åœ¨é€”åº“å­˜ (MUST rebuild as in_transit dictionary with UID keys)
        intransit_file = orchestrator_dir / f"planning_intransit_{date_str}.csv"
        if intransit_file.exists():
            try:
                intransit_df = pd.read_csv(intransit_file, dtype=object)
            except EmptyDataError:
                intransit_df = pd.DataFrame()
            if not intransit_df.empty:
                intransit_df = _normalize_identifiers(intransit_df)
                # Rebuild in_transit dictionary: transit_uid -> transit_record
                orchestrator.in_transit = {}
                for _, row in intransit_df.iterrows():
                    transit_uid = row.get('transit_uid')
                    if transit_uid is not None and str(transit_uid).strip() and str(transit_uid) != 'None':
                        uid_str = str(transit_uid)
                        # Safely convert quantity to int
                        try:
                            quantity = int(float(row.get('quantity', 0) or 0))
                        except (ValueError, TypeError):
                            quantity = 0
                        
                        orchestrator.in_transit[uid_str] = {
                            'material': str(row.get('material', '')),
                            'sending': str(row.get('sending', '')),
                            'receiving': str(row.get('receiving', '')),
                            'actual_ship_date': str(row.get('actual_ship_date', '')),
                            'actual_delivery_date': str(row.get('actual_delivery_date', '')),
                            'quantity': quantity,
                            'ori_deployment_uid': str(row.get('ori_deployment_uid', '')),
                            'vehicle_uid': str(row.get('vehicle_uid', ''))
                        }
            else:
                orchestrator.in_transit = {}
            print(f"  âœ… æ¢å¤åœ¨é€”è®°å½•: {len(orchestrator.in_transit)} æ¡")
        else:
            orchestrator.in_transit = {}
        
        # 3. æ¢å¤å¼€æ”¾è°ƒæ‹¨ (MUST be a dict with UID keys, not a list)
        deployment_file = orchestrator_dir / f"open_deployment_{date_str}.csv"
        if deployment_file.exists():
            try:
                deployment_df = pd.read_csv(deployment_file, dtype=object)
            except EmptyDataError:
                deployment_df = pd.DataFrame()
            if not deployment_df.empty:
                deployment_df = _normalize_identifiers(deployment_df)
                # Rebuild as dictionary: uid -> deployment_record
                orchestrator.open_deployment = {}
                for _, row in deployment_df.iterrows():
                    uid = row.get('ori_deployment_uid')
                    if uid is not None and str(uid).strip() and str(uid) != 'None':
                        uid_str = str(uid)
                        # Safely convert deployed_qty to int
                        try:
                            deployed_qty = int(float(row.get('deployed_qty', 0) or 0))
                        except (ValueError, TypeError):
                            deployed_qty = 0
                        
                        orchestrator.open_deployment[uid_str] = {
                            'material': str(row.get('material', '')),
                            'sending': str(row.get('sending', '')),
                            'receiving': str(row.get('receiving', '')),
                            'planned_deployment_date': str(row.get('planned_deployment_date', '')),
                            'deployed_qty': deployed_qty,
                            'demand_element': str(row.get('demand_element', ''))
                        }
            else:
                orchestrator.open_deployment = {}
            print(f"  âœ… æ¢å¤è°ƒæ‹¨è®°å½•: {len(orchestrator.open_deployment)} æ¡")
        else:
            orchestrator.open_deployment = {}
        
        # 4. æ¢å¤ç©ºé—´é…é¢
        space_file = orchestrator_dir / f"space_quota_{date_str}.csv"
        if space_file.exists():
            try:
                space_df = pd.read_csv(space_file, dtype=object)
            except EmptyDataError:
                space_df = pd.DataFrame()
            if not space_df.empty:
                space_df = _normalize_identifiers(space_df)
                orchestrator.space_quota = {}
                for _, row in space_df.iterrows():
                    key = str(row.get('location', '')).strip()
                    try:
                        used = float(row.get('used_capacity', 0) or 0)
                    except Exception:
                        used = 0.0
                    try:
                        total = float(row.get('total_capacity', 0) or 0)
                    except Exception:
                        total = 0.0
                    orchestrator.space_quota[key] = {'used': used, 'total': total}
            else:
                orchestrator.space_quota = {}
            print(f"  âœ… æ¢å¤ç©ºé—´é…é¢: {len(space_df)} æ¡")
        else:
            orchestrator.space_quota = {}
        
        # 5. æ¢å¤ç”Ÿäº§è®¡åˆ’backlog (future production)
        production_backlog_file = orchestrator_dir / f"production_plan_backlog_{date_str}.csv"
        if production_backlog_file.exists():
            try:
                backlog_df = pd.read_csv(production_backlog_file, dtype=object)
            except EmptyDataError:
                backlog_df = pd.DataFrame()
            if not backlog_df.empty:
                backlog_df = _normalize_identifiers(backlog_df)
                # Convert quantity to int
                if 'quantity' in backlog_df.columns:
                    backlog_df['quantity'] = pd.to_numeric(backlog_df['quantity'], errors='coerce').fillna(0).astype(int)
                # Convert available_date to datetime to match original structure
                if 'available_date' in backlog_df.columns:
                    backlog_df['available_date'] = pd.to_datetime(backlog_df['available_date']).dt.normalize()
                orchestrator.production_plan_backlog = backlog_df.to_dict('records')
            else:
                orchestrator.production_plan_backlog = []
            print(f"  âœ… æ¢å¤ç”Ÿäº§è®¡åˆ’backlog: {len(orchestrator.production_plan_backlog)} æ¡")
        else:
            orchestrator.production_plan_backlog = []
        
        # 6. æ¢å¤å†å²æ—¥å¿—ï¼ˆè¿‘æœŸçš„éƒ¨åˆ†ï¼‰ - å¯é…ç½®å›æº¯å¤©æ•°
        restore_date_obj = pd.to_datetime(restore_date)
        log_start_date = restore_date_obj - pd.Timedelta(days=log_lookback_days)
        
        orchestrator.shipment_log = []
        orchestrator.production_gr = []
        orchestrator.delivery_gr = []
        orchestrator.delivery_shipment_log = []
        orchestrator.inventory_change_log = []
        orchestrator.daily_logs = []
        
        current_scan_date = log_start_date
        while current_scan_date <= restore_date_obj:
            scan_date_str = current_scan_date.strftime('%Y%m%d')
            
            # æ¢å¤å‘è´§æ—¥å¿—
            shipment_file = orchestrator_dir / f"shipment_log_{scan_date_str}.csv"
            if shipment_file.exists():
                try:
                    shipment_df = pd.read_csv(shipment_file, dtype=object)
                except EmptyDataError:
                    shipment_df = pd.DataFrame()
                if not shipment_df.empty:
                    shipment_df = _normalize_identifiers(shipment_df)
                    orchestrator.shipment_log.extend(shipment_df.to_dict('records'))
            
            # æ¢å¤ç”Ÿäº§å…¥åº“æ—¥å¿—
            production_file = orchestrator_dir / f"production_gr_{scan_date_str}.csv"
            if production_file.exists():
                try:
                    production_df = pd.read_csv(production_file, dtype=object)
                except EmptyDataError:
                    production_df = pd.DataFrame()
                if not production_df.empty:
                    production_df = _normalize_identifiers(production_df)
                    orchestrator.production_gr.extend(production_df.to_dict('records'))
            
            # æ¢å¤æ”¶è´§æ—¥å¿—
            delivery_file = orchestrator_dir / f"delivery_gr_{scan_date_str}.csv"
            if delivery_file.exists():
                try:
                    delivery_df = pd.read_csv(delivery_file, dtype=object)
                except EmptyDataError:
                    delivery_df = pd.DataFrame()
                if not delivery_df.empty:
                    delivery_df = _normalize_identifiers(delivery_df)
                    orchestrator.delivery_gr.extend(delivery_df.to_dict('records'))
            
            # æ¢å¤ç«™ç‚¹é—´å‘è¿æ—¥å¿— (delivery_shipment_log)
            dship_file = orchestrator_dir / f"delivery_shipment_log_{scan_date_str}.csv"
            if dship_file.exists():
                try:
                    dship_df = pd.read_csv(dship_file, dtype=object)
                except EmptyDataError:
                    dship_df = pd.DataFrame()
                if not dship_df.empty:
                    dship_df = _normalize_identifiers(dship_df)
                    orchestrator.delivery_shipment_log.extend(dship_df.to_dict('records'))
            
            # æ¢å¤åº“å­˜å˜åŠ¨æ—¥å¿— (inventory_change_log)
            invchg_file = orchestrator_dir / f"inventory_change_log_{scan_date_str}.csv"
            if invchg_file.exists():
                try:
                    invchg_df = pd.read_csv(invchg_file, dtype=object)
                except EmptyDataError:
                    invchg_df = pd.DataFrame()
                if not invchg_df.empty:
                    invchg_df = _normalize_identifiers(invchg_df)
                    orchestrator.inventory_change_log.extend(invchg_df.to_dict('records'))
            
            # æ¢å¤ daily_logsï¼ˆæ±‡æ€»æ—¥å¿—ï¼‰
            daily_file = orchestrator_dir / f"daily_logs_{scan_date_str}.csv"
            if daily_file.exists():
                try:
                    daily_df = pd.read_csv(daily_file, dtype=object)
                except EmptyDataError:
                    daily_df = pd.DataFrame()
                if not daily_df.empty:
                    # daily_logs å¯èƒ½ä¸å«æ ‡å‡†æ ‡è¯†ç¬¦åˆ—ï¼Œä½†è°ƒç”¨normalizeä¸ä¼šæœ‰å®³
                    daily_df = _normalize_identifiers(daily_df)
                    orchestrator.daily_logs.extend(daily_df.to_dict('records'))
            
            current_scan_date += pd.Timedelta(days=1)
        
        print(f"  âœ… æ¢å¤å‘è´§æ—¥å¿—: {len(orchestrator.shipment_log)} æ¡")
        print(f"  âœ… æ¢å¤ç”Ÿäº§æ—¥å¿—: {len(orchestrator.production_gr)} æ¡")
        print(f"  âœ… æ¢å¤æ”¶è´§æ—¥å¿—: {len(orchestrator.delivery_gr)} æ¡")
        print(f"  âœ… æ¢å¤ç«™ç‚¹é—´å‘è¿æ—¥å¿—: {len(orchestrator.delivery_shipment_log)} æ¡")
        print(f"  âœ… æ¢å¤åº“å­˜å˜åŠ¨æ—¥å¿—: {len(orchestrator.inventory_change_log)} æ¡")
        print(f"  âœ… æ¢å¤daily_logs: {len(orchestrator.daily_logs)} æ¡")
        
        # 6. é‡å»ºdate-indexed dictionaries for Phase 6 optimization
        print(f"  ğŸ”§ é‡å»ºæ—¥æœŸç´¢å¼•å­—å…¸...")
        orchestrator.production_gr_by_date = {}
        orchestrator.delivery_gr_by_date = {}
        orchestrator.shipment_log_by_date = {}
        orchestrator.delivery_shipment_log_by_date = {}
        
        # Index production_gr
        for record in orchestrator.production_gr:
            date_key = record.get('date', '')
            if date_key not in orchestrator.production_gr_by_date:
                orchestrator.production_gr_by_date[date_key] = []
            orchestrator.production_gr_by_date[date_key].append(record)
        
        # Index delivery_gr
        for record in orchestrator.delivery_gr:
            date_key = record.get('date', '')
            if date_key not in orchestrator.delivery_gr_by_date:
                orchestrator.delivery_gr_by_date[date_key] = []
            orchestrator.delivery_gr_by_date[date_key].append(record)
        
        # Index shipment_log
        for record in orchestrator.shipment_log:
            date_key = record.get('date', '')
            if date_key not in orchestrator.shipment_log_by_date:
                orchestrator.shipment_log_by_date[date_key] = []
            orchestrator.shipment_log_by_date[date_key].append(record)
        
        # Index delivery_shipment_log
        for record in orchestrator.delivery_shipment_log:
            date_key = record.get('date', '')
            if date_key not in orchestrator.delivery_shipment_log_by_date:
                orchestrator.delivery_shipment_log_by_date[date_key] = []
            orchestrator.delivery_shipment_log_by_date[date_key].append(record)
        
        print(f"  âœ… æ—¥æœŸç´¢å¼•é‡å»ºå®Œæˆ: production_gr={len(orchestrator.production_gr_by_date)} å¤©, "
              f"delivery_gr={len(orchestrator.delivery_gr_by_date)} å¤©, "
              f"shipment_log={len(orchestrator.shipment_log_by_date)} å¤©, "
              f"delivery_shipment_log={len(orchestrator.delivery_shipment_log_by_date)} å¤©")
        
        # 7. è®¾ç½®å½“å‰æ—¥æœŸ
        orchestrator.current_date = restore_date_obj
        
        print(f"  ğŸ¯ OrchestratorçŠ¶æ€æ¢å¤å®Œæˆ")
        
    except Exception as e:
        print(f"  âŒ çŠ¶æ€æ¢å¤å¤±è´¥: {e}")
        raise

def check_resume_capability(output_base_dir: str, start_date: str, end_date: str):
    """
    æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»­è·‘ï¼Œè¿”å›ç»­è·‘ä¿¡æ¯
    
    Returns:
        dict: {
            'can_resume': bool,
            'last_complete_date': str,  
            'resume_from_date': str,
            'days_completed': int,
            'days_remaining': int
        }
    """
    last_complete_date = detect_last_complete_date(output_base_dir, start_date, end_date)
    
    if last_complete_date is None:
        return {
            'can_resume': False,
            'last_complete_date': None,
            'resume_from_date': start_date,
            'days_completed': 0,
            'days_remaining': len(pd.date_range(start_date, end_date, freq='D'))
        }
    
    # è®¡ç®—ç»­è·‘ä¿¡æ¯
    last_date_obj = pd.to_datetime(last_complete_date)
    resume_from_date = (last_date_obj + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
    
    total_dates = pd.date_range(start_date, end_date, freq='D')
    completed_dates = pd.date_range(start_date, last_complete_date, freq='D')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å…¨éƒ¨å®Œæˆ
    if last_complete_date >= end_date:
        return {
            'can_resume': False,
            'last_complete_date': last_complete_date,
            'resume_from_date': None,
            'days_completed': len(completed_dates),
            'days_remaining': 0,
            'already_completed': True
        }
    
    remaining_dates = pd.date_range(resume_from_date, end_date, freq='D')
    
    return {
        'can_resume': True,
        'last_complete_date': last_complete_date,
        'resume_from_date': resume_from_date,
        'days_completed': len(completed_dates),
        'days_remaining': len(remaining_dates)
    }

# ========================= åŸæœ‰å‡½æ•° =========================

# æ ‡è¯†ç¬¦å­—æ®µæ ‡å‡†åŒ–å‡½æ•°ï¼ˆç»Ÿä¸€å¤„ç†æ‰€æœ‰é…ç½®è¡¨ï¼‰
def _normalize_location(location_str) -> str:
    """Normalize location string by padding with leading zeros to 4 digits if numeric"""
    if pd.isna(location_str) or location_str is None:
        return ""
    
    location_str = str(location_str).strip()
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—å­—ç¬¦ä¸²
        if location_str.isdigit():
            return str(int(location_str)).zfill(4)
        else:
            # éæ•°å­—locationï¼ˆå¦‚A888ï¼‰ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œä¸åšpadding
            return location_str
    except (ValueError, TypeError):
        return str(location_str)

def _normalize_material(material_str) -> str:
    """Normalize material string to ensure consistent format"""
    if material_str is None or material_str == '' or str(material_str).lower() in ['nan', 'none', '<na>']:
        return ""
    
    try:
        # å¦‚æœæ˜¯æ•°å­—ï¼ˆintæˆ–floatï¼‰ï¼Œè½¬æ¢ä¸ºæ•´æ•°å­—ç¬¦ä¸²ä»¥ç§»é™¤å¤šä½™çš„.0
        if isinstance(material_str, (int, float)) or str(material_str).replace('.', '').replace('-', '').isdigit():
            return str(int(float(material_str)))
        else:
            # éæ•°å­—materialï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
            return str(material_str).strip()
    except (ValueError, TypeError):
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
        return str(material_str).strip()

def _normalize_sending(sending_str) -> str:
    """Normalize sending string by padding with leading zeros to 4 digits if numeric"""
    if pd.isna(sending_str) or sending_str is None:
        return ""
    
    sending_str = str(sending_str).strip()
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—å­—ç¬¦ä¸²
        if sending_str.isdigit():
            return str(int(sending_str)).zfill(4)
        else:
            # éæ•°å­—sendingï¼ˆå¦‚A888ï¼‰ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œä¸åšpadding
            return sending_str
    except (ValueError, TypeError):
        return str(sending_str)

def _normalize_receiving(receiving_str) -> str:
    """Normalize receiving string by padding with leading zeros to 4 digits if numeric"""
    if pd.isna(receiving_str) or receiving_str is None:
        return ""
    
    receiving_str = str(receiving_str).strip()
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—å­—ç¬¦ä¸²
        if receiving_str.isdigit():
            return str(int(receiving_str)).zfill(4)
        else:
            # éæ•°å­—receivingï¼ˆå¦‚A888ï¼‰ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œä¸åšpadding
            return receiving_str
    except (ValueError, TypeError):
        return str(receiving_str)

def _normalize_identifiers(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize identifier columns to string format with proper formatting"""
    if df.empty:
        return df
    
    # Define identifier columns that need string conversion
    identifier_cols = ['material', 'location', 'sending', 'receiving', 'sourcing', 'dps_location', 'from_material', 'to_material', 'line', 'delegate_line', 'changeover_id']
    
    df = df.copy()
    for col in identifier_cols:
        if col in df.columns:
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ object dtype (Python str) è€Œä¸æ˜¯ pandas StringDtype
            # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸åç»­ astype(str) çš„ä¸€è‡´æ€§
            df[col] = df[col].astype(str)
            # Apply specific normalization for location-type fields
            if col in ['location', 'dps_location']:
                df[col] = df[col].apply(_normalize_location)
            elif col == 'sending':
                df[col] = df[col].apply(_normalize_sending)
            elif col == 'receiving':
                df[col] = df[col].apply(_normalize_receiving)
            # Apply specific normalization for material-type fields
            elif col in ['material', 'from_material', 'to_material']:
                df[col] = df[col].apply(_normalize_material)
            # changeover_id å’Œ line åªéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä¸éœ€è¦ç‰¹æ®Šæ ¼å¼åŒ–
            # (å·²åœ¨ astype('string') æ—¶å¤„ç†)
            # For other identifier columns (line, delegate_line, etc), ensure they are properly formatted strings
            elif col in ['changeover_id', 'line', 'delegate_line']:
                # è¿™äº›å­—æ®µåªéœ€è¦ä¿æŒä¸ºå­—ç¬¦ä¸²ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
                pass
            else:
                df[col] = df[col].apply(lambda x: str(x) if pd.notna(x) else "")
    
    return df

def run_module4_integrated(
    config_dict: dict,
    module3_output_dir: str,
    simulation_date: pd.Timestamp,
    simulation_start: pd.Timestamp,
    output_dir: str
) -> pd.DataFrame:
    """
    é›†æˆæ¨¡å¼è¿è¡Œ Module4 ç”Ÿäº§è®¡åˆ’ï¼Œç›´æ¥ä½¿ç”¨ config_dict æ•°æ®
    
    Args:
        config_dict: é…ç½®æ•°æ®å­—å…¸
        module3_output_dir: Module3 è¾“å‡ºç›®å½•
        simulation_date: å½“å‰ä»¿çœŸæ—¥æœŸ
        simulation_start: ä»¿çœŸå¼€å§‹æ—¥æœŸ
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        pd.DataFrame: ç”Ÿäº§è®¡åˆ’æ•°æ®
    """
    try:
        # éªŒè¯å¿…éœ€çš„Module4é…ç½®æ•°æ®
        required_m4_configs = [
            'M4_MaterialLocationLineCfg',
            'M4_LineCapacity', 
            'M4_ChangeoverMatrix',
            'M4_ChangeoverDefinition',
            'M4_ProductionReliability'
        ]
        
        for config_name in required_m4_configs:
            if config_name not in config_dict or config_dict[config_name].empty:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„Module4é…ç½®æ•°æ®ï¼š{config_name}")
        
        # ç›´æ¥æ„å»º Module4 æ‰€éœ€çš„é…ç½®æ•°æ®
        # ç›´æ¥ä½¿ç”¨config_dictï¼Œä¸å†éœ€è¦å­é…ç½®å­—å…¸
        m4_config = config_dict
        
        # åŠ è½½ Module3 çš„æ—¥åº¦å‡€éœ€æ±‚æ•°æ®
        net_demand_df = module4.load_daily_net_demand(module3_output_dir, simulation_date)
        net_demand_df = module4._cast_identifiers_to_str(net_demand_df, ['material', 'location'])
        
        # ğŸ”§ ä¿®å¤Module3â†’Module4æ•°æ®æµï¼šæ ‡å‡†åŒ–materialå­—æ®µï¼Œç§»é™¤.0åç¼€
        if not net_demand_df.empty and 'material' in net_demand_df.columns:
            net_demand_df['material'] = net_demand_df['material'].apply(_normalize_material).astype('string')
        
        if net_demand_df.empty:
            print(f"Warning: No NetDemand data for {simulation_date.strftime('%Y-%m-%d')}. Generating empty output.")
        
        # ç¡®ä¿ requirement_date æ˜¯ datetime ç±»å‹
        if not net_demand_df.empty and 'requirement_date' in net_demand_df.columns:
            net_demand_df['requirement_date'] = pd.to_datetime(net_demand_df['requirement_date'])
        
        # æ„å»ºæ— çº¦æŸè®¡åˆ’
        mlcfg = m4_config['M4_MaterialLocationLineCfg']
        
        # ç¡®ä¿MLCFGä¹Ÿåº”ç”¨ç±»å‹è½¬æ¢ï¼ˆä¸NetDemandä¿æŒä¸€è‡´ï¼‰
        mlcfg = module4._cast_identifiers_to_str(mlcfg.copy(), ['material', 'location'])
        
        issues = []
        uncon_plan = module4.build_unconstrained_plan_for_single_day(
            net_demand_df, mlcfg, simulation_date, simulation_start, issues
        )
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ‡å‡†åŒ–uncon_planä¸­çš„materialå­—æ®µï¼Œç¡®ä¿ä¸changeover matrixä¸€è‡´
        if not uncon_plan.empty and 'material' in uncon_plan.columns:
            # print(f"\nğŸ” DEBUG uncon_plan æ ‡å‡†åŒ–å‰:")
            # print(f"  material dtype: {uncon_plan['material'].dtype}")
            # print(f"  å‰5ä¸ª material: {list(uncon_plan['material'].head())}")
            
            uncon_plan['material'] = uncon_plan['material'].apply(_normalize_material).astype('string')
            
            # print(f"\n  æ ‡å‡†åŒ–å:")
            # print(f"  material dtype: {uncon_plan['material'].dtype}")
            # print(f"  å‰5ä¸ª material: {list(uncon_plan['material'].head())}")
            # print(f"  Line åˆ—: {list(uncon_plan['line'].unique())}")
        
        # è®¾ç½®äº§èƒ½åˆ†é…å‚æ•°
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ‡å‡†åŒ– ChangeoverMatrix ä¸­çš„å­—æ®µä¸ºå­—ç¬¦ä¸²ç±»å‹
        co_mat_df = m4_config['M4_ChangeoverMatrix'].copy()
        
        # ğŸ” è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®ç±»å‹
        # print(f"\nğŸ” DEBUG M4 ChangeoverMatrix æ•°æ®ç±»å‹:")
        # print(f"  åŸå§‹ from_material dtype: {co_mat_df['from_material'].dtype}")
        # print(f"  åŸå§‹ to_material dtype: {co_mat_df['to_material'].dtype}")
        # print(f"  åŸå§‹ changeover_id dtype: {co_mat_df['changeover_id'].dtype}")
        # print(f"  å‰5æ¡è®°å½•:")
        # print(co_mat_df.head())
        
        co_mat_df['from_material'] = co_mat_df['from_material'].astype(str)
        co_mat_df['to_material'] = co_mat_df['to_material'].astype(str)
        co_mat_df['changeover_id'] = co_mat_df['changeover_id'].astype(str)
        
        # print(f"\n  è½¬æ¢å from_material dtype: {co_mat_df['from_material'].dtype}")
        # print(f"  è½¬æ¢å to_material dtype: {co_mat_df['to_material'].dtype}")
        # print(f"  è½¬æ¢å changeover_id dtype: {co_mat_df['changeover_id'].dtype}")
        # print(f"  è½¬æ¢åå‰5æ¡è®°å½•:")
        # print(co_mat_df.head())
        
        # Note: Changeover å»é‡å·²åœ¨ load_configuration ä¸­å®Œæˆ
        
        co_mat = co_mat_df.set_index(['from_material', 'to_material'])['changeover_id']
        # å¯¹MultiIndexè¿›è¡Œæ’åºä»¥é¿å…æ€§èƒ½è­¦å‘Š
        co_mat = co_mat.sort_index()
        
        # print(f"\n  Co_mat ç´¢å¼•ç±»å‹: {co_mat.index.dtypes}")
        # print(f"  Co_mat æ€»æ¡ç›®æ•°: {len(co_mat)}")
        # print(f"  å‰5ä¸ªç´¢å¼•: {list(co_mat.index[:5])}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ‡å‡†åŒ– ChangeoverDefinition ä¸­çš„ changeover_id ä¸ºå­—ç¬¦ä¸²ç±»å‹
        co_def_df = m4_config['M4_ChangeoverDefinition'].copy()
        co_def_df['changeover_id'] = co_def_df['changeover_id'].astype(str)
        co_def = co_def_df.set_index(['changeover_id', 'line'])['time'].to_dict()
        
        cap_df = m4_config['M4_LineCapacity'].copy()
        cap_df['date'] = pd.to_datetime(cap_df['date'])
        
        rate_map = mlcfg.set_index(['material', 'delegate_line'])['prd_rate']
        rate_map.index.set_names(['material', 'line'], inplace=True)
        
        # åŠ è½½å‰ä¸€å¤©äº§çº¿çŠ¶æ€ç”¨äºè·¨å¤©è½¬äº§è¿ç»­æ€§
        previous_line_states = module4.load_line_state(output_dir, simulation_date)
        if previous_line_states:
            print(f"  ğŸ”„ åŠ è½½å‰ä¸€å¤©äº§çº¿çŠ¶æ€: {list(previous_line_states.keys())}")
        else:
            print(f"  ğŸ”„ æ— å‰ä¸€å¤©äº§çº¿çŠ¶æ€ - å…¨æ–°å¼€å§‹")
        
        # åŠ è½½ä¹‹å‰æ‰€æœ‰ä»¿çœŸæ—¥æœŸå·²åˆ†é…çš„äº§èƒ½
        previously_allocated_capacity = module4.load_all_previous_capacity(output_dir, simulation_date)
        if previously_allocated_capacity:
            print(f"  ğŸ”„ åŠ è½½ä¹‹å‰å·²åˆ†é…äº§èƒ½: {len(previously_allocated_capacity)} ä¸ªäº§èƒ½åˆ†é…")
        else:
            print(f"  ğŸ”„ æ— ä¹‹å‰å·²åˆ†é…äº§èƒ½ - å…¨æ–°å¼€å§‹")
        
        # åˆ†é…äº§èƒ½ï¼ˆæ”¯æŒè·¨å¤©è½¬äº§è¿ç»­æ€§å’Œäº§èƒ½è·Ÿè¸ªï¼‰
        plan_log, exceed_log = module4.centralized_capacity_allocation_with_changeover(
            uncon_plan, cap_df, rate_map, co_mat, co_def, mlcfg,
            previous_line_states=previous_line_states, simulation_date=simulation_date,
            previously_allocated_capacity=previously_allocated_capacity, issues=issues
        )
        
        # ä»¿çœŸç”Ÿäº§å¯é æ€§
        random_seed = m4_config.get('RandomSeed', 42)
        plan_log = module4.simulate_production(plan_log, m4_config['M4_ProductionReliability'], seed=random_seed)
        
        # è®¡ç®—æ¢äº§æŒ‡æ ‡
        changeover_log = module4.calculate_changeover_metrics(plan_log, co_def_df)
        
        # æå–å¹¶ä¿å­˜å½“å¤©äº§çº¿çŠ¶æ€ä¾›ä¸‹ä¸€å¤©ä½¿ç”¨ï¼ˆå¸¦è·¨å¤©è½¬äº§æ£€æµ‹ï¼‰
        current_line_states = module4.extract_line_states_from_plan(plan_log, cap_df, co_def, simulation_date, rate_map.to_dict())
        if current_line_states:
            module4.save_line_state(output_dir, simulation_date, current_line_states)
            print(f"  ğŸ’¾ ä¿å­˜å½“å¤©äº§çº¿çŠ¶æ€: {list(current_line_states.keys())}")
        
        # æå–å¹¶ä¿å­˜å½“å¤©åˆ†é…çš„äº§èƒ½ä¾›åç»­ä»¿çœŸæ—¥æœŸä½¿ç”¨
        current_allocated_capacity = module4.extract_allocated_capacity_from_plan(plan_log, rate_map.to_dict(), co_def)
        if current_allocated_capacity:
            module4.save_allocated_capacity(output_dir, simulation_date, current_allocated_capacity)
            print(f"  ğŸ’¾ ä¿å­˜å½“å¤©åˆ†é…äº§èƒ½: {len(current_allocated_capacity)} ä¸ªäº§èƒ½åˆ†é… (å°æ—¶å•ä½)")
        
        # å»é‡é—®é¢˜
        issues = module4.dedup_issues(issues)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        base_output_file = os.path.join(output_dir, "Module4Output.xlsx")
        daily_output_path = module4.write_output(
            plan_log, exceed_log, issues, changeover_log, 
            base_output_file, simulation_date
        )
        
        print(f"Module4 daily output generated: {daily_output_path}")
        
        # è¿”å›ç”Ÿäº§è®¡åˆ’æ•°æ®ï¼ˆåªè¿”å›å½“æ—¥å¯ç”¨çš„ç”Ÿäº§ï¼‰
        if not plan_log.empty and 'available_date' in plan_log.columns:
            plan_log['available_date'] = pd.to_datetime(plan_log['available_date'])
            current_production = plan_log[plan_log['available_date'] >= simulation_date.normalize()]
            
            # ç¡®ä¿è¿”å›çš„æ•°æ®æ ‡è¯†ç¬¦å·²æ ‡å‡†åŒ–ï¼Œä¸orchestratoræœŸæœ›æ ¼å¼ä¸€è‡´
            if not current_production.empty:
                current_production = _normalize_identifiers(current_production)
                
            return current_production
        else:
            return pd.DataFrame()
        
    except Exception as e:
        import traceback
        print(f'[ERROR] Module4 integrated execution failed for {simulation_date.strftime("%Y-%m-%d")}: {str(e)}')
        print("Full traceback:")
        traceback.print_exc()
        return pd.DataFrame()

# ========== Module4 é›†æˆè¾…åŠ©å‡½æ•°ï¼ˆæ¸…ç†åï¼‰ ==========

# ä»¥ä¸‹å‡½æ•°ä¿ç•™ä½œä¸ºé»˜è®¤é…ç½®çš„å¤‡ç”¨ï¼Œä½†ä¸å†ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶













def load_all_historical_production_plans(module4_output_dir: str, current_date: pd.Timestamp, start_date: pd.Timestamp) -> pd.DataFrame:
    """
    åŠ è½½æ‰€æœ‰å†å²çš„M4ç”Ÿäº§è®¡åˆ’ï¼Œç­›é€‰å‡ºå½“æ—¥åº”è¯¥å…¥åº“çš„ç”Ÿäº§
    
    Args:
        module4_output_dir: Module4 è¾“å‡ºç›®å½•
        current_date: å½“å‰æ—¥æœŸ
        start_date: ä»¿çœŸå¼€å§‹æ—¥æœŸ
        
    Returns:
        pd.DataFrame: å½“æ—¥åº”è¯¥å…¥åº“çš„ç”Ÿäº§è®¡åˆ’æ•°æ®
    """
    all_production_plans = []
    
    # éå†ä»ä»¿çœŸå¼€å§‹åˆ°å½“å‰æ—¥æœŸçš„æ‰€æœ‰M4è¾“å‡ºæ–‡ä»¶
    date_range = pd.date_range(start_date, current_date, freq='D')
    
    for date in date_range:
        m4_file = Path(module4_output_dir) / f"Module4Output_{date.strftime('%Y%m%d')}.xlsx"
        
        if m4_file.exists():
            try:
                xl = pd.ExcelFile(m4_file)
                if 'ProductionPlan' in xl.sheet_names:
                    production_df = xl.parse('ProductionPlan')
                    
                    if not production_df.empty:
                        # æ·»åŠ æ•°æ®æ¥æºæ ‡è¯†
                        production_df['source_file'] = str(m4_file)
                        production_df['source_date'] = date
                        all_production_plans.append(production_df)
                        
            except Exception as e:
                print(f"Warning: Failed to read {m4_file}: {e}")
                continue
    
    if not all_production_plans:
        return pd.DataFrame()
    
    # åˆå¹¶æ‰€æœ‰ç”Ÿäº§è®¡åˆ’
    combined_production = pd.concat(all_production_plans, ignore_index=True)
    
    # ç­›é€‰å‡ºå½“æ—¥åº”è¯¥å…¥åº“çš„ç”Ÿäº§ (available_date = current_date)
    if 'available_date' in combined_production.columns:
        combined_production['available_date'] = pd.to_datetime(combined_production['available_date'])
        daily_available = combined_production[
            combined_production['available_date'].dt.normalize() == current_date.normalize()
        ]
        
        # if not daily_available.empty:
        #     print(f"  ğŸ“¦ å‘ç°å½“æ—¥å…¥åº“çš„å†å²ç”Ÿäº§: {len(daily_available)} æ¡è®°å½•")
        #     for _, row in daily_available.iterrows():
        #         print(f"    {row['material']}@{row['location']}: {row['produced_qty']} (ç”Ÿäº§æ—¥æœŸ: {row['source_date'].strftime('%Y-%m-%d')})")
        
        return daily_available[['material', 'location', 'line', 'simulation_date', 'available_date', 'produced_qty']]
    
    return pd.DataFrame()

def load_module4_production_output(output_path: str, current_date: pd.Timestamp) -> pd.DataFrame:
    """
    ä» Module4 è¾“å‡ºæ–‡ä»¶ä¸­åŠ è½½ç”Ÿäº§è®¡åˆ’æ•°æ® (ä¿ç•™ç”¨äºå‘åå…¼å®¹)
    
    Args:
        output_path: Module4 è¾“å‡ºæ–‡ä»¶è·¯å¾„
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        pd.DataFrame: ç”Ÿäº§è®¡åˆ’æ•°æ®
    """
    try:
        if not os.path.exists(output_path):
            print(f"Warning: Module4 output file not found: {output_path}")
            return pd.DataFrame()
            
        xl = pd.ExcelFile(output_path)
        if 'ProductionPlan' not in xl.sheet_names:
            print(f"Warning: ProductionPlan sheet not found in {output_path}")
            return pd.DataFrame()
            
        production_df = xl.parse('ProductionPlan')
        
        # ç­›é€‰å½“æ—¥çš„ç”Ÿäº§è®¡åˆ’ (available_date = current_date)
        if not production_df.empty and 'available_date' in production_df.columns:
            production_df['available_date'] = pd.to_datetime(production_df['available_date'])
            # åªè¿”å›å½“æ—¥æˆ–æœªæ¥çš„ç”Ÿäº§è®¡åˆ’
            production_df = production_df[production_df['available_date'] >= current_date.normalize()]
            
        return production_df
        
    except Exception as e:
        print(f"Error loading Module4 production output: {e}")
        return pd.DataFrame()

def load_global_seed(config_dict: dict) -> int:
    """
    ç»Ÿä¸€ä» Global_Seed sheet è¯»å–éšæœºç§å­
    
    Args:
        config_dict: é…ç½®æ•°æ®å­—å…¸
        
    Returns:
        int: éšæœºç§å­å€¼ï¼Œé»˜è®¤ä¸º 42
    """
    if 'Global_Seed' in config_dict and not config_dict['Global_Seed'].empty:
        seed_df = config_dict['Global_Seed']
        if 'seed' in seed_df.columns:
            seed_value = int(seed_df.iloc[0]['seed'])
            print(f"ğŸŒ± ä» Global_Seed è¯»å–éšæœºç§å­: {seed_value}")
            return seed_value
        elif len(seed_df.columns) > 0 and len(seed_df) > 0:
            # å…¼å®¹æ—§æ ¼å¼ï¼Œè¯»å–ç¬¬ä¸€åˆ—ç¬¬ä¸€è¡Œ
            seed_value = int(seed_df.iloc[0, 0])
            print(f"ğŸŒ± ä» Global_Seed å…¼å®¹æ ¼å¼è¯»å–éšæœºç§å­: {seed_value}")
            return seed_value
    
    print(f"âš ï¸  æœªæ‰¾åˆ° Global_Seed é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼: 42")
    return 42

def set_module_seeds(config_dict: dict, global_seed: int = None):
    """
    ä¸ºæ‰€æœ‰æ¨¡å—è®¾ç½®ç»Ÿä¸€çš„éšæœºç§å­
    
    Args:
        config_dict: é…ç½®æ•°æ®å­—å…¸
        global_seed: å…¨å±€ç§å­å€¼ï¼Œå¦‚æœä¸º None åˆ™ä»é…ç½®è¯»å–
    """
    if global_seed is None:
        global_seed = load_global_seed(config_dict)
    
    # è®¾ç½® numpyå…¨å±€ç§å­
    np.random.seed(global_seed)
    
    # ä¸ºå„æ¨¡å—è®¾ç½®ç§å­ï¼ˆåœ¨é…ç½®ä¸­è¦†ç›–æ¨¡å—ç‰¹å®šé…ç½®ï¼‰
    config_dict['M1_RandomSeed'] = global_seed
    config_dict['M3_RandomSeed'] = global_seed  
    config_dict['M4_RandomSeed'] = global_seed
    config_dict['M5_RandomSeed'] = global_seed
    config_dict['M6_RandomSeed'] = global_seed
    
    print(f"âœ¨ å·²ä¸ºæ‰€æœ‰æ¨¡å—è®¾ç½®ç»Ÿä¸€éšæœºç§å­: {global_seed}")
    return global_seed

def load_configuration(config_path: str) -> dict:
    """
    åŠ è½½é…ç½®æ•°æ®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: é…ç½®æ•°æ®å­—å…¸
    """
    print(f"ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    
    try:
        xl = pd.ExcelFile(config_path)
        config_dict = {}
        
        # åŠ è½½æ‰€æœ‰é…ç½®è¡¨
        for sheet_name in xl.sheet_names:
            config_dict[sheet_name] = xl.parse(sheet_name)
            print(f"  âœ… åŠ è½½é…ç½®è¡¨: {sheet_name} ({len(config_dict[sheet_name])} è¡Œ)")
        
        # ç¡®ä¿å¿…è¦çš„é…ç½®è¡¨å­˜åœ¨
        required_sheets = [
            'M1_InitialInventory',
            'Global_SpaceCapacity',
            'Global_Network',
            'Global_LeadTime',
            'Global_DemandPriority'
        ]
        
        missing_sheets = [sheet for sheet in required_sheets if sheet not in config_dict]
        if missing_sheets:
            print(f"âš ï¸  ç¼ºå°‘å¿…è¦é…ç½®è¡¨: {missing_sheets}")
            # åˆ›å»ºç©ºçš„é…ç½®è¡¨
            for sheet in missing_sheets:
                config_dict[sheet] = pd.DataFrame()
        
        # ç»Ÿä¸€æ ‡å‡†åŒ–æ‰€æœ‰é…ç½®è¡¨çš„æ ‡è¯†ç¬¦å­—æ®µ
        print(f"ğŸ”§ æ­£åœ¨æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µ...")
        standardized_count = 0
        for sheet_name, df in config_dict.items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡è¯†ç¬¦å­—æ®µ
                identifier_cols = ['material', 'location', 'sending', 'receiving', 'sourcing', 'dps_location', 'from_material', 'to_material', 'line', 'delegate_line', 'changeover_id']
                has_identifiers = any(col in df.columns for col in identifier_cols)
                
                if has_identifiers:
                    original_dtypes = {col: str(df[col].dtype) for col in identifier_cols if col in df.columns}
                    config_dict[sheet_name] = _normalize_identifiers(df)
                    new_dtypes = {col: str(config_dict[sheet_name][col].dtype) for col in identifier_cols if col in config_dict[sheet_name].columns}
                    
                    # è®°å½•æ ‡å‡†åŒ–çš„å­—æ®µ
                    normalized_fields = []
                    for col in identifier_cols:
                        if col in df.columns and original_dtypes[col] != new_dtypes[col]:
                            normalized_fields.append(f"{col}({original_dtypes[col]}â†’{new_dtypes[col]})")
                    
                    if normalized_fields:
                        print(f"  ğŸ”§ {sheet_name}: {', '.join(normalized_fields)}")
                        standardized_count += 1
        
        if standardized_count > 0:
            print(f"âœ… å·²æ ‡å‡†åŒ– {standardized_count} ä¸ªé…ç½®è¡¨çš„æ ‡è¯†ç¬¦å­—æ®µ")
        else:
            print(f"âœ… æ‰€æœ‰é…ç½®è¡¨çš„æ ‡è¯†ç¬¦å­—æ®µå·²æ˜¯æ ‡å‡†æ ¼å¼")
        
        # ğŸ”§ Changeover é…ç½®æ ¡éªŒå’Œå»é‡
        if 'M4_ChangeoverMatrix' in config_dict and not config_dict['M4_ChangeoverMatrix'].empty:
            print(f"\nğŸ”§ æ ¡éªŒ Changeover Matrix é…ç½®...")
            co_matrix = config_dict['M4_ChangeoverMatrix']
            
            # æ£€æŸ¥é‡å¤å®šä¹‰
            duplicates = co_matrix[co_matrix.duplicated(subset=['from_material', 'to_material'], keep=False)]
            if not duplicates.empty:
                print(f"  âš ï¸  å‘ç° {len(duplicates)} æ¡é‡å¤çš„ changeover matrix å®šä¹‰")
                
                # è¯¦ç»†æ£€æŸ¥æ¯ç»„é‡å¤
                for (from_mat, to_mat), group in duplicates.groupby(['from_material', 'to_material']):
                    unique_coids = group['changeover_id'].unique()
                    if len(unique_coids) > 1:
                        # ä¸åŒçš„ changeover_id - ä¸¥é‡é”™è¯¯
                        print(f"    âŒ ERROR: {from_mat} â†’ {to_mat} æœ‰ {len(unique_coids)} ä¸ªä¸åŒçš„ changeover_id: {list(unique_coids)}")
                    else:
                        # ç›¸åŒçš„ changeover_id - åªæ˜¯é‡å¤
                        print(f"    âš ï¸  {from_mat} â†’ {to_mat} æœ‰ {len(group)} æ¡é‡å¤è®°å½• (changeover_id={unique_coids[0]})")
                
                # å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰
                original_count = len(co_matrix)
                config_dict['M4_ChangeoverMatrix'] = co_matrix.drop_duplicates(
                    subset=['from_material', 'to_material'], keep='first'
                )
                removed_count = original_count - len(config_dict['M4_ChangeoverMatrix'])
                print(f"  ğŸ”§ å·²å»é™¤ {removed_count} æ¡é‡å¤è®°å½•")
            else:
                print(f"  âœ… Changeover Matrix æ— é‡å¤å®šä¹‰")
        
        # ğŸ”§ ChangeoverDefinition é…ç½®æ ¡éªŒå’Œå»é‡
        if 'M4_ChangeoverDefinition' in config_dict and not config_dict['M4_ChangeoverDefinition'].empty:
            print(f"\nğŸ”§ æ ¡éªŒ Changeover Definition é…ç½®...")
            co_def = config_dict['M4_ChangeoverDefinition']
            
            # æ£€æŸ¥é‡å¤å®šä¹‰
            duplicates = co_def[co_def.duplicated(subset=['changeover_id', 'line'], keep=False)]
            if not duplicates.empty:
                print(f"  âš ï¸  å‘ç° {len(duplicates)} æ¡é‡å¤çš„ changeover definition å®šä¹‰")
                
                # è¯¦ç»†æ£€æŸ¥æ¯ç»„é‡å¤
                for (coid, line), group in duplicates.groupby(['changeover_id', 'line']):
                    unique_times = group['time'].unique()
                    if len(unique_times) > 1:
                        # ä¸åŒçš„ time - ä¸¥é‡é”™è¯¯
                        print(f"    âŒ ERROR: changeover_id={coid}, line={line} æœ‰ {len(unique_times)} ä¸ªä¸åŒçš„ time å€¼: {list(unique_times)}")
                    else:
                        # ç›¸åŒçš„å‚æ•° - åªæ˜¯é‡å¤
                        print(f"    âš ï¸  changeover_id={coid}, line={line} æœ‰ {len(group)} æ¡é‡å¤è®°å½• (time={unique_times[0]})")
                
                # å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰
                original_count = len(co_def)
                config_dict['M4_ChangeoverDefinition'] = co_def.drop_duplicates(
                    subset=['changeover_id', 'line'], keep='first'
                )
                removed_count = original_count - len(config_dict['M4_ChangeoverDefinition'])
                print(f"  ğŸ”§ å·²å»é™¤ {removed_count} æ¡é‡å¤è®°å½•")
            else:
                print(f"  âœ… Changeover Definition æ— é‡å¤å®šä¹‰")
        
        # Module4 é…ç½®è¡¨æ˜ å°„ï¼ˆä¸ºäº†å‘åå…¼å®¹ï¼‰
        print(f"\nğŸ”§ æ­£åœ¨æ˜ å°„ Module4 é…ç½®è¡¨...")
        module4_mappings = {
            'M4_MaterialLocationLineCfg': 'MaterialLocationLineCfg',
            'M4_LineCapacity': 'LineCapacity',
            'M4_ChangeoverMatrix': 'ChangeoverMatrix',
            'M4_ChangeoverDefinition': 'ChangeoverDefinition',
            'M4_ProductionReliability': 'ProductionReliability'
        }

        mapped_count = 0
        for original_key, mapped_key in module4_mappings.items():
            if original_key in config_dict and not config_dict[original_key].empty:
                config_dict[mapped_key] = config_dict[original_key]
                print(f"  ğŸ”§ æ˜ å°„ {original_key} â†’ {mapped_key}")
                mapped_count += 1

        if mapped_count > 0:
            print(f"âœ… å·²æ˜ å°„ {mapped_count} ä¸ª Module4 é…ç½®è¡¨")
        else:
            print(f"âœ… æ— éœ€æ˜ å°„ Module4 é…ç½®è¡¨")
        
        return config_dict
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        raise

def run_integrated_simulation(
    config_path: str,
    start_date: str,
    end_date: str,
    output_base_dir: str = "./integrated_output",
    force_restart: bool = False
):
    """
    è¿è¡Œå®Œæ•´çš„é›†æˆä»¿çœŸ
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        start_date: ä»¿çœŸå¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        end_date: ä»¿çœŸç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        force_restart: å¼ºåˆ¶ä»å¤´å¼€å§‹ï¼Œå¿½ç•¥ç»­è·‘èƒ½åŠ›
    """
    print(f"ğŸš€ å¼€å§‹é›†æˆä»¿çœŸ: {start_date} åˆ° {end_date}")
    print("=" * 60)
    
    # 1. é¢„éªŒè¯é…ç½®
    print(f"ğŸ” æ­£åœ¨è¿è¡Œä»¿çœŸå‰é…ç½®éªŒè¯...")
    validation_passed, validation_report = run_pre_simulation_validation(config_path, output_base_dir)
    
    print(f"ğŸ“ éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆ: {validation_report}")
    
    if not validation_passed:
        print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æŸ¥çœ‹éªŒè¯æŠ¥å‘Šå¹¶ä¿®å¤é”™è¯¯åå†è¿è¡Œä»¿çœŸã€‚")
        return {
            'validation_passed': False,
            'validation_report': validation_report,
            'simulation_completed': False
        }
    
    print("âœ… é…ç½®éªŒè¯é€šè¿‡ï¼Œå¼€å§‹ä»¿çœŸ...")
    
    # 2. æ£€æŸ¥ç»­è·‘èƒ½åŠ›
    actual_start_date = start_date
    is_resuming = False
    resume_info = None
    
    if force_restart:
        print(f"ğŸ”„ å¼ºåˆ¶é‡å¯æ¨¡å¼ï¼šå¿½ç•¥ä»»ä½•ç°æœ‰çŠ¶æ€ï¼Œä»å¤´å¼€å§‹")
    else:
        resume_info = check_resume_capability(output_base_dir, start_date, end_date)
        
        if resume_info.get('already_completed', False):
            print(f"ğŸ‰ ä»¿çœŸå·²å®Œæˆï¼æœ€åå¤„ç†æ—¥æœŸ: {resume_info['last_complete_date']}")
            print(f"   æ€»å…±å¤„ç†äº† {resume_info['days_completed']} å¤©")
            return {
                'validation_passed': True,
                'simulation_completed': True,
                'already_completed': True,
                'dates_processed': resume_info['days_completed'],
                'last_complete_date': resume_info['last_complete_date']
            }
        elif resume_info['can_resume']:
            print(f"ğŸ”„ æ£€æµ‹åˆ°æœªå®Œæˆçš„ä»¿çœŸï¼Œæ”¯æŒç»­è·‘:")
            print(f"   å·²å®Œæˆ: {resume_info['days_completed']} å¤© (åˆ° {resume_info['last_complete_date']})")
            print(f"   å‰©ä½™: {resume_info['days_remaining']} å¤© (ä» {resume_info['resume_from_date']} å¼€å§‹)")
            
            # æä¾›é€‰æ‹©ï¼ˆåœ¨å®é™…å®ç°ä¸­å¯ä»¥åŠ å…¥ç”¨æˆ·ç¡®è®¤ï¼‰
            print(f"   âœ… å°†ä» {resume_info['resume_from_date']} ç»§ç»­è¿è¡Œ")
            actual_start_date = resume_info['resume_from_date'] 
            is_resuming = True
        else:
            print(f"ğŸ“ æœªå‘ç°å¯ç»­è·‘çš„çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹")
    
    # 3. åˆå§‹åŒ–æ—¶é—´ç®¡ç†å™¨
    time_manager = initialize_time_manager(actual_start_date)
    
    # 3. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_base_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    orchestrator_output_dir = output_dir / "orchestrator"
    module_outputs = {
        'module1': output_dir / "module1",
        'module3': output_dir / "module3", 
        'module4': output_dir / "module4",
        'module5': output_dir / "module5",
        'module6': output_dir / "module6"
    }
    
    for module_dir in module_outputs.values():
        module_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½é…ç½®
    config_dict = load_configuration(config_path)
    
    # è®¾ç½®å…¨å±€éšæœºç§å­
    global_seed = set_module_seeds(config_dict)
    
    # åˆå§‹åŒ–Orchestrator
    print(f"\nğŸ¯ åˆå§‹åŒ–Orchestrator")
    orchestrator = create_orchestrator(
        start_date=start_date,
        output_dir=str(orchestrator_output_dir)
    )
    # è®¾ç½® open deployment çš„æ¸…ç†å¤©æ•°ï¼Œ3ä»£è¡¨ä¿ç•™3å¤©
    orchestrator.set_past_due_cleanup_grace_days(100)
    if is_resuming:
        # ç»­è·‘æ¨¡å¼ï¼šæ¢å¤çŠ¶æ€
        print(f"\nğŸ”„ ç»­è·‘æ¨¡å¼ï¼šæ¢å¤OrchestratorçŠ¶æ€")
        restore_orchestrator_state(orchestrator, resume_info['last_complete_date'], output_base_dir)
        
        # è®¾ç½®ç©ºé—´å®¹é‡ï¼ˆç»­è·‘æ—¶ä¹Ÿéœ€è¦é‡æ–°è®¾ç½®ç©ºé—´å®¹é‡é…ç½®ï¼‰
        if 'Global_SpaceCapacity' in config_dict and not config_dict['Global_SpaceCapacity'].empty:
            orchestrator.set_space_capacity(config_dict['Global_SpaceCapacity'])
    else:
        # å…¨æ–°å¼€å§‹ï¼šè®¾ç½®åˆå§‹çŠ¶æ€
        print(f"\nğŸ†• å…¨æ–°å¼€å§‹ï¼šè®¾ç½®åˆå§‹çŠ¶æ€")
        
        # è®¾ç½®åˆå§‹åº“å­˜
        if 'M1_InitialInventory' in config_dict and not config_dict['M1_InitialInventory'].empty:
            orchestrator.initialize_inventory(config_dict['M1_InitialInventory'])
        else:
            print("âš ï¸  æœªæ‰¾åˆ°åˆå§‹åº“å­˜é…ç½®ï¼Œä½¿ç”¨ç©ºåº“å­˜")
            orchestrator.initialize_inventory(pd.DataFrame(columns=['material', 'location', 'quantity']))
        
        # è®¾ç½®ç©ºé—´å®¹é‡
        if 'Global_SpaceCapacity' in config_dict and not config_dict['Global_SpaceCapacity'].empty:
            orchestrator.set_space_capacity(config_dict['Global_SpaceCapacity'])
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ç©ºé—´å®¹é‡é…ç½®")
    
    # ç”Ÿæˆä»¿çœŸæ—¥æœŸèŒƒå›´ï¼ˆä½¿ç”¨å®é™…å¼€å§‹æ—¥æœŸï¼‰
    sim_dates = pd.date_range(actual_start_date, end_date, freq='D')
    total_days = len(pd.date_range(start_date, end_date, freq='D'))
    
    if is_resuming:
        print(f"ğŸ“… ç»­è·‘æ—¥æœŸèŒƒå›´: {len(sim_dates)} å¤© (å‰©ä½™)")
        print(f"   åŸå§‹æ€»å¤©æ•°: {total_days}")
        print(f"   å·²å®Œæˆ: {resume_info['days_completed']} å¤©")
        print(f"   å‰©ä½™å¤„ç†: {len(sim_dates)} å¤©")
    else:
        print(f"ğŸ“… ä»¿çœŸæ—¥æœŸèŒƒå›´: {len(sim_dates)} å¤©")
    
    # æ¯æ—¥å¾ªç¯æ‰§è¡Œ
    all_results = {
        'module1': [],
        'module3': [],
        'module4': [], 
        'module5': [],
        'module6': []
    }
    
    for i, current_date in enumerate(sim_dates, 1):
        # è®¡ç®—å®é™…çš„æ€»è¿›åº¦ï¼ˆè€ƒè™‘ç»­è·‘æƒ…å†µï¼‰
        if is_resuming:
            actual_day_number = resume_info['days_completed'] + i
            total_original_days = total_days
            progress_info = f"ç¬¬ {actual_day_number}/{total_original_days} å¤© (ç»­è·‘ç¬¬ {i}/{len(sim_dates)} å¤©)"
        else:
            progress_info = f"ç¬¬ {i}/{len(sim_dates)} å¤©"
            
        print(f"\n{'='*20} {progress_info}: {current_date.strftime('%Y-%m-%d')} {'='*20}")
        
        # ==================== æ¯æ—¥å¼€å§‹ï¼šGRå…¥åº“å¤„ç† ====================
        try:
            print(f"\nğŸŒ… æ¯æ—¥å¼€å§‹çŠ¶æ€æ›´æ–°")
            
            # ğŸ”„ ç¬¬0æ­¥ï¼šä¿å­˜æœŸåˆåº“å­˜å¿«ç…§ï¼ˆåœ¨ä»»ä½•å˜åŠ¨ä¹‹å‰ï¼‰
            print(f"  ğŸ’¾ ä¿å­˜æœŸåˆåº“å­˜å¿«ç…§...")
            orchestrator.save_beginning_inventory(current_date.strftime('%Y-%m-%d'))
            orchestrator.cleanup_past_due_open_deployments(current_date.strftime('%Y-%m-%d'),grace_days=getattr(orchestrator, "cleanup_grace_days", 0),write_audit=True)

            # ğŸ”„ ç¬¬1æ­¥ï¼šå¤„ç†å½“æ—¥åˆ°è¾¾çš„delivery GR (in-transit â†’ inventory)
            print(f"  ğŸ“¦ å¤„ç†å½“æ—¥delivery GRåˆ°è¾¾...")
            orchestrator._process_delivery_arrivals(current_date.strftime('%Y-%m-%d'))
            
            # ğŸ”„ ç¬¬2æ­¥ï¼šå¤„ç†å†å²ç”Ÿäº§çš„å½“æ—¥å…¥åº“ (historical production â†’ inventory)
            print(f"  ğŸ­ å¤„ç†å†å²ç”Ÿäº§å½“æ—¥å…¥åº“...")
            historical_production = load_all_historical_production_plans(
                module4_output_dir=str(module_outputs['module4']),
                current_date=current_date,
                start_date=pd.to_datetime(start_date)
            )
            
            if not historical_production.empty:
                print(f"    ğŸ“¦ å½“æ—¥éœ€è¦å…¥åº“çš„å†å²ç”Ÿäº§: {len(historical_production)} æ¡è®°å½•")
                # ğŸ”§ æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
                historical_production_normalized = _normalize_identifiers(historical_production)
                orchestrator.process_module4_production(historical_production_normalized, current_date.strftime('%Y-%m-%d'))
            else:
                print(f"    ğŸ“¦ å½“æ—¥æ— å†å²ç”Ÿäº§å…¥åº“")
                
        except Exception as e:
            print(f"  âŒ æ¯æ—¥å¼€å§‹å¤„ç†å¤±è´¥: {e}")
        
        # ==================== æ¨¡å—è¿è¡Œåºåˆ— ====================
        # åˆå§‹åŒ–æ¯æ—¥æ•°æ®å˜é‡
        m1_shipments = pd.DataFrame()
        m4_production = pd.DataFrame()
        m5_deployment_df = pd.DataFrame()
        m6_delivery_df = pd.DataFrame()
        
        try:
            # ========== M1: è®¢å•ç”Ÿæˆ + ç«‹å³åº“å­˜æ‰£å‡ ==========
            print(f"\n1ï¸âƒ£ è¿è¡Œ Module1 - è®¢å•ç”Ÿæˆ")
            try:
                m1_result = module1.run_daily_order_generation(
                    config_dict=config_dict,
                    simulation_date=current_date,
                    output_dir=str(module_outputs['module1']),
                    orchestrator=orchestrator
                )
                m1_shipments = m1_result.get('shipment_df', pd.DataFrame())
                
                # ğŸ”„ ç«‹å³å¤„ç†M1 shipmentï¼Œæ‰£å‡åº“å­˜
                if not m1_shipments.empty:
                    print(f"    ğŸšš ç«‹å³å¤„ç†M1 shipmentï¼Œæ‰£å‡åº“å­˜...")
                    # ğŸ”§ æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
                    m1_shipments_normalized = _normalize_identifiers(m1_shipments)
                    orchestrator.process_module1_shipments(m1_shipments_normalized, current_date.strftime('%Y-%m-%d'))
                    print(f"    âœ… å·²æ‰£å‡ {len(m1_shipments_normalized)} ä¸ªshipmentçš„åº“å­˜")
                
                print(f"  âœ… Module1 å®Œæˆ - ç”Ÿæˆ {len(m1_result.get('orders_df', []))} ä¸ªè®¢å•, {len(m1_shipments)} ä¸ªå‘è´§")
                all_results['module1'].append(m1_result)
            except Exception as e:
                print(f"  âŒ Module1 å¤±è´¥: {e}")
                m1_shipments = pd.DataFrame()  # å¤±è´¥æ—¶ä½¿ç”¨ç©ºæ•°æ®
                # ä¸ç”¨continueï¼Œè®©åé¢çš„æ¨¡å—ç»§ç»­æ‰§è¡Œ
            
            # ========== M4: ç”Ÿäº§è®¡åˆ’ + ç«‹å³å½“æ—¥ç”Ÿäº§å…¥åº“ ==========
            print(f"\n2ï¸âƒ£ è¿è¡Œ Module4 - ç”Ÿäº§è®¡åˆ’")
            try:
                # ä½¿ç”¨é›†æˆæ¨¡å¼ç›´æ¥è°ƒç”¨ Module4 (æ”¹è¿›çš„è§£å†³æ–¹æ¡ˆ)
                m4_production = run_module4_integrated(
                    config_dict=config_dict,
                    module3_output_dir=str(module_outputs['module3']),
                    simulation_date=current_date,
                    simulation_start=pd.to_datetime(start_date),
                    output_dir=str(module_outputs['module4'])
                )
                
                # ğŸ”„ ç«‹å³å¤„ç†M4å½“æ—¥ç”Ÿäº§å…¥åº“
                if not m4_production.empty:
                    # ç­›é€‰å½“æ—¥å¯ç”¨çš„ç”Ÿäº§ (available_date = current_date)
                    daily_available = m4_production[
                        pd.to_datetime(m4_production['available_date']).dt.normalize() == current_date.normalize()
                    ]
                    
                    if not daily_available.empty:
                        print(f"    ğŸ­ ç«‹å³å¤„ç†M4å½“æ—¥ç”Ÿäº§å…¥åº“...")
                        # ğŸ”§ æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
                        daily_available_normalized = _normalize_identifiers(daily_available)
                        orchestrator.process_module4_production(daily_available_normalized, current_date.strftime('%Y-%m-%d'))
                        print(f"    âœ… å·²å…¥åº“ {len(daily_available_normalized)} æ¡å½“æ—¥ç”Ÿäº§")
                    else:
                        print(f"    ğŸ“¦ M4å½“æ—¥æ— å¯ç”¨ç”Ÿäº§å…¥åº“")
                
                print(f"  âœ… Module4 å®Œæˆ - ç”Ÿæˆç”Ÿäº§è®¡åˆ’: {len(m4_production)} æ¡è®°å½•")
                all_results['module4'].append({'production_df': m4_production})
            except Exception as e:
                print(f"  âŒ Module4 å¤±è´¥: {e}")
                m4_production = pd.DataFrame()  # å¤±è´¥æ—¶ä½¿ç”¨ç©ºæ•°æ®
            
            # ========== M5: éƒ¨ç½²è®¡åˆ’ ==========
            print(f"\n3ï¸âƒ£ è¿è¡Œ Module5 - éƒ¨ç½²è®¡åˆ’")
            try:
                # å¯ç”¨æ€§èƒ½åˆ†æ
                with PerformanceProfiler("Module5", output_dir=Path(output_base_dir) / "performance", enabled=True):
                    m5_result = module5.main(
                        # é›†æˆæ¨¡å¼å‚æ•°
                        config_dict=config_dict,
                        module1_output_dir=str(module_outputs['module1']),
                        module4_output_path=str(module_outputs['module4'] / f"Module4Output_{current_date.strftime('%Y%m%d')}.xlsx"),
                        orchestrator=orchestrator,
                        current_date=current_date.strftime('%Y-%m-%d'),
                        # è¾“å‡ºè·¯å¾„
                        output_path=str(module_outputs['module5'] / f"Module5Output_{current_date.strftime('%Y%m%d')}.xlsx")
                    )
                
                # è·å–éƒ¨ç½²è®¡åˆ’æ•°æ®
                if m5_result and 'deployment_plan' in m5_result:
                    deployment_plan_df = m5_result['deployment_plan']
                    # print(f"    ğŸ” Module5è¿”å›çš„éƒ¨ç½²è®¡åˆ’: {len(deployment_plan_df)} æ¡è®°å½•")
                    
                    if not deployment_plan_df.empty:
                        # print(f"    ğŸ“Š éƒ¨ç½²è®¡åˆ’ç¤ºä¾‹æ•°æ®:")
                        # print(f"    åˆ—å: {list(deployment_plan_df.columns)}")
                        # if len(deployment_plan_df) > 0:
                        #     first_row = deployment_plan_df.iloc[0]
                        #     print(f"    ç¬¬ä¸€è¡Œæ•°æ®: {dict(first_row)}")
                        #     if 'deployed_qty_invCon' in deployment_plan_df.columns:
                        #         qty_stats = deployment_plan_df['deployed_qty_invCon'].describe()
                        #         print(f"    deployed_qty_invConç»Ÿè®¡: {qty_stats}")
                        
                        # è¿‡æ»¤å‡ºæœ‰å®é™…éƒ¨ç½²é‡çš„è®¡åˆ’ï¼Œæ’é™¤è‡ªå¾ªç¯ï¼ˆsending=receivingï¼‰
                        valid_deployment = deployment_plan_df[
                            (deployment_plan_df['deployed_qty_invCon'] > 0) & 
                            (deployment_plan_df['deployed_qty_invCon'].notna()) &
                            (deployment_plan_df['sending'] != deployment_plan_df['receiving'])  # æ’é™¤è‡ªå¾ªç¯
                        ].copy()
                        
                        print(f"    ğŸ¯ æœ‰æ•ˆéƒ¨ç½²è®¡åˆ’: {len(valid_deployment)}/{len(deployment_plan_df)} æ¡")
                        
                        if not valid_deployment.empty:
                            # æ£€æŸ¥æ˜¯å¦å·²æœ‰deployed_qtyåˆ—ï¼Œé¿å…é‡å¤
                            if 'deployed_qty' in valid_deployment.columns:
                                # å¦‚æœå·²æœ‰deployed_qtyåˆ—ï¼Œç›´æ¥ä½¿ç”¨
                                m5_deployment_df = valid_deployment[[
                                    'material', 'sending', 'receiving', 'date', 'deployed_qty', 'demand_element'
                                ]].rename(columns={'date': 'planned_deployment_date'})
                            else:
                                # é‡å‘½ååˆ—ä»¥åŒ¹é…orchestratoræœŸæœ›çš„æ ¼å¼
                                m5_deployment_df = valid_deployment.rename(columns={
                                    'date': 'planned_deployment_date',
                                    'deployed_qty_invCon': 'deployed_qty'
                                })[['material', 'sending', 'receiving', 'planned_deployment_date', 'deployed_qty', 'demand_element']]
                            
                            # ğŸ”§ æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
                            m5_deployment_df = _normalize_identifiers(m5_deployment_df)
                            
                            # print(f"    âœ… æœ€ç»ˆä¼ é€’ç»™Orchestratorçš„æ•°æ®: {len(m5_deployment_df)} æ¡")
                            # if len(m5_deployment_df) > 0:
                            #     final_qty_stats = m5_deployment_df['deployed_qty'].describe()
                            #     print(f"    deployed_qtyç»Ÿè®¡: {final_qty_stats}")
                            #     print(f"    æ•°æ®ç±»å‹: material={m5_deployment_df['material'].dtype}, sending={m5_deployment_df['sending'].dtype}, receiving={m5_deployment_df['receiving'].dtype}")
                            
                            # ğŸ”„ ç«‹å³å¤„ç†M5 deploymentï¼Œæ›´æ–°open deployment
                            print(f"    ğŸ“¦ ç«‹å³å¤„ç†M5 deploymentï¼Œæ›´æ–°open deployment...")
                            orchestrator.process_module5_deployment(m5_deployment_df, current_date.strftime('%Y-%m-%d'))
                            print(f"    âœ… å·²æ›´æ–° {len(m5_deployment_df)} æ¡éƒ¨ç½²è®¡åˆ’åˆ°open deployment")
                            
                            print(f"  âœ… Module5 å®Œæˆ - ç”Ÿæˆ {len(m5_deployment_df)} æ¡æœ‰æ•ˆéƒ¨ç½²è®¡åˆ’")
                        else:
                            print(f"  âœ… Module5 å®Œæˆ - æ— æœ‰æ•ˆéƒ¨ç½²è®¡åˆ’")
                    else:
                        print(f"  âœ… Module5 å®Œæˆ - éƒ¨ç½²è®¡åˆ’ä¸ºç©º")
                else:
                    print(f"  âœ… Module5 å®Œæˆ - æ— è¿”å›ç»“æœ")
                
                all_results['module5'].append(m5_result)
            except Exception as e:
                print(f"  âŒ Module5 å¤±è´¥: {e}")
                # ä¸ç”¨continueï¼Œè®©åé¢çš„æ¨¡å—ç»§ç»­æ‰§è¡Œ
                m5_deployment_df = pd.DataFrame()  # è®¾ç½®é»˜è®¤å€¼
            
            # ========== M6: ç‰©æµæ‰§è¡Œ + ç«‹å³å¤šçŠ¶æ€æ›´æ–° ==========
            print(f"\n4ï¸âƒ£ è¿è¡Œ Module6 - ç‰©æµæ‰§è¡Œ")
            try:
                m6_result = module6.run_daily_physical_flow(
                    config_dict=config_dict,
                    orchestrator=orchestrator,
                    current_date=current_date,
                    output_dir=str(module_outputs['module6']),
                    max_wait_days=30,
                    random_seed=config_dict.get('M6_RandomSeed', 42)  # ä½¿ç”¨ç»Ÿä¸€ç§å­
                )
                
                # è·å–äº¤ä»˜è®¡åˆ’æ•°æ®
                if m6_result and 'delivery_plan' in m6_result:
                    m6_delivery_df = m6_result.get('delivery_plan', pd.DataFrame())
                    
                    # ğŸ”„ ç«‹å³å¤„ç†M6 deliveryï¼Œæ›´æ–°å¤šä¸ªçŠ¶æ€
                    if not m6_delivery_df.empty:
                        print(f"    ğŸš› ç«‹å³å¤„ç†M6 deliveryï¼Œæ›´æ–°åº“å­˜/open deployment/in-transit...")
                        # ğŸ”§ æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§
                        m6_delivery_df_normalized = _normalize_identifiers(m6_delivery_df)
                        orchestrator.process_module6_delivery(m6_delivery_df_normalized, current_date.strftime('%Y-%m-%d'))
                        print(f"    âœ… å·²å¤„ç† {len(m6_delivery_df_normalized)} æ¡deliveryè®¡åˆ’ï¼Œæ›´æ–°ç›¸å…³çŠ¶æ€")
                    
                    print(f"  âœ… Module6 å®Œæˆ - ç”Ÿæˆ {len(m6_delivery_df)} æ¡äº¤ä»˜è®¡åˆ’")
                else:
                    print(f"  âœ… Module6 å®Œæˆ - æ— äº¤ä»˜è®¡åˆ’")
                    m6_delivery_df = pd.DataFrame()
                
                all_results['module6'].append(m6_result)
            except Exception as e:
                print(f"  âŒ Module6 å¤±è´¥: {e}")
                # ä¸ç”¨continueï¼Œè®©åé¢çš„æ¨¡å—ç»§ç»­æ‰§è¡Œ
                m6_delivery_df = pd.DataFrame()  # è®¾ç½®é»˜è®¤å€¼
            
            # ========== M3: å‡€éœ€æ±‚è®¡ç®— ==========
            print(f"\n5ï¸âƒ£ è¿è¡Œ Module3 - å‡€éœ€æ±‚è®¡ç®—")
            try:
                # å¯ç”¨æ€§èƒ½åˆ†æ
                with PerformanceProfiler("Module3", output_dir=Path(output_base_dir) / "performance", enabled=True):
                    m3_result = module3.run_integrated_mode(
                        module1_output_dir=str(module_outputs['module1']),
                        orchestrator=orchestrator,
                        config_dict=config_dict,
                        start_date=current_date.strftime('%Y-%m-%d'),
                        end_date=current_date.strftime('%Y-%m-%d'),
                        output_dir=str(module_outputs['module3'])
                    )
                print(f"  âœ… Module3 å®Œæˆ")
                all_results['module3'].append(m3_result)
            except Exception as e:
                print(f"  âŒ Module3 å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                # ä¸ç”¨continueï¼Œè®©ä»–ç»§ç»­æ‰§è¡Œ
            
            # ==================== æ¯æ—¥ç»“æŸï¼šä¿å­˜çŠ¶æ€ ====================
            print(f"\nğŸ’¾ æ¯æ—¥ç»“æŸçŠ¶æ€ä¿å­˜")
            try:
                # ä¿å­˜æœŸæœ«åº“å­˜å¿«ç…§ï¼ˆåœ¨ä¿å­˜çŠ¶æ€ä¹‹å‰ï¼‰
                orchestrator.save_ending_inventory(current_date.strftime('%Y-%m-%d'))
                
                # è¾“å‡ºè¯¦ç»†çš„åº“å­˜å˜åŠ¨è®°å½•ç”¨äºè°ƒè¯•
                orchestrator.output_daily_inventory_summary(current_date.strftime('%Y-%m-%d'))
                
                # ç›´æ¥ä¿å­˜æ¯æ—¥çŠ¶æ€ï¼ŒçŠ¶æ€æ›´æ–°å·²åœ¨å„æ¨¡å—è¿è¡Œåå®æ—¶å®Œæˆ
                orchestrator.save_daily_state(current_date.strftime('%Y-%m-%d'))
                
                # è·å–å½“æ—¥ç»Ÿè®¡
                stats = orchestrator.get_summary_statistics(current_date.strftime('%Y-%m-%d'))
                print(f"  ğŸ“Š å½“æ—¥ç»Ÿè®¡: {stats}")
                print(f"  ğŸ’¾ Orchestrator çŠ¶æ€å·²ä¿å­˜")
                
            except Exception as e:
                print(f"  âŒ æ¯æ—¥çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
                # ä¸ç”¨continueï¼Œè®©ä»–ç»§ç»­åˆ°ä¸‹ä¸€å¤©
            
            print(f"âœ… ç¬¬ {i} å¤©å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¬¬ {i} å¤©å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print(f"\nğŸ“Š ä»¿çœŸå®ŒæˆæŠ¥å‘Š")
    print("=" * 60)
    print(f"ä»¿çœŸæœŸé—´: {start_date} åˆ° {end_date} ({len(sim_dates)} å¤©)")
    print(f"è¾“å‡ºç›®å½•: {output_base_dir}")
    
    for module_name, results in all_results.items():
        print(f"{module_name.upper()}: {len(results)} å¤©æˆåŠŸå¤„ç†")
    
    # è¿›è¡Œåº“å­˜å¹³è¡¡æ£€æŸ¥
    print(f"\nğŸ” æ­£åœ¨è¿›è¡Œåº“å­˜å¹³è¡¡æ£€æŸ¥...")
    validation_manager = ValidationManager(str(output_dir))
    inventory_checker = InventoryBalanceChecker(validation_manager, orchestrator)
    balance_passed = inventory_checker.validate_inventory_consistency(start_date, end_date)
    
    if balance_passed:
        print("âœ… åº“å­˜å¹³è¡¡æ£€æŸ¥é€šè¿‡")
    else:
        print("âš ï¸  åº“å­˜å¹³è¡¡æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹éªŒè¯æŠ¥å‘Š")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    summary_generator = SummaryReportGenerator(str(output_dir), config_dict)
    summary_reports = summary_generator.generate_all_reports(start_date, end_date)
    
    # å†™å…¥åº“å­˜å¹³è¡¡æ£€æŸ¥æŠ¥å‘Š
    balance_report_path = validation_manager.write_report()
    
    # è·å–æœ€ç»ˆOrchestratorç»Ÿè®¡
    final_date = sim_dates[-1].strftime('%Y-%m-%d')
    final_stats = orchestrator.get_summary_statistics(final_date)
    print(f"\nğŸ¯ æœ€ç»ˆOrchestratorçŠ¶æ€:")
    for key, value in final_stats.items():
        print(f"  {key}: {value}")
    
    if is_resuming:
        total_processed = resume_info['days_completed'] + len(sim_dates)
        print(f"\nğŸ‰ ç»­è·‘ä»¿çœŸå®Œæˆ!")
        print(f"   æœ¬æ¬¡å¤„ç†: {len(sim_dates)} å¤©")
        print(f"   æ€»å…±å®Œæˆ: {total_processed} å¤©")
    else:
        total_processed = len(sim_dates)
        print(f"\nğŸ‰ é›†æˆä»¿çœŸå®Œæˆ!")
        print(f"   æ€»å…±å¤„ç†: {total_processed} å¤©")
    
    return {
        'validation_passed': True,
        'simulation_completed': True,
        'is_resuming': is_resuming,
        'dates_processed_this_run': len(sim_dates),
        'total_dates_processed': total_processed if is_resuming else len(sim_dates),
        'resume_info': resume_info if is_resuming else None,
        'results': all_results,
        'final_stats': final_stats,
        'output_directory': output_base_dir,
        'validation_report': validation_report,
        'balance_check_passed': balance_passed,
        'balance_report': balance_report_path,
        'summary_reports': summary_reports
    }

def main():
    """ä¸»å‡½æ•° - ç‹¬ç«‹æ‰§è¡Œé›†æˆä»¿çœŸ"""
    # é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡æŒ‡å®šï¼‰
    import argparse
    
    parser = argparse.ArgumentParser(description="è¿è¡Œä¾›åº”é“¾é›†æˆä»¿çœŸ")
    parser.add_argument("--config", "-c", 
                       default="./config/integration_config.json",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./config/integration_config.json)")
    parser.add_argument("--start-date", "-s", 
                       default="2024-01-01",
                       help="ä»¿çœŸå¼€å§‹æ—¥æœŸ (é»˜è®¤: 2024-01-01)")
    parser.add_argument("--end-date", "-e", 
                       default="2024-01-05",
                       help="ä»¿çœŸç»“æŸæ—¥æœŸ (é»˜è®¤: 2024-01-03)")
    parser.add_argument("--output", "-o", 
                       default=None,
                       help="è¾“å‡ºç›®å½• (é»˜è®¤: æ ¹æ®é…ç½®æ–‡ä»¶åç”Ÿæˆ)")
    parser.add_argument("--force-restart", 
                       action="store_true",
                       help="å¼ºåˆ¶ä»å¤´å¼€å§‹ï¼Œå¿½ç•¥ç»­è·‘èƒ½åŠ› (é»˜è®¤: False)")
    parser.add_argument("--check-resume", 
                       action="store_true",
                       help="ä»…æ£€æŸ¥ç»­è·‘çŠ¶æ€ï¼Œä¸æ‰§è¡Œä»¿çœŸ (é»˜è®¤: False)")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("è¯·æä¾›æœ‰æ•ˆçš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œæˆ–ä½¿ç”¨æµ‹è¯•è„šæœ¬ç”Ÿæˆé…ç½®")
        sys.exit(1)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œæ ¹æ®é…ç½®æ–‡ä»¶åç”Ÿæˆ
    if args.output is None:
        config_name = os.path.splitext(os.path.basename(args.config))[0]
        args.output = f"./{config_name}_output"
        print(f"ğŸ’« ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {args.output}")
    
    # å¤„ç†ç»­è·‘æ£€æŸ¥é€‰é¡¹
    if args.check_resume:
        print(f"ğŸ” æ£€æŸ¥ç»­è·‘çŠ¶æ€...")
        resume_info = check_resume_capability(args.output, args.start_date, args.end_date)
        
        print(f"\nğŸ“Š ç»­è·‘çŠ¶æ€æŠ¥å‘Š:")
        print(f"  è¾“å‡ºç›®å½•: {args.output}")
        print(f"  åŸå§‹æ—¥æœŸèŒƒå›´: {args.start_date} åˆ° {args.end_date}")
        
        if resume_info.get('already_completed', False):
            print(f"  âœ… ä»¿çœŸå·²å®Œæˆï¼")
            print(f"     æœ€åå¤„ç†æ—¥æœŸ: {resume_info['last_complete_date']}")
            print(f"     æ€»å¤„ç†å¤©æ•°: {resume_info['days_completed']}")
        elif resume_info['can_resume']:
            print(f"  ğŸ”„ å¯ä»¥ç»­è·‘ï¼")
            print(f"     å·²å®Œæˆ: {resume_info['days_completed']} å¤© (åˆ° {resume_info['last_complete_date']})")
            print(f"     å‰©ä½™: {resume_info['days_remaining']} å¤© (ä» {resume_info['resume_from_date']} å¼€å§‹)")
        else:
            print(f"  ğŸ“ æ— æ³•ç»­è·‘ï¼Œéœ€è¦ä»å¤´å¼€å§‹")
            print(f"     éœ€è¦å¤„ç†: {resume_info['days_remaining']} å¤©")
        
        return  # ä»…æ£€æŸ¥ï¼Œä¸æ‰§è¡Œ
    
    # å¤„ç†å¼ºåˆ¶é‡å¯é€‰é¡¹
    if args.force_restart:
        print(f"ğŸ”„ å¼ºåˆ¶é‡å¯æ¨¡å¼ï¼šå°†ä»å¤´å¼€å§‹ï¼Œå¿½ç•¥ä»»ä½•ç°æœ‰çŠ¶æ€")
        # å¯ä»¥è€ƒè™‘åˆ é™¤ç°æœ‰è¾“å‡ºç›®å½•ï¼Œæˆ–è€…ä¿®æ”¹run_integrated_simulationå‡½æ•°æ¥æ”¯æŒå¼ºåˆ¶é‡å¯
        # è¿™é‡Œæš‚æ—¶é€šè¿‡æ·»åŠ æ ‡å¿—æ¥å®ç°
    
    try:
        # æ·»åŠ å¼ºåˆ¶é‡å¯å‚æ•°ï¼ˆéœ€è¦ä¿®æ”¹run_integrated_simulationå‡½æ•°ç­¾åï¼‰
        result = run_integrated_simulation(
            config_path=args.config,
            start_date=args.start_date,
            end_date=args.end_date,
            output_base_dir=args.output,
            force_restart=args.force_restart  # æ–°å¢å‚æ•°
        )
        
        print(f"\nâœ… ä»¿çœŸç»“æœ:")
        if result.get('is_resuming', False):
            print(f"  ç»­è·‘æ¨¡å¼: æ˜¯")
            print(f"  æœ¬æ¬¡å¤„ç†å¤©æ•°: {result.get('dates_processed_this_run', 0)}")
            print(f"  æ€»å¤„ç†å¤©æ•°: {result.get('total_dates_processed', 0)}")
        else:
            print(f"  å…¨æ–°è¿è¡Œ: æ˜¯")  
            print(f"  å¤„ç†å¤©æ•°: {result.get('dates_processed_this_run', 0)}")
        print(f"  è¾“å‡ºç›®å½•: {result.get('output_directory', 'Unknown')}")
        
        if result.get('already_completed', False):
            print(f"  ğŸ“ æ³¨æ„: ä»¿çœŸä¹‹å‰å·²å®Œæˆï¼Œæ— éœ€å¤„ç†")
        
    except Exception as e:
        print(f"âŒ é›†æˆä»¿çœŸå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()