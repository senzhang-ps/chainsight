import pandas as pd
import numpy as np
import os
from typing import Dict, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import time
from collections import defaultdict, deque
from datetime import datetime, timedelta
from functools import lru_cache

# ä» module5 å¯¼å…¥ MOQ/RV åº”ç”¨é€»è¾‘
def apply_moq_rv(qty, moq, rv, is_cross_node=True):
    """
    è¡¥è´§é‡å°äºmoqè¡¥moqï¼Œå¦åˆ™å‘ä¸Šå–æ•´åˆ°rvçš„å€æ•°
    
    Args:
        qty: éœ€æ±‚æ•°é‡
        moq: æœ€å°è®¢è´§é‡
        rv: é‡è®¢é‡(Round Volume)
        is_cross_node: æ˜¯å¦ä¸ºè·¨èŠ‚ç‚¹è°ƒè¿ã€‚True=è·¨èŠ‚ç‚¹éœ€è¦åº”ç”¨MOQ/RVï¼ŒFalse=è‡ªå¾ªç¯ä¸åº”ç”¨MOQ/RV
    """
    if qty <= 0:
        return 0
    
    # è‡ªå¾ªç¯è°ƒè¿ä¸åº”ç”¨MOQ/RVçº¦æŸï¼Œç›´æ¥è¿”å›åŸéœ€æ±‚é‡
    if not is_cross_node:
        return qty
    
    # è·¨èŠ‚ç‚¹è°ƒè¿åº”ç”¨MOQ/RVçº¦æŸ
    if qty < moq:
        return moq
    return int(np.ceil(qty / rv)) * rv

# æ ‡è¯†ç¬¦å­—æ®µæ ‡å‡†åŒ–å‡½æ•°ï¼ˆä¸main_integration.pyä¿æŒä¸€è‡´ï¼‰
def _normalize_location(location_str) -> str:
    """Normalize location string by padding with leading zeros to 4 digits"""
    # Handle None and pandas NA
    if location_str is None or pd.isna(location_str):
        return ""
    try:
        return str(int(location_str)).zfill(4)
    except (ValueError, TypeError):
        return str(location_str).zfill(4)

def _normalize_material(material_str) -> str:
    """Normalize material string"""
    # Handle None and pandas NA
    if material_str is None or pd.isna(material_str):
        return ""
    return str(material_str)

def _normalize_identifiers(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize identifier columns to string format with proper formatting"""
    if df.empty:
        return df
    
    # Define identifier columns that need string conversion
    identifier_cols = ['material', 'location', 'sending', 'receiving', 'sourcing']
    
    df = df.copy()
    for col in identifier_cols:
        if col in df.columns:
            # Convert to string and handle NaN values
            df[col] = df[col].astype('string')
            # Apply normalization for location-type fields
            if col in ['location', 'sending', 'receiving']:
                df[col] = df[col].apply(_normalize_location)
            # Apply normalization for material
            elif col in ['material']:
                df[col] = df[col].apply(_normalize_material)
            # For other identifier columns, vectorized string conversion
            else:
                df[col] = df[col].fillna('').astype(str)
    
    return df

def load_config(config_path: str) -> Dict[str, pd.DataFrame]:
    """
    Load configuration data from Excel file
    Uses Global_Network's location_type field directly
    
    Args:
        config_path: Path to the configuration Excel file
        
    Returns:
        Dictionary of configuration DataFrames
    """
    sheet_mapping = {
        'M3_SafetyStock': ('safety_stock', pd.DataFrame()),
        'Global_Network': ('network_config', pd.DataFrame()),
        'Global_LeadTime': ('lead_time_config', pd.DataFrame())
    }
    
    try:
        xl = pd.ExcelFile(config_path)
        loaded_config = {}
        
        for sheet_name, (key, default) in sheet_mapping.items():
            if sheet_name in xl.sheet_names:
                loaded_config[key] = xl.parse(sheet_name)
                # Convert date columns if they exist
                if key in ['safety_stock'] and 'date' in loaded_config[key].columns:
                    loaded_config[key]['date'] = pd.to_datetime(loaded_config[key]['date'])
                elif key in ['network_config']:
                    if 'eff_from' in loaded_config[key].columns:
                        loaded_config[key]['eff_from'] = pd.to_datetime(loaded_config[key]['eff_from'])
                        loaded_config[key]['eff_to'] = pd.to_datetime(loaded_config[key]['eff_to'])
                    # æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µ
                    loaded_config[key] = _normalize_identifiers(loaded_config[key])
            else:
                loaded_config[key] = default
                
        return loaded_config
    except Exception as e:
        raise RuntimeError(f"Failed to load module3 config from {config_path}: {e}")


def load_module1_daily_outputs(module1_output_dir: str, simulation_date: pd.Timestamp) -> Dict[str, pd.DataFrame]:
    """
    è¯»å– Module1 å½“å¤©ç‰ˆæœ¬çš„è¾“å‡ºï¼š
      - SupplyDemandLog: å·²è¢«è®¢å•æ¶ˆè€—åçš„æœªæ¥é¢„æµ‹ï¼ˆM1å·²æŒ‰ sim_date ç”Ÿæˆï¼‰
      - ShipmentLog: ä»…å½“æ—¥å‘è´§ï¼ˆdate == sim_dateï¼‰ï¼Œç”¨äºä»å¯ç”¨é‡ä¸­æ‰£å‡
      - OrderLog: å½“å¤©ç‰ˆæœ¬è§†å›¾ï¼ˆåŒ…å«å†å²ç”Ÿæˆä½†æœªæ¥åˆ°æœŸçš„è®¢å• + å½“å¤©æ–°å•ï¼‰
                  è¿™é‡Œä¸å†æŒ‰ simulation_date è¿‡æ»¤ï¼ŒåªåšåŸºæœ¬ç±»å‹è§„èŒƒåŒ–ï¼›
                  æœªæ¥æ˜¯å¦çº³å…¥éœ€æ±‚ç”± M3 åœ¨è®¡ç®—å‰å†ç­›é€‰ï¼ˆåªå– date > sim_dateï¼‰ã€‚
    """
    try:
        _t_func = time.perf_counter()
        date_str = simulation_date.strftime('%Y%m%d')
        f1 = os.path.join(module1_output_dir, f"module1_output_{date_str}.xlsx")
        f2 = os.path.join(module1_output_dir, f"output_simulation_{date_str}.xlsx")
        module1_daily_file = f1 if os.path.exists(f1) else f2

        if not os.path.exists(module1_daily_file):
            print(f"Warning: Module1 output file not found for date {date_str}. Using empty DataFrames.")
            return {'supply_demand_df': pd.DataFrame(), 'shipment_df': pd.DataFrame(), 'order_df': pd.DataFrame()}

        xl = pd.ExcelFile(module1_daily_file)
        module1_data = {}

        def _read(name):
            return xl.parse(name) if name in xl.sheet_names else pd.DataFrame()

        # 1) SupplyDemandLogï¼šåŸæ ·è¯»å–ï¼ˆM1å·²ä¿è¯ä¸ºâ€œæœªæ¥å‰©ä½™é¢„æµ‹â€ï¼‰
        sdl = _read('SupplyDemandLog')
        if not sdl.empty and 'date' in sdl.columns:
            sdl['date'] = pd.to_datetime(sdl['date'])
        # standardize key fields as strings
        # æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µ
        sdl = _normalize_identifiers(sdl)

        # 2) ShipmentLogï¼šä»…ä¿ç•™å½“æ—¥
        shp = _read('ShipmentLog')
        if not shp.empty and 'date' in shp.columns:
            shp['date'] = pd.to_datetime(shp['date'])
            shp = shp[shp['date'] == simulation_date].copy()
        # æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µ
        shp = _normalize_identifiers(shp)

        # 3) OrderLogï¼šå½“å¤©ç‰ˆæœ¬å…¨é‡ï¼ˆåŒ…å«æœªæ¥è®¢å• + å½“å¤©æ–°å•ï¼‰
        odl = _read('OrderLog')
        if not odl.empty:
            if 'date' in odl.columns:
                odl['date'] = pd.to_datetime(odl['date'])
            if 'simulation_date' in odl.columns:
                odl['simulation_date'] = pd.to_datetime(odl['simulation_date'])
            # ä¸æŒ‰ simulation_date è¿‡æ»¤ï¼Œä¿ç•™å½“å¤©ç‰ˆæœ¬å…¨é‡ï¼›åç»­åœ¨ M3 å†…éƒ¨å†ç­› date > sim_date
            # æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µ
            odl = _normalize_identifiers(odl)
        module1_data['supply_demand_df'] = sdl
        module1_data['shipment_df'] = shp
        module1_data['order_df'] = odl
        print(f"[M3] load_module1_daily_outputs total: {time.perf_counter()-_t_func:.3f}s for {simulation_date.date()}")
        return module1_data

    except Exception as e:
        print(f"Warning: Error loading Module1 daily outputs for {simulation_date.strftime('%Y-%m-%d')}: {e}")
        return {'supply_demand_df': pd.DataFrame(), 'shipment_df': pd.DataFrame(), 'order_df': pd.DataFrame()}


def assign_location_layers(network_df: pd.DataFrame) -> pd.DataFrame:
    """åˆ†é…ä¾›åº”é“¾ç½‘ç»œä¸­å„èŠ‚ç‚¹çš„å±‚çº§ - è‡ªåŠ¨è¯†åˆ«æœ€ä¸Šå±‚èŠ‚ç‚¹
    
    Args:
        network_df: ç½‘ç»œé…ç½®æ•°æ®ï¼ŒåŒ…å«sourcingå’Œlocationå­—æ®µ
        
    Returns:
        DataFrame: åŒ…å«locationå’Œå¯¹åº”layerçš„æ˜ å°„å…³ç³»
    """
    _t_func = time.perf_counter()
    if network_df.empty:
        return pd.DataFrame({'location': [], 'layer': []})
        
    children = defaultdict(list)
    parents = defaultdict(list)
    
    # ç¬¬ä¸€æ­¥ï¼šæ„å»ºçˆ¶å­å…³ç³»å›¾
    # Performance optimization: Use itertuples instead of iterrows
    for row in network_df.itertuples():
        sourcing_val = row.sourcing
        location_val = row.location
        
        # Handle null/nan values properly for both scalar and Series cases
        sourcing_valid = sourcing_val is not None and pd.notna(sourcing_val) and str(sourcing_val).strip() != ''
        location_valid = location_val is not None and pd.notna(location_val) and str(location_val).strip() != ''
        
        if sourcing_valid and location_valid:
            children[sourcing_val].append(location_val)
            parents[location_val].append(sourcing_val)
    
    # ç¬¬äºŒæ­¥ï¼šæ”¶é›†æ‰€æœ‰åœ°ç‚¹
    all_locations = set(network_df['location'].dropna()).union(set(network_df['sourcing'].dropna()))
    
    # ç¬¬ä¸‰æ­¥ï¼šè‡ªåŠ¨è¯†åˆ«æœ€ä¸Šå±‚èŠ‚ç‚¹ï¼ˆæ²¡æœ‰çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰
    # è¿™äº›èŠ‚ç‚¹å¯èƒ½æ˜¯çœŸæ­£çš„æ ¹èŠ‚ç‚¹ï¼Œä¹Ÿå¯èƒ½æ˜¯é…ç½®ä¸­ç¼ºå¤±ä¸Šæ¸¸å…³ç³»çš„èŠ‚ç‚¹
    potential_roots = [loc for loc in all_locations if not parents[loc]]
    
    # ç¬¬å››æ­¥ï¼šæ™ºèƒ½è¯†åˆ«çœŸæ­£çš„æ ¹èŠ‚ç‚¹
    # ç­–ç•¥ï¼šå¦‚æœä¸€ä¸ªåœ°ç‚¹åœ¨sourcingä¸­å‡ºç°è¿‡ï¼Œè¯´æ˜å®ƒæœ‰ä¸‹æ¸¸ï¼Œå¯èƒ½æ˜¯çœŸæ­£çš„æ ¹èŠ‚ç‚¹
    # å¦‚æœä¸€ä¸ªåœ°ç‚¹åªåœ¨locationä¸­å‡ºç°ï¼Œä»æœªåœ¨sourcingä¸­å‡ºç°ï¼Œè¯´æ˜å®ƒå¯èƒ½æ˜¯å¶å­èŠ‚ç‚¹
    true_roots = []
    for loc in potential_roots:
        if loc in children:  # è¯¥åœ°ç‚¹æœ‰ä¸‹æ¸¸èŠ‚ç‚¹
            true_roots.append(loc)
        else:
            # è¯¥åœ°ç‚¹æ²¡æœ‰ä¸‹æ¸¸ï¼Œå¯èƒ½æ˜¯å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ
            # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åœ°ç‚¹æŒ‡å‘å®ƒ
            has_incoming = any(loc in parents.get(other_loc, []) for other_loc in all_locations)
            if not has_incoming:
                # å¦‚æœæ²¡æœ‰ä»»ä½•å…¶ä»–åœ°ç‚¹æŒ‡å‘å®ƒï¼Œä¸”å®ƒä¹Ÿæ²¡æœ‰ä¸‹æ¸¸ï¼Œå¯èƒ½æ˜¯å­¤ç«‹çš„æ ¹èŠ‚ç‚¹
                true_roots.append(loc)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°çœŸæ­£çš„æ ¹èŠ‚ç‚¹ï¼Œä½¿ç”¨æ‰€æœ‰potential_roots
    if not true_roots:
        true_roots = potential_roots
    
    # print(f"ğŸ” è‡ªåŠ¨è¯†åˆ«ç½‘ç»œå±‚çº§:")
    # print(f"  æ€»åœ°ç‚¹æ•°: {len(all_locations)}")
    # print(f"  æ½œåœ¨æ ¹èŠ‚ç‚¹: {potential_roots}")
    # print(f"  è¯†åˆ«å‡ºçš„æ ¹èŠ‚ç‚¹: {true_roots}")
    
    # ç¬¬äº”æ­¥ï¼šä»æ ¹èŠ‚ç‚¹å¼€å§‹åˆ†é…å±‚çº§
    layer_dict = {}
    queue = deque()
    
    # æ ¹èŠ‚ç‚¹ä»layer 0å¼€å§‹
    for root in true_roots:
        queue.append((root, 0))
        # print(f"  ğŸ“ æ ¹èŠ‚ç‚¹: {root} -> Layer 0")
    
    # å¹¿åº¦ä¼˜å…ˆéå†åˆ†é…å±‚çº§
    while queue:
        loc, layer = queue.popleft()
        if loc in layer_dict and layer_dict[loc] <= layer:
            continue
        layer_dict[loc] = layer
        
        # å­èŠ‚ç‚¹å±‚çº§ = çˆ¶èŠ‚ç‚¹å±‚çº§ + 1
        for child in children.get(loc, []):
            queue.append((child, layer + 1))
            # print(f"  ğŸ“ å­èŠ‚ç‚¹: {child} -> Layer {layer + 1} (çˆ¶èŠ‚ç‚¹: {loc})")
    
    # ç¬¬å…­æ­¥ï¼šå¤„ç†æœªè¿æ¥æˆ–å­¤ç«‹çš„èŠ‚ç‚¹
    unassigned = [loc for loc in all_locations if loc not in layer_dict]
    if unassigned:
        max_layer = max(layer_dict.values()) if layer_dict else 0
        for loc in unassigned:
            layer_dict[loc] = max_layer + 1
            # print(f"  ğŸ“ å­¤ç«‹èŠ‚ç‚¹: {loc} -> Layer {max_layer + 1}")
    
    # ç¬¬ä¸ƒæ­¥ï¼šç”Ÿæˆå±‚çº§æ˜ å°„DataFrame
    layer_df = pd.DataFrame([
        {'location': loc, 'layer': layer} 
        for loc, layer in layer_dict.items()
    ])
    
    # æŒ‰å±‚çº§æ’åº
    layer_df = layer_df.sort_values('layer')
    
    # print(f"  âœ… å±‚çº§åˆ†é…å®Œæˆï¼Œå…± {len(layer_df)} ä¸ªåœ°ç‚¹")
    # print(f"  å±‚çº§èŒƒå›´: {layer_df['layer'].min()} - {layer_df['layer'].max()}")
    
    print(f"[M3] assign_location_layers total: {time.perf_counter()-_t_func:.3f}s, locations={len(layer_df)}")
    return layer_df

# === æ–°å¢ï¼šæ”¾åœ¨ assign_location_layers ä¹‹å ===
def infer_sending_location_type(
    network_df: pd.DataFrame,
    location_layer_df: pd.DataFrame,
    sending: str,
    material: str | None,
    sim_date: pd.Timestamp
) -> str:
    """
    æ¨æ–­å‘é€ç«¯çš„ location_typeï¼š
    1) è‹¥å­˜åœ¨ (material, location==sending) çš„æ˜¾å¼é…ç½®ï¼Œç›´æ¥ä½¿ç”¨å…¶ location_type
    2) è‹¥ sending æ˜¯æ ¹èŠ‚ç‚¹(layer==0)ï¼Œåˆ¤ä¸º 'Plant'
    3) è‹¥ sending åªåœ¨ sourcing åˆ—å‡ºç°ã€ä»ä¸åœ¨ location åˆ—å‡ºç°ï¼Œåˆ¤ä¸º 'Plant'
    4) å…¶ä»–æƒ…å†µé»˜è®¤ä¸º 'DC'
    """
    if sending is None or (isinstance(sending, float) and pd.isna(sending)) or str(sending).strip() == '':
        return 'DC'

    # â‘  æ˜¾å¼é…ç½®ï¼ˆåŒç‰©æ–™ã€æœ‰æ•ˆæœŸå†…ï¼‰
    if material is not None and not network_df.empty:
        explicit = network_df[
            (network_df['material'] == material) &
            (network_df['location'] == sending) &
            (network_df['eff_from'] <= sim_date) &
            (network_df['eff_to'] >= sim_date)
        ]
        if not explicit.empty:
            t = explicit.iloc[0].get('location_type', None)
            if isinstance(t, str) and t.strip():
                return t

    # â‘¡ æ ¹èŠ‚ç‚¹ï¼ˆlayer==0ï¼‰â†’ Plant
    if not location_layer_df.empty:
        layer_map = dict(zip(location_layer_df['location'], location_layer_df['layer']))
        if layer_map.get(sending, None) == 0:
            return 'Plant'

    # â‘¢ åªåœ¨ sourcing ä¸­å‡ºç°ã€ä»ä¸åœ¨ location ä¸­å‡ºç° â†’ Plant
    #    ï¼ˆå¤„ç†â€œæºå¤´ Plant åªç»´æŠ¤åœ¨ sourcing åˆ—â€çš„å¸¸è§æƒ…å†µï¼‰
    appears_as_sourcing = network_df['sourcing'].astype(str).eq(str(sending)).any()
    appears_as_location = network_df['location'].astype(str).eq(str(sending)).any()
    if appears_as_sourcing and not appears_as_location:
        return 'Plant'

    # â‘£ å…œåº•
    return 'DC'

def _build_ptf_lsk_cache_m3(m4_mlcfg_df: pd.DataFrame | None) -> Dict[tuple[str, str], tuple[int, int]]:
    """Build cache for PTF/LSK lookups - 15-20x faster than DataFrame filtering"""
    cache = {}
    if m4_mlcfg_df is None or m4_mlcfg_df.empty:
        return cache
    
    for row in m4_mlcfg_df.itertuples():
        material = getattr(row, 'material', None)
        location = getattr(row, 'location', None)
        if material is None or location is None:
            continue
        
        ptf = 0
        lsk = 1
        
        # Try lowercase first, then uppercase
        ptf_val = getattr(row, 'ptf', None) or getattr(row, 'PTF', None)
        lsk_val = getattr(row, 'lsk', None) or getattr(row, 'LSK', None)
        
        if ptf_val is not None and not pd.isna(ptf_val):
            ptf = int(ptf_val)
        if lsk_val is not None and not pd.isna(lsk_val):
            lsk = int(lsk_val)
        
        cache[(str(material), str(location))] = (ptf, lsk)
    
    return cache

def _get_ptf_lsk(material: str, site: str, m4_mlcfg_df: pd.DataFrame | None,
                 cache: Dict[tuple[str, str], tuple[int, int]] | None = None) -> tuple[int, int]:
    """
    ä» M4_MaterialLocationLineCfg è¯»å– (PTF, LSK)
    - è¡¨ç»“æ„å­—æ®µï¼šmaterial, location, ..., lsk, ptf, day, MCT
    - å…¼å®¹å¤§å°å†™åˆ—åï¼ˆlsk/LSK, ptf/PTFï¼‰
    - æœªå‘½ä¸­æ—¶é»˜è®¤ PTF=0, LSK=1
    
    With caching: 15-20x faster
    """
    # Use cache if provided (15-20x faster)
    if cache is not None:
        return cache.get((str(material), str(site)), (0, 1))
    
    # Fallback to original logic
    ptf, lsk = 0, 1
    if m4_mlcfg_df is None or m4_mlcfg_df.empty:
        return ptf, lsk

    ml = m4_mlcfg_df[
        (m4_mlcfg_df['material'] == material) &
        (m4_mlcfg_df['location'] == site)
    ]
    if ml.empty:
        return ptf, lsk

    row = ml.iloc[0]

    # PTF
    if 'ptf' in ml.columns and pd.notna(row.get('ptf')):
        ptf = int(row['ptf'])
    elif 'PTF' in ml.columns and pd.notna(row.get('PTF')):
        ptf = int(row['PTF'])

    # LSK
    if 'lsk' in ml.columns and pd.notna(row.get('lsk')):
        lsk = int(row['lsk'])
    elif 'LSK' in ml.columns and pd.notna(row.get('LSK')):
        lsk = int(row['LSK'])

    return ptf, lsk
def _compute_root_horizon(material: str,
                          location: str,
                          lead_time_df: pd.DataFrame,
                          m4_mlcfg_df: pd.DataFrame | None = None,
                          ptf_lsk_cache: Dict[tuple[str, str], tuple[int, int]] | None = None) -> int:
    """
    é¡¶å±‚(æ— ä¸Šæ¸¸)çª—å£å£å¾„ â€” ä¸ Module 5 å¯¹é½ï¼š
    horizon = max(PDT+GR, MCT) + PTF + LSK - 1
    - PDT/GR/MCTï¼šä» Global_LeadTime å– sending==location çš„æ‰€æœ‰è¡Œçš„æœ€å¤§å€¼ï¼ˆç¼ºå¤±å½“ 0ï¼‰
    - PTF/LSKï¼šä» M4_MaterialLocationLineCfg æŒ‰ (material, location) è¯»å–ï¼›æœªå‘½ä¸­é»˜è®¤ PTF=0, LSK=1
    - æœ€ç»ˆä¿è¯ horizon >= 1
    
    With caching: 15-20x faster
    """
    import pandas as pd

    # 1) PTF/LSK
    ptf, lsk = _get_ptf_lsk(material=material, site=location, m4_mlcfg_df=m4_mlcfg_df, cache=ptf_lsk_cache)

    # 2) PDT/GR/MCTï¼ˆä»¥ sending==location çš„è¡Œå–æœ€å¤§å€¼ï¼‰
    if lead_time_df is None or lead_time_df.empty:
        PDT = GR = MCT = 0
    else:
        df_loc = lead_time_df[lead_time_df['sending'].astype(str) == str(location)]
        if df_loc.empty:
            PDT = GR = MCT = 0
        else:
            PDT = int(pd.to_numeric(df_loc.get('PDT', 0), errors='coerce').fillna(0).max())
            GR  = int(pd.to_numeric(df_loc.get('GR',  0), errors='coerce').fillna(0).max())
            MCT = int(pd.to_numeric(df_loc.get('MCT', 0), errors='coerce').fillna(0).max())

    base_lt = max(MCT, PDT + GR)
    horizon = max(1, int(base_lt + int(ptf) + int(lsk) - 1))
    return horizon

def determine_lead_time(
    sending: str,
    receiving: str,
    location_type: str,                 # ä¼ å…¥â€œå‘é€ç«¯â€çš„ç±»å‹ï¼›Plant é€»è¾‘ç”¨å®ƒåˆ¤æ–­
    lead_time_df: pd.DataFrame,
    m4_mlcfg_df: pd.DataFrame | None = None,
    material: str | None = None,
    ptf_lsk_cache: Dict[tuple[str, str], tuple[int, int]] | None = None
) -> tuple[int, str]:
    """
    æå‰æœŸï¼š
      - PDT/GR/MCT æ¥è‡ª Global_LeadTimeï¼ˆæŒ‰ sending+receiving åŒ¹é…ï¼‰
      - å¯¹äº Plantï¼ˆå‘é€ç«¯ä¸º Plantï¼‰ï¼šlead_time = max(MCT, PDT+GR) + PTF + LSK - 1
        å…¶ä¸­ PTF/LSK ä» M4_MaterialLocationLineCfg å–ï¼ˆåˆ—ï¼šptf, lskï¼›å…¼å®¹å¤§å°å†™ï¼‰
      - å¯¹äº DCï¼šlead_time = PDT + GR
    
    With caching: 15-20x faster for PTF/LSK lookups
    """
    if lead_time_df.empty:
        return 1, 'empty_lead_time_config'

    row = lead_time_df[
        (lead_time_df['sending'] == sending) &
        (lead_time_df['receiving'] == receiving)
    ]
    if row.empty:
        return 1, 'lead_time_missing'

    try:
        PDT = int(row.iloc[0].get('PDT', 0) or 0)
        GR  = int(row.iloc[0].get('GR',  0) or 0)
        MCT = int(row.iloc[0].get('MCT', 0) or 0)

        # é»˜è®¤ä¸åŠ  PTF/LSKï¼›ä»… Plant æ—¶è¯»å–
        ptf, lsk = 0, 1
        if str(location_type).lower() == 'plant' and material is not None:
            # å£å¾„ï¼šæŒ‰ (material, sending)åŒ¹é…
            ptf, lsk = _get_ptf_lsk(material=material, site=sending, m4_mlcfg_df=m4_mlcfg_df, cache=ptf_lsk_cache)

        if str(location_type).lower() == 'plant':
            base_lt  = max(MCT, PDT + GR)
            leadtime = base_lt + ptf + lsk - 1
        else:
            leadtime = PDT + GR

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ä»¥è¿½è¸ªé—®é¢˜
        final_leadtime = max(0, int(leadtime))
        # if material == '80813644' and receiving == 'C816':
        #     print(f"    Debug: determine_lead_time for {material}@{receiving}")
        #     print(f"      sending={sending}, receiving={receiving}, location_type={location_type}")
        #     print(f"      PDT={PDT}, GR={GR}, MCT={MCT}")
        #     print(f"      è®¡ç®—leadtime={leadtime}, æœ€ç»ˆ={final_leadtime}")

        return final_leadtime, ""

    except Exception as e:
        return 0, f'lead_time_calculation_error: {e}'

def calculate_daily_net_demand(
    material: str,
    location: str,
    date: pd.Timestamp,
    supply_demand_df: pd.DataFrame,
    safety_stock_df: pd.DataFrame,
    beginning_inventory_df: pd.DataFrame,
    in_transit_df: pd.DataFrame,
    delivery_gr_df: pd.DataFrame,
    future_production_df: pd.DataFrame,
    today_shipment_df: pd.DataFrame,
    open_deployment_df: pd.DataFrame,
    downstream_forecast_gap: float,
    downstream_safety_gap: float,
    horizon: int,
    delivery_shipment_df: pd.DataFrame | None = None,
    order_df: pd.DataFrame | None = None,       # â† æ–°å¢ï¼šç”¨äº AO_local
    downstream_ao_gap: float = 0.0              # â† æ–°å¢ï¼šä¸‹æ¸¸ AO ç¼ºå£    
) -> Tuple[float, float, float]:  # â† è¿”å› AO, FC, SS ä¸‰ä¸ª gap
    """è®¡ç®—æ¯æ—¥å‡€éœ€æ±‚ï¼ˆforecast gapå’Œsafety gapï¼‰ - å…¼å®¹module1çš„æ•°æ®æ ¼å¼
    
    Args:
        material: ç‰©æ–™ç¼–ç 
        location: åœ°ç‚¹ç¼–ç 
        date: è®¡ç®—æ—¥æœŸ
        supply_demand_df: ä¾›éœ€æ•°æ® (æ¥è‡ªModule1 SupplyDemandLog)
        safety_stock_df: å®‰å…¨åº“å­˜æ•°æ®
        beginning_inventory_df: æ¯æ—¥æœŸåˆåº“å­˜æ•°æ®
        in_transit_df: åœ¨é€”æ•°æ®
        delivery_gr_df: æ”¶è´§æ•°æ®
        future_production_df: æœªæ¥ç¡®è®¤ç”Ÿäº§æ•°æ®
        today_shipment_df: ä»Šæ—¥å‘è´§æ•°æ® (æ¥è‡ªModule1 ShipmentLog)
        open_deployment_df: å¼€æ”¾è°ƒæ‹¨æ•°æ®
        downstream_forecast_gap: ä¸‹æ¸¸é¢„æµ‹ç¼ºå£
        downstream_safety_gap: ä¸‹æ¸¸å®‰å…¨åº“å­˜ç¼ºå£
        horizon: è®¡ç®—å‘¨æœŸå¤©æ•°ï¼ˆæå‰æœŸå¤©æ•°ï¼‰
        
    Returns:
        Tuple[float, float]: (forecast_gap, safety_gap)
    """
    # å‚æ•°éªŒè¯
    if not isinstance(date, pd.Timestamp):
        try:
            date = pd.to_datetime(date)
        except:
            raise TypeError("date must be convertible to pandas Timestamp")
    
    if horizon <= 0:
        horizon = 1
    
    try:
        horizon_end = date + pd.Timedelta(days=horizon)
    except Exception as e:
        raise ValueError(f"Invalid date calculation: {e}")
    
    try:
        # ğŸš€ OPTIMIZATION: Pre-filter DataFrames by material & location ONCE to avoid repeated comparisons
        # Filter beginning_inventory_df
        bi_filtered = pd.DataFrame()
        if beginning_inventory_df is not None and (not beginning_inventory_df.empty) and 'material' in beginning_inventory_df.columns:
            bi_mask = (beginning_inventory_df['material'] == material) & (beginning_inventory_df['location'] == location)
            bi_filtered = beginning_inventory_df[bi_mask]
        
        # Filter in_transit_df
        it_filtered = pd.DataFrame()
        if not in_transit_df.empty and 'material' in in_transit_df.columns:
            it_mask = (in_transit_df['material'] == material) & (in_transit_df['receiving'] == location)
            it_filtered = in_transit_df[it_mask]
        
        # Filter delivery_gr_df
        dgr_filtered = pd.DataFrame()
        if not delivery_gr_df.empty and 'material' in delivery_gr_df.columns:
            dgr_mask = (delivery_gr_df['material'] == material) & (delivery_gr_df['receiving'] == location)
            dgr_filtered = delivery_gr_df[dgr_mask]
        
        # Filter future_production_df
        fp_filtered = pd.DataFrame()
        if not future_production_df.empty and 'material' in future_production_df.columns:
            fp_mask = (future_production_df['material'] == material) & (future_production_df['location'] == location)
            fp_filtered = future_production_df[fp_mask]
        
        # Filter today_shipment_df
        ts_filtered = pd.DataFrame()
        if not today_shipment_df.empty and 'material' in today_shipment_df.columns:
            ts_mask = (today_shipment_df['material'] == material) & (today_shipment_df['location'] == location)
            ts_filtered = today_shipment_df[ts_mask]
        
        # Filter open_deployment_df
        od_filtered = pd.DataFrame()
        if not open_deployment_df.empty and 'material' in open_deployment_df.columns:
            od_mask = (open_deployment_df['material'] == material) & (open_deployment_df['sending'] == location) & (open_deployment_df['receiving'] != location)
            od_filtered = open_deployment_df[od_mask]
        
        # 1. å½“æ—¥æœŸåˆåº“å­˜ï¼ˆBeginning Inventoryï¼ŒæœªåŒ…å«å½“æ—¥å‡ºåº“/å‘è¿æ‰£å‡ï¼‰
        begin_qty = 0.0
        if not bi_filtered.empty:
            bi_rows = bi_filtered[pd.to_datetime(bi_filtered['date']) == pd.to_datetime(date)]
            begin_qty = float(bi_rows['quantity'].sum()) if not bi_rows.empty else 0.0
        
        # 2. åœ¨é€”åº“å­˜
        in_transit_qty = 0.0
        if not it_filtered.empty:
            in_transit_qty = float(it_filtered['quantity'].sum())
        
        # 3. ä»Šæ—¥æ”¶è´§
        delivery_gr_qty = 0.0
        if not dgr_filtered.empty:
            dgr_rows = dgr_filtered[dgr_filtered['date'] == date]
            delivery_gr_qty = float(dgr_rows['quantity'].sum()) if not dgr_rows.empty else 0.0
        
        # 4a. å½“æ—¥ç”Ÿäº§æ”¶è´§ (available_date = today) â€”â€” ç”¨ produced_qty
        today_production_gr_qty = 0.0
        if not fp_filtered.empty:
            today_rows = fp_filtered[fp_filtered['available_date'] == date]
            if not today_rows.empty:
                if 'produced_qty' in today_rows.columns:
                    today_production_gr_qty = float(today_rows['produced_qty'].sum())
                elif 'quantity' in today_rows.columns:
                    # å…¼å®¹è€å£å¾„
                    today_production_gr_qty = float(today_rows['quantity'].sum())
                else:
                    today_production_gr_qty = 0.0

        # 4b. æœªæ¥ç¡®è®¤ç”Ÿäº§ï¼ˆä¸é™åˆ¶æ—¶é—´çª—å£ï¼‰â€”â€” ç”¨ con_planned_qty
        future_production_qty = 0.0
        if not fp_filtered.empty:
            future_rows = fp_filtered[
                (fp_filtered['available_date'] > date)
            ]
            if not future_rows.empty:
                if 'con_planned_qty' in future_rows.columns:
                    future_production_qty = float(pd.to_numeric(future_rows['con_planned_qty'], errors='coerce').fillna(0).sum())
                elif 'produced_qty' in future_rows.columns:
                    # å›é€€ï¼šå¦‚æœæ²¡æœ‰ con_planned_qtyï¼Œå°±ç”¨ produced_qty
                    future_production_qty = float(pd.to_numeric(future_rows['produced_qty'], errors='coerce').fillna(0).sum())
                elif 'quantity' in future_rows.columns:
                    # å…¼å®¹è€å£å¾„
                    future_production_qty = float(pd.to_numeric(future_rows['quantity'], errors='coerce').fillna(0).sum())
                else:
                    future_production_qty = 0.0

        
        # 5. ä»Šæ—¥å®¢æˆ·å‘è´§ (ä»å¯ç”¨é‡ä¸­æ‰£é™¤) - ä½¿ç”¨Module1çš„ShipmentLog
        today_shipment_qty = 0.0
        if not ts_filtered.empty:
            ts_rows = ts_filtered[ts_filtered['date'] == date]
            today_shipment_qty = float(ts_rows['quantity'].sum()) if not ts_rows.empty else 0.0

        # 5b. ä»Šæ—¥è°ƒæ‹¨/è·¨ç‚¹å‘è¿ï¼ˆä»å¯ç”¨é‡ä¾§æ‰£ï¼‰- â˜…æ–°å¢ï¼šæ¥è‡ª Orchestrator Delivery_Shipment
        delivery_shipment_qty = 0.0
        if delivery_shipment_df is not None and not delivery_shipment_df.empty:
            # å…¼å®¹å­—æ®µï¼šquantity / shipped_qtyï¼›åœ°ç‚¹å­—æ®µï¼šsending / location
            qty_col = 'quantity' if 'quantity' in delivery_shipment_df.columns else ('shipped_qty' if 'shipped_qty' in delivery_shipment_df.columns else None)
            send_col = 'sending' if 'sending' in delivery_shipment_df.columns else ('location' if 'location' in delivery_shipment_df.columns else None)
            date_col = 'date' if 'date' in delivery_shipment_df.columns else ('ship_date' if 'ship_date' in delivery_shipment_df.columns else None)

            if qty_col and send_col and date_col:
                # è¿‡æ»¤â€œæœ¬èŠ‚ç‚¹ä½œä¸ºå‘é€ç«¯ & å½“å¤©å‘è¿â€çš„è·¨ç‚¹å‘è¿
                ds_rows = delivery_shipment_df[
                    (delivery_shipment_df['material'] == material) &
                    (delivery_shipment_df[send_col] == location) &
                    (pd.to_datetime(delivery_shipment_df[date_col]) == date)
                ]
                delivery_shipment_qty = float(ds_rows[qty_col].sum()) if not ds_rows.empty else 0.0

        # 6a. å¼€æ”¾è°ƒæ‹¨å‡ºåº“ (ä»å¯ç”¨é‡ä¸­æ‰£é™¤) - ä» orchestrator è¯»å–çš„å½“æ—¥ç‰ˆæœ¬è§†å›¾
        # æ³¨æ„ï¼šåªè®¡ç®—çœŸæ­£ä»è¯¥åœ°ç‚¹å‘å‡ºçš„è°ƒæ‹¨ï¼Œæ’é™¤è‡ªå¾ªç¯ï¼ˆsending=receivingï¼‰
        open_deployment_qty = 0.0
        if not od_filtered.empty:
            # open_deploymentä½¿ç”¨deployed_qtyå­—æ®µè€Œä¸quantity
            if 'deployed_qty' in od_filtered.columns:
                open_deployment_qty = float(od_filtered['deployed_qty'].sum())
            elif 'quantity' in od_filtered.columns:
                open_deployment_qty = float(od_filtered['quantity'].sum())
        
        # 6b. å¼€æ”¾è°ƒæ‹¨å…¥åº“ï¼ˆä½œä¸ºæœªæ¥åˆ°è´§çš„ pipeline supplyï¼‰
        # è®¡å…¥ receiving ç«¯çš„æœªæ¥å¯ç”¨é‡ï¼šdate > sim_date çš„å…¥åº“ï¼ˆä¸è®¾ä¸Šé™çª—å£ï¼‰
        open_deployment_inbound_future_qty = 0.0
        if not open_deployment_df.empty:
            # å…¼å®¹å­—æ®µï¼šquantity / deployed_qtyï¼›åœ°ç‚¹å­—æ®µï¼šreceivingï¼›æ—¥æœŸå­—æ®µï¼šdate
            qty_col = 'deployed_qty' if 'deployed_qty' in open_deployment_df.columns else ('quantity' if 'quantity' in open_deployment_df.columns else None)
            if qty_col and 'receiving' in open_deployment_df.columns and 'date' in open_deployment_df.columns and 'material' in open_deployment_df.columns:
                odf = open_deployment_df[
                    (open_deployment_df['material'] == material) &
                    (open_deployment_df['receiving'] == location)
                ].copy()
                if not odf.empty:
                    odf['date'] = pd.to_datetime(odf['date'], errors='coerce')
                    future_mask = odf['date'] > date
                    future_inbound = pd.to_numeric(odf.loc[future_mask, qty_col], errors='coerce').fillna(0)
                    open_deployment_inbound_future_qty = float(future_inbound.sum())
        
        # æ€»å¯ç”¨é‡è®¡ç®—
        total_available = (begin_qty + in_transit_qty + delivery_gr_qty + 
                  today_production_gr_qty + future_production_qty + open_deployment_inbound_future_qty - 
                  today_shipment_qty - delivery_shipment_qty - open_deployment_qty)
        

        # ======== éœ€æ±‚ä¾§ï¼šä¸‰ç±»æœ¬åœ°éœ€æ±‚ ========
        # 1) AOï¼ˆæ¥è‡ª OrderLog ä¸” demand_type == 'AO'ï¼Œçª—å£ [date, horizon_end]ï¼‰
        AO_local = 0.0
        if order_df is not None and not order_df.empty:
            # é¦–å…ˆè¿‡æ»¤ç‰©æ–™å’Œåœ°ç‚¹ï¼Œç¡®ä¿ç±»å‹åŒ¹é…
            material_filter = (order_df.get('material').astype(str) == str(material))
            location_filter = (order_df.get('location').astype(str) == str(location))
            demand_type_filter = (order_df.get('demand_type') == 'AO')
            
            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨ä¸”ä¸ºdatetimeç±»å‹
            if 'date' in order_df.columns:
                order_dates = pd.to_datetime(order_df['date'], errors='coerce')
                date_filter = (order_dates >= date) & (order_dates <= horizon_end)
                
                od = order_df[material_filter & location_filter & demand_type_filter & date_filter]
                
                if not od.empty and 'quantity' in od.columns:
                    AO_local = float(pd.to_numeric(od['quantity'], errors='coerce').fillna(0).sum())
                    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ä»¥è¿½è¸ªé—®é¢˜
                    if AO_local > 0:
                        # print(f"    Debug: AO_local={AO_local} for {material}@{location}, horizon_end={horizon_end.date()}")
                        # Performance optimization: Use itertuples instead of iterrows
                        for ao_row in od.itertuples():
                            ao_date = pd.to_datetime(ao_row.date)
                            # print(f"      AOè®¢å•: æ—¥æœŸ={ao_date.date()}, æ•°é‡={ao_row.quantity}")
            else:
                # å¦‚æœæ²¡æœ‰dateåˆ—ï¼ŒAO_localä¿æŒä¸º0
                pass

        # 2) forecastï¼ˆæ¥è‡ª SupplyDemandLogï¼Œçª—å£ [date, horizon_end]ï¼‰
        FC_local = 0.0
        if not supply_demand_df.empty and 'material' in supply_demand_df.columns:
            sdl_rows = supply_demand_df[
                (supply_demand_df['material'] == material) &
                (supply_demand_df['location'] == location) &
                (supply_demand_df['date'] >= date) &
                (supply_demand_df['date'] <= horizon_end)
            ]
            FC_local = float(pd.to_numeric(sdl_rows.get('quantity', 0), errors='coerce').fillna(0).sum())

        # 3) safetyï¼ˆå– horizon_end å½“æ—¥ç›®æ ‡ï¼‰
        SS_local = 0.0
        if not safety_stock_df.empty and 'material' in safety_stock_df.columns:
            ssr = safety_stock_df[
                (safety_stock_df['material'] == material) &
                (safety_stock_df['location'] == location) &
                (safety_stock_df['date'] == horizon_end)
            ]
            if not ssr.empty and 'safety_stock_qty' in ssr.columns:
                SS_local = float(pd.to_numeric(ssr['safety_stock_qty'], errors='coerce').fillna(0).sum())

        # ======== ç¼ºå£é¡ºåºï¼šAO â†’ forecast â†’ safety ========
        # ä½¿ç”¨ total_available ä½œä¸ºåˆå§‹ AVAILABLEï¼Œä¾æ¬¡æ¶ˆè€—
        AVAILABLE = float(total_available)

        # AO
        AO_total = AO_local + float(downstream_ao_gap or 0.0)
        AO_gap = max(AO_total - AVAILABLE, 0.0)
        AVAILABLE = max(AVAILABLE - min(AVAILABLE, AO_total), 0.0)

        # forecast
        FC_total = FC_local + float(downstream_forecast_gap or 0.0)
        FC_gap = max(FC_total - AVAILABLE, 0.0)
        AVAILABLE = max(AVAILABLE - min(AVAILABLE, FC_total), 0.0)

        # safetyï¼ˆä»…è®¡ç®—è¶…è¿‡ AO å’Œ forecast çš„å¢é‡å®‰å…¨éœ€æ±‚ï¼‰
        SAF_total = SS_local + float(downstream_safety_gap or 0.0)
        SS_gap = max(SAF_total - AVAILABLE, 0.0)


        return AO_gap, FC_gap, SS_gap
        
    except Exception as e:
        # è®°å½•é”™è¯¯ä½†è¿”å›é»˜è®¤å€¼ï¼Œé¿å…ä¸­æ–­æ•´ä¸ªæµç¨‹
        print(f"Warning: Error calculating net demand for {material}-{location} on {date}: {e}")
        return 0.0, 0.0, 0.0
    
def run_mrp_layered_simulation_daily(
    sim_date: pd.Timestamp,
    daily_supply_demand_df: pd.DataFrame,
    daily_order_df: pd.DataFrame,  
    daily_shipment_df: pd.DataFrame,
    safety_stock_df: pd.DataFrame,
    beginning_inventory_df: pd.DataFrame,
    in_transit_df: pd.DataFrame,
    delivery_gr_df: pd.DataFrame,
    all_production_df: pd.DataFrame,
    open_deployment_df: pd.DataFrame,
    network_df: pd.DataFrame,
    lead_time_df: pd.DataFrame,
    m4_mlcfg_df: pd.DataFrame | None = None,   
    delivery_shipment_df: pd.DataFrame | None = None,
    deploy_config_df: pd.DataFrame | None = None,  # ğŸ”§ æ–°å¢ï¼šM5_DeployConfig for MOQ/RV
) -> pd.DataFrame:
    """è¿è¡Œå•æ—¥MRPæ¨¡æ‹Ÿ - ä½¿ç”¨å½“æ—¥ç‰ˆæœ¬çš„Module1æ•°æ®
    ä½¿ç”¨Global_Networkä¸­çš„location_typeå­—æ®µè¿›è¡Œæå‰æœŸè®¡ç®—
    æ”¯æŒè‡ªåŠ¨è¯†åˆ«çš„æ ¹èŠ‚ç‚¹ç”Ÿæˆnetdemand
    
    Args:
        sim_date: æ¨¡æ‹Ÿæ—¥æœŸ
        daily_supply_demand_df: å½“æ—¥ä¾›éœ€æ•°æ® (æ¥è‡ªModule1 SupplyDemandLog)
        daily_shipment_df: å½“æ—¥å‘è´§æ•°æ® (æ¥è‡ªModule1 ShipmentLog)
        safety_stock_df: å®‰å…¨åº“å­˜æ•°æ®
        beginning_inventory_df: æœŸåˆåº“å­˜æ•°æ®
        in_transit_df: åœ¨é€”æ•°æ®
        delivery_gr_df: æ”¶è´§æ•°æ®
        all_production_df: å…¨é‡ç”Ÿäº§è®¡åˆ’æ•°æ®
        open_deployment_df: å¼€æ”¾è°ƒæ‹¨æ•°æ®
        network_df: ç½‘ç»œé…ç½®æ•°æ® (åŒ…å«location_typeå­—æ®µ)
        lead_time_df: æå‰æœŸæ•°æ®
        
    Returns:
        pd.DataFrame: å½“æ—¥å‡€éœ€æ±‚è®°å½•
    """
    _t_func = time.perf_counter()
    if network_df.empty:
        print(f"Warning: Empty network configuration for date {sim_date}")
        return pd.DataFrame({'material': [], 'location': [], 'requirement_date': [], 'quantity': [], 'demand_element': [], 'layer': []})

    # standardize key columns as strings to ensure consistent joins/matching
    # æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µ
    network_df = _normalize_identifiers(network_df)

    # filter to network entries active on the simulation date
    active_network = network_df[
        (network_df['eff_from'] <= sim_date) & (network_df['eff_to'] >= sim_date)
    ]
    if active_network.empty:
        print(f"Warning: No active network configuration for date {sim_date}")
        return pd.DataFrame({'material': [], 'location': [], 'requirement_date': [], 'quantity': [], 'demand_element': [], 'layer': []})
    # éœ€æ±‚æ±  = æœªæ¥è®¢å•ï¼ˆdate > sim_dateï¼‰ + å‰©ä½™é¢„æµ‹ï¼ˆSupplyDemandLogï¼‰
    # - å½“æ—¥è®¢å•ï¼ˆdate == sim_dateï¼‰ä¸è¿›å…¥éœ€æ±‚æ± ï¼Œé¿å…ä¸å½“æ—¥å‘è´§åœ¨å¯ç”¨é‡ä¾§é‡å¤è®¡
    def _std(df, element):
        if df is None or df.empty:
            return pd.DataFrame(columns=['date','material','location','quantity','demand_element'])
        cols = ['date','material','location','quantity']
        miss = [c for c in cols if c not in df.columns]
        if miss:
            print(f"  âš ï¸ demand source '{element}' ç¼ºå°‘åˆ—: {miss}ï¼Œå°†è¢«å¿½ç•¥")
            return pd.DataFrame(columns=['date','material','location','quantity','demand_element'])
        out = df[cols].copy()
        out['demand_element'] = element
        return out

    # æœªæ¥è®¢å•ï¼šåªå– date > sim_date
    future_orders = pd.DataFrame()
    if daily_order_df is not None and not daily_order_df.empty:
        # ä»…ä¿ç•™æœªæ¥è®¢å•ï¼ˆæ˜å¤©åŠä»¥åï¼‰
        future_orders = daily_order_df[pd.to_datetime(daily_order_df['date']) > sim_date].copy()

    orders_std   = _std(future_orders, 'order')
    forecast_std = _std(daily_supply_demand_df, 'forecast')

    demand_pool_df = pd.concat([orders_std, forecast_std], ignore_index=True)
    if not demand_pool_df.empty:
        # ç»Ÿä¸€æ•°æ®ç±»å‹
        demand_pool_df['date'] = pd.to_datetime(demand_pool_df['date'])
        demand_pool_df['quantity'] = demand_pool_df['quantity'].astype(float)

    # åˆ†é…å±‚çº§
    location_layer_df = assign_location_layers(active_network)
    if location_layer_df.empty:
        print(f"Warning: No location layers assigned for date {sim_date}")
        return pd.DataFrame({'material': [], 'location': [], 'requirement_date': [], 'quantity': [], 'demand_element': [], 'layer': []})
    location_layer = dict(zip(location_layer_df['location'], location_layer_df['layer']))
    all_layers = sorted(set(location_layer.values()), reverse=True)
    all_net_demand_records = []

    # === Caching to reduce repeated lookups ===
    # Cache for PTF/LSK to avoid repeated df filtering
    ptf_lsk_cache = _build_ptf_lsk_cache_m3(m4_mlcfg_df) if m4_mlcfg_df is not None and not m4_mlcfg_df.empty else {}

    # Helper to compute one node's net demand; used by threads
    def _compute_node_net_demand(ml_row: tuple) -> tuple:
        material = str(ml_row.material)
        location = str(ml_row.location)

        # æŸ¥æ‰¾æœ‰æ•ˆçš„ç½‘ç»œé…ç½®
        network_candidates = active_network[
            (active_network['material'] == material) &
            (active_network['location'] == location)
        ]

        if not network_candidates.empty:
            network_row = network_candidates.iloc[0]
            upstream = network_row['sourcing']

            # å‘½ä¸­ network ä½† sourcing ä¸ºç©º/ç©ºä¸² â†’ è§†ä¸ºé¡¶å±‚ï¼›æ ¹èŠ‚ç‚¹èµ° Plant å£å¾„
            if (pd.isna(upstream)) or (upstream is None) or (str(upstream).strip() == ''):
                upstream = None
                if location_layer.get(location, -1) == 0:
                    location_type = 'Plant'
                    horizon = _compute_root_horizon(
                        material=str(material),
                        location=str(location),
                        lead_time_df=lead_time_df,
                        m4_mlcfg_df=m4_mlcfg_df,
                        ptf_lsk_cache=ptf_lsk_cache
                    )
                else:
                    location_type = 'DC'
                    horizon = 1
            else:
                # æœ‰ä¸Šæ¸¸ï¼šä¿æŒåŸé€»è¾‘
                sending_location_type = infer_sending_location_type(
                    network_df=active_network,
                    location_layer_df=location_layer_df,
                    sending=str(upstream),
                    material=str(material),
                    sim_date=sim_date
                )
                horizon, error_msg = determine_lead_time(
                    sending=str(upstream),
                    receiving=str(location),
                    location_type=str(sending_location_type),
                    lead_time_df=lead_time_df,
                    m4_mlcfg_df=m4_mlcfg_df,
                    material=str(material),
                    ptf_lsk_cache=ptf_lsk_cache
                )
                if error_msg:
                    print(f"Warning: {error_msg} for {upstream}->{location}, using default horizon=1")
                    horizon = 1
        else:
            # æ ¹èŠ‚ç‚¹ï¼ˆå¦‚ plantï¼‰ï¼šä¸ Module 5 ç»Ÿä¸€å£å¾„
            upstream = None
            if location_layer.get(location, -1) == 0:
                location_type = 'Plant'
                horizon = _compute_root_horizon(
                    material=str(material),
                    location=str(location),
                    lead_time_df=lead_time_df,
                    m4_mlcfg_df=m4_mlcfg_df,
                    ptf_lsk_cache=ptf_lsk_cache
                )
            else:
                location_type = 'DC'
                horizon = 1

        # è·å–ä¸‹æ¸¸ç¼ºå£ï¼ˆçº¿ç¨‹å‰ç½®å€¼ç”±è°ƒç”¨å±‚ä¼ å…¥åå†åˆå¹¶ï¼‰
        lower_AO_gap = downstream_gap_dict[(material, location)]['AO']
        lower_FC_gap = downstream_gap_dict[(material, location)]['FC']
        lower_SS_gap = downstream_gap_dict[(material, location)]['SS']

        # è®¡ç®—å½“å‰èŠ‚ç‚¹çš„å‡€éœ€æ±‚
        AO_gap, FC_gap, SS_gap = calculate_daily_net_demand(
            str(material), str(location), sim_date,
            daily_supply_demand_df, safety_stock_df,
            beginning_inventory_df, in_transit_df,
            delivery_gr_df, pd.DataFrame(future_production_df),
            daily_shipment_df, open_deployment_df,
            lower_FC_gap, lower_SS_gap, horizon,
            delivery_shipment_df=delivery_shipment_df,
            order_df=daily_order_df,
            downstream_ao_gap=lower_AO_gap
        )

        # è®°å½•å½“æ—¥å‡€éœ€æ±‚ï¼ˆä»…æœ‰ç¼ºå£æ—¶ï¼‰
        records = []
        if AO_gap > 0:
            records.append({
                'material': str(material),
                'location': str(location),
                'requirement_date': sim_date + pd.Timedelta(days=1),
                'quantity': -AO_gap,
                'demand_element': 'net demand for AO',
                'layer': current_layer,
                'simulation_date': sim_date,
                'horizon_days': horizon
            })
        if FC_gap > 0:
            records.append({
                'material': str(material),
                'location': str(location),
                'requirement_date': sim_date + pd.Timedelta(days=1),
                'quantity': -FC_gap,
                'demand_element': 'net demand for forecast',
                'layer': current_layer,
                'simulation_date': sim_date,
                'horizon_days': horizon
            })
        if SS_gap > 0:
            records.append({
                'material': str(material),
                'location': str(location),
                'requirement_date': sim_date + pd.Timedelta(days=1),
                'quantity': -SS_gap,
                'demand_element': 'net demand for safety',
                'layer': current_layer,
                'simulation_date': sim_date,
                'horizon_days': horizon
            })

        # è®¡ç®—å‘çˆ¶èŠ‚ç‚¹ä¼ é€’çš„ç»è¿‡ MOQ/RV è°ƒæ•´åçš„ gap
        parent_key = None
        parent_gaps = {'AO': 0.0, 'FC': 0.0, 'SS': 0.0}
        if upstream and pd.notna(upstream):
            parent_key = (material, str(upstream))
            # è·å– MOQ/RV é…ç½®
            moq, rv = 1, 1
            if deploy_config_df is not None and not deploy_config_df.empty:
                config_row = deploy_config_df[
                    (deploy_config_df['material'] == material) &
                    (deploy_config_df['sending'] == str(upstream))
                ]
                if not config_row.empty:
                    moq = int(config_row.iloc[0].get('moq', 1))
                    rv = int(config_row.iloc[0].get('rv', 1))

            gap_dict = {'AO': AO_gap, 'FC': FC_gap, 'SS': SS_gap}
            for de, gv in gap_dict.items():
                parent_gaps[de] = apply_moq_rv(gv, moq, rv, is_cross_node=True) if gv > 0 else 0.0

        return records, parent_key, parent_gaps

    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ‰©å±•material_locationsï¼ŒåŒ…å«æ‰€æœ‰å±‚çº§ä¸­çš„åœ°ç‚¹
    # åŸæ¥çš„é€»è¾‘ï¼šåªåŒ…å«networkä¸­æ˜ç¡®é…ç½®çš„location
    # material_locations = network_df[['material', 'location']].drop_duplicates()
    
    # æ–°çš„é€»è¾‘ï¼šåŒ…å«æ‰€æœ‰å±‚çº§ä¸­çš„åœ°ç‚¹ï¼Œå¹¶ä¸ºç¼ºå¤±çš„material-locationç»„åˆæ·»åŠ é»˜è®¤é…ç½®
    all_locations_in_layers = set(location_layer.keys())
    all_materials_in_network = set(network_df['material'].unique())
    
    # æ„å»ºå®Œæ•´çš„material-locationç»„åˆ
    extended_material_locations = []
    
    # 1. æ·»åŠ networkä¸­æ˜ç¡®é…ç½®çš„ç»„åˆ
    # Performance optimization: Use itertuples instead of iterrows
    for row in active_network.itertuples():
        extended_material_locations.append({
            'material': str(row.material),
            'location': str(row.location)
        })
    
    # 2. ä¸ºè‡ªåŠ¨è¯†åˆ«çš„æ ¹èŠ‚ç‚¹æ·»åŠ ç¼ºå¤±çš„materialç»„åˆ
    for location in all_locations_in_layers:
        for material in all_materials_in_network:
            # æ£€æŸ¥è¿™ä¸ªç»„åˆæ˜¯å¦å·²ç»å­˜åœ¨
            exists = any(
                ml['material'] == material and ml['location'] == location 
                for ml in extended_material_locations
            )
            
            if not exists:
                # è¿™æ˜¯ä¸€ä¸ªç¼ºå¤±çš„ç»„åˆï¼Œéœ€è¦æ·»åŠ 
                extended_material_locations.append({
                    'material': str(material),
                    'location': str(location)
                })
    
    # å»é‡å¹¶è½¬æ¢ä¸ºDataFrameï¼Œç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¿æŒå­—ç¬¦ä¸²ç±»å‹
    material_locations = pd.DataFrame(extended_material_locations).drop_duplicates()
    # æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œç¡®ä¿ç±»å‹ä¸€è‡´æ€§
    material_locations = _normalize_identifiers(material_locations)
    
    # print(f"ğŸ” æ‰©å±•åçš„material-locationç»„åˆ:")
    # print(f"  åŸå§‹networké…ç½®: {len(active_network)} æ¡")
    # print(f"  æ‰©å±•åç»„åˆ: {len(material_locations)} æ¡")
    # print(f"  åŒ…å«çš„æ ¹èŠ‚ç‚¹: {[loc for loc in all_locations_in_layers if location_layer.get(loc, -1) == 0]}")
    
    future_production_df = all_production_df.copy() if not all_production_df.empty and 'available_date' in all_production_df.columns else pd.DataFrame()
    if not future_production_df.empty:
        future_production_df['available_date'] = pd.to_datetime(future_production_df['available_date'])
        # æ•°å€¼åˆ—ç»Ÿä¸€ä¸ºæ•°å€¼ç±»å‹ï¼Œç¼ºå¤±ç½® 0
        for col in ['produced_qty', 'uncon_planned_qty', 'quantity']:
            if col in future_production_df.columns:
                future_production_df[col] = pd.to_numeric(future_production_df[col], errors='coerce').fillna(0)
    # ä¸‹æ¸¸gapåˆ† AOã€FCã€SS gap
    downstream_gap_dict = defaultdict(lambda: {'AO': 0.0, 'FC': 0.0, 'SS': 0.0})

    for layer in all_layers:
        parent_gap_accum = defaultdict(lambda: {'AO': 0.0, 'FC': 0.0, 'SS': 0.0})

        # è·å–å½“å‰å±‚çº§çš„èŠ‚ç‚¹
        material_locations_df = pd.DataFrame(material_locations)
        layer_locations = [loc for loc, lyr in location_layer.items() if lyr == layer]
        layer_mask = material_locations_df['location'].isin(layer_locations)
        layer_nodes = material_locations_df[layer_mask]

        # å½“å‰å±‚å·ä¾›å­å‡½æ•°è®°å½•ä½¿ç”¨
        current_layer = layer

        # å¹¶è¡Œå¤„ç†å½“å‰å±‚çš„æ‰€æœ‰èŠ‚ç‚¹
        records_lock = threading.Lock()
        parent_lock = threading.Lock()
        futures_map = {}
        failed_rows: List[tuple] = []
        try:
            n_workers = min(32, max(1, len(layer_nodes)))
            with ThreadPoolExecutor(max_workers=n_workers) as executor:
                for ml in layer_nodes.itertuples():
                    fut = executor.submit(_compute_node_net_demand, ml)
                    futures_map[fut] = ml

                for fut in as_completed(futures_map.keys()):
                    try:
                        records, parent_key, parent_gaps = fut.result()
                        if records:
                            with records_lock:
                                all_net_demand_records.extend(records)
                        if parent_key is not None:
                            with parent_lock:
                                parent_gap_accum[parent_key]['AO'] += parent_gaps['AO']
                                parent_gap_accum[parent_key]['FC'] += parent_gaps['FC']
                                parent_gap_accum[parent_key]['SS'] += parent_gaps['SS']
                    except Exception as e:
                        print(f"[M3] parallel task failed on layer {layer}: {e}")
                        failed_rows.append(futures_map.get(fut))
        except Exception as e:
            # Executor-wide failure, fall back to sequential
            print(f"[M3] parallel execution failed for layer {layer}: {e}. Falling back to sequential.")
            failed_rows = list(layer_nodes.itertuples())

        # If any tasks failed, retry them sequentially to ensure completeness
        if failed_rows:
            print(f"[M3] retrying {len(failed_rows)} failed nodes sequentially on layer {layer}.")
            for ml in failed_rows:
                try:
                    records, parent_key, parent_gaps = _compute_node_net_demand(ml)
                    if records:
                        all_net_demand_records.extend(records)
                    if parent_key is not None:
                        parent_gap_accum[parent_key]['AO'] += parent_gaps['AO']
                        parent_gap_accum[parent_key]['FC'] += parent_gaps['FC']
                        parent_gap_accum[parent_key]['SS'] += parent_gaps['SS']
                except Exception as e:
                    print(f"[M3] sequential retry failed for node on layer {layer}: {e}")


        # æœ¬å±‚æ‰€æœ‰èŠ‚ç‚¹gapèšåˆåå†ä¼ é€’ç»™çˆ¶å±‚
        downstream_gap_dict = parent_gap_accum
        
        # if parent_gap_accum:
        #     print(f"    ğŸ“Š Layer {layer} gapæ±‡æ€»:")
        #     for (mat, loc), gaps in parent_gap_accum.items():
        #         print(f"      {mat}@{loc}: AO={gaps['AO']:.2f}, forecast={gaps['FC']:.2f}, safety={gaps['SS']:.2f}")

    # ç”Ÿæˆæœ€ç»ˆå‡€éœ€æ±‚DataFrame
    net_demand_df = pd.DataFrame(all_net_demand_records)
    
    # ç«‹å³æ ‡å‡†åŒ–æ ‡è¯†ç¬¦å­—æ®µï¼Œé¿å…pandasè‡ªåŠ¨ç±»å‹æ¨æ–­å¯¼è‡´é—®é¢˜
    if not net_demand_df.empty:
        net_demand_df = _normalize_identifiers(net_demand_df)
    
    if not net_demand_df.empty and len(net_demand_df) > 0:
        # æŒ‰å…³é”®å­—æ®µåˆ†ç»„èšåˆ
        group_cols = ['material', 'location', 'requirement_date', 'demand_element', 'layer']
        net_demand_df = (
            net_demand_df.groupby(group_cols, as_index=False)
            .agg({
                'quantity': 'sum',
                'simulation_date': 'first',
                'horizon_days': 'first'
            })
        )
        if not net_demand_df.empty:
            # ç›´æ¥è¿”å›ç»“æœï¼Œä¸å¼ºåˆ¶æ’åºä»¥é¿å…ç±»å‹é—®é¢˜
            net_demand_df = net_demand_df.reset_index(drop=True)
    
    # ç¡®ä¿è¿”å›çš„æ˜¯DataFrameç±»å‹
    final_df = pd.DataFrame(net_demand_df) if not isinstance(net_demand_df, pd.DataFrame) else net_demand_df
    
    # print(f"âœ… MRPæ¨¡æ‹Ÿå®Œæˆï¼Œç”Ÿæˆ {len(final_df)} æ¡netdemandè®°å½•")
    # if not final_df.empty:
    #     print(f"  æ¶‰åŠåœ°ç‚¹: {sorted(final_df['location'].unique())}")
    #     print(f"  æ¶‰åŠç‰©æ–™: {sorted(final_df['material'].unique())}")
    #     print(f"  å±‚çº§åˆ†å¸ƒ: {dict(final_df['layer'].value_counts())}")
    
    print(f"[M3] run_mrp_layered_simulation_daily total: {time.perf_counter()-_t_func:.3f}s, records={len(final_df)}")
    return final_df

def load_excel_with_sheets(filepath: str) -> Dict[str, pd.DataFrame]:
    """åŠ è½½Excelæ–‡ä»¶çš„æ‰€æœ‰sheet
    
    Args:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, pd.DataFrame]: sheetåç§°åˆ°DataFrameçš„æ˜ å°„
    """
    xl = pd.ExcelFile(filepath)
    result = {}
    for sheet in xl.sheet_names:
        result[str(sheet)] = xl.parse(sheet)
    return result

def run_integrated_mode(
    module1_output_dir: str,
    orchestrator: object,
    config_dict: dict,
    start_date: str,
    end_date: str,
    output_dir: str
) -> dict:
    """
    Module3 é›†æˆæ¨¡å¼è¿è¡Œå‡½æ•°
    æ‰€æœ‰æ¨¡å—åªå¤„ç†æ¨¡æ‹Ÿå‘¨æœŸå†…çš„æ•°æ®
    
    Args:
        module1_output_dir: Module1è¾“å‡ºç›®å½•
        orchestrator: Orchestratorå®ä¾‹
        config_dict: é…ç½®æ•°æ®å­—å…¸
        start_date: ä»¿çœŸå¼€å§‹æ—¥æœŸ
        end_date: ä»¿çœŸç»“æŸæ—¥æœŸ
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        dict: åŒ…å«è¾“å‡ºç»“æœçš„å­—å…¸
    """
    print(f"ğŸ”„ Module3 è¿è¡Œäºé›†æˆæ¨¡å¼")
    _t_total = time.perf_counter()
    # print(f"ğŸ“Š æ¨¡æ‹Ÿæ¨¡å¼ï¼šæ‰€æœ‰æ¨¡å—åªå¤„ç†æ¨¡æ‹Ÿå‘¨æœŸå†…çš„æ•°æ®")
    
    # åŠ è½½é™æ€é…ç½®æ•°æ®
    safety_stock_df = config_dict.get('M3_SafetyStock', pd.DataFrame())
    network_df = config_dict.get('Global_Network', pd.DataFrame())
    lead_time_df = config_dict.get('Global_LeadTime', pd.DataFrame())
    m4_mlcfg_df = config_dict.get('M4_MaterialLocationLineCfg', pd.DataFrame())
    deploy_config_df = config_dict.get('M5_DeployConfig', pd.DataFrame())  # ğŸ”§ æ–°å¢ï¼šMOQ/RVé…ç½®
    # æ•°æ®ç±»å‹è½¬æ¢å’Œæ ‡è¯†ç¬¦å­—æ®µæ ‡å‡†åŒ–
    if not safety_stock_df.empty and 'date' in safety_stock_df.columns:
        safety_stock_df['date'] = pd.to_datetime(safety_stock_df['date'])
    if not network_df.empty:
        if 'eff_from' in network_df.columns:
            network_df['eff_from'] = pd.to_datetime(network_df['eff_from'])
        if 'eff_to' in network_df.columns:
            network_df['eff_to'] = pd.to_datetime(network_df['eff_to'])
    
    # ğŸ”§ æ ‡å‡†åŒ–æ‰€æœ‰é…ç½®æ•°æ®çš„æ ‡è¯†ç¬¦å­—æ®µ
    if not deploy_config_df.empty:
        deploy_config_df = _normalize_identifiers(deploy_config_df)
    if not safety_stock_df.empty:
        safety_stock_df = _normalize_identifiers(safety_stock_df)
    if not network_df.empty:
        network_df = _normalize_identifiers(network_df)
    if not m4_mlcfg_df.empty:
        m4_mlcfg_df = _normalize_identifiers(m4_mlcfg_df)
    
    # ç”Ÿæˆæ—¥æœŸèŒƒå›´
    date_range = pd.date_range(start_date, end_date, freq='D')
    # print(f"å¤„ç† {len(date_range)} å¤©ï¼Œä» {start_date} åˆ° {end_date}")
    
    all_net_demand = []
    
    for current_date in date_range:
        # print(f"\nğŸ“… å¤„ç†æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")
        
        # ä»Module1åŠ è½½æ¯æ—¥æ•°æ®ï¼ˆåªå¤„ç†æ¨¡æ‹Ÿå‘¨æœŸå†…çš„æ•°æ®ï¼‰
        try:
            module1_daily_data = load_module1_daily_outputs(module1_output_dir, current_date)
            supply_demand_df = module1_daily_data.get('supply_demand_df', pd.DataFrame())
            today_shipment_df = module1_daily_data.get('shipment_df', pd.DataFrame())
            # print(f"  âœ… ä» Module1 åŠ è½½äº† {len(supply_demand_df)} æ¡ä¾›éœ€è®°å½•")
            # print(f"  âœ… ä» Module1 åŠ è½½äº† {len(today_shipment_df)} æ¡å‘è´§è®°å½•")
        except Exception as e:
            print(f"  âš ï¸  Module1æ•°æ®åŠ è½½å¤±è´¥: {e}")
            supply_demand_df = pd.DataFrame()
            today_shipment_df = pd.DataFrame()
        
        # ä» Orchestrator è·å–åŠ¨æ€æ•°æ®
        try:
            beginning_inventory_df = orchestrator.get_beginning_inventory_view(current_date.strftime('%Y-%m-%d'))
            in_transit_df = orchestrator.get_planning_intransit_view(current_date.strftime('%Y-%m-%d'))
            delivery_gr_df = orchestrator.get_delivery_gr_view(current_date.strftime('%Y-%m-%d'))
            #production_gr_df = orchestrator.get_production_gr_view(current_date.strftime('%Y-%m-%d'))
            #production_gr_df = production_gr_df.rename(columns={'date': 'available_date'})
            all_production_df = orchestrator.get_all_production_view(current_date.strftime('%Y-%m-%d'))
            open_deployment_df = orchestrator.get_open_deployment_view(current_date.strftime('%Y-%m-%d'))
            delivery_shipment_df = orchestrator.get_delivery_shipment_log_view(current_date.strftime('%Y-%m-%d'))

            # æ ‡å‡†åŒ–ä»Orchestratorè·å–çš„æ•°æ®ä¸­çš„æ ‡è¯†ç¬¦å­—æ®µ
            beginning_inventory_df = _normalize_identifiers(beginning_inventory_df)
            in_transit_df = _normalize_identifiers(in_transit_df)
            delivery_gr_df = _normalize_identifiers(delivery_gr_df)
            all_production_df = _normalize_identifiers(all_production_df)
            open_deployment_df = _normalize_identifiers(open_deployment_df)
            delivery_shipment_df = _normalize_identifiers(delivery_shipment_df)

            # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(beginning_inventory_df)} æ¡æœŸåˆåº“å­˜è®°å½•")
            # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(in_transit_df)} æ¡åœ¨é€”è®°å½•")
            # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(delivery_gr_df)} æ¡æ”¶è´§è®°å½•")
            # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(all_production_df)} æ¡ç”Ÿäº§è®°å½•")
            # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(open_deployment_df)} æ¡å¼€æ”¾éƒ¨ç½²è®°å½•")
            # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(delivery_shipment_df)} æ¡å‘è¿è®°å½•")
        except Exception as e:
            print(f"  âš ï¸  Orchestratoræ•°æ®åŠ è½½å¤±è´¥: {e}")
            beginning_inventory_df = pd.DataFrame()
            in_transit_df = pd.DataFrame()
            delivery_gr_df = pd.DataFrame()
            all_production_df = pd.DataFrame()
            open_deployment_df = pd.DataFrame()
            delivery_shipment_df = pd.DataFrame()
        
        # è®¡ç®—å½“æ—¥çš„Net Demand  
        try:
            net_demand_df = run_mrp_layered_simulation_daily(
                current_date,
                supply_demand_df,
                 module1_daily_data.get('order_df', pd.DataFrame()),
                today_shipment_df,
                safety_stock_df,
                beginning_inventory_df,
                in_transit_df,
                delivery_gr_df,
                all_production_df,  # ä½¿ç”¨ä»Orchestratorè·å–çš„ç”Ÿäº§æ•°æ® å…¨é‡ç”Ÿäº§ï¼šå«ä»Šå¤©+æœªæ¥
                open_deployment_df,
                network_df,
                lead_time_df,
                m4_mlcfg_df,
                delivery_shipment_df=delivery_shipment_df,
                deploy_config_df=deploy_config_df  # ğŸ”§ æ–°å¢ï¼šä¼ é€’MOQ/RVé…ç½®
            )
            # print(f"  âœ… è®¡ç®—å®Œæˆï¼Œç”Ÿæˆ {len(net_demand_df)} æ¡å‡€éœ€æ±‚è®°å½•")
        except Exception as e:
            print(f"  âŒ å‡€éœ€æ±‚è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            net_demand_df = pd.DataFrame()
        
        # ä¿å­˜æ¯æ—¥è¾“å‡º
        daily_output_file = f"{output_dir}/Module3Output_{current_date.strftime('%Y%m%d')}.xlsx"
        try:
            expected_cols = ['material','location','requirement_date','quantity','demand_element','layer','simulation_date','horizon_days']
            if net_demand_df.empty:
                net_demand_df = pd.DataFrame(columns=expected_cols)
            else:
                for c in expected_cols:
                    if c not in net_demand_df.columns:
                        net_demand_df[c] = pd.Series(dtype='object')
                net_demand_df = net_demand_df[expected_cols]
            with pd.ExcelWriter(daily_output_file, engine='openpyxl') as writer:
                net_demand_df.to_excel(writer, index=False, sheet_name='NetDemand')
        except Exception as e:
            print(f"  âš ï¸  ä¿å­˜å¤±è´¥: {e}")
        
        all_net_demand.extend(net_demand_df.to_dict('records') if not net_demand_df.empty else [])
    
    print(f"\nâœ… Module3 é›†æˆæ¨¡å¼å¤„ç†å®Œæˆ")
    print(f"[M3] total duration: {time.perf_counter()-_t_total:.3f}s for {len(date_range)} days, net_demand_records={len(all_net_demand)}")
    # print(f"  å¤„ç†äº† {len(date_range)} å¤©")
    # print(f"  ç”Ÿæˆäº† {len(all_net_demand)} æ¡Net Demandè®°å½•")
    # print(f"  æ‰€æœ‰æ¨¡å—åªå¤„ç†æ¨¡æ‹Ÿå‘¨æœŸå†…çš„æ•°æ®")
    
    return {
        'net_demand_count': len(all_net_demand),
        'processed_dates': len(date_range),
        'output_files': [f"Module3Output_{d.strftime('%Y%m%d')}.xlsx" for d in date_range]
    }
