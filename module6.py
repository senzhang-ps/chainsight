# module 6 - Physical Flow Management Module
# module_6_2_4log.py
#
# Integration Mode Support:
# - Standalone Mode: Excel file input/output (legacy)
# - Integrated Mode: Config dict + Orchestrator integration
#
# Data Sources (Integrated):
# - OpenDeployment: orchestrator.get_open_deployment(current_date)
# - M6_ Configs: M6_TruckReleaseCon, M6_TruckCapacityPlan, etc.
# - Global_ Configs: Global_DemandPriority, Global_LeadTime
#
# Orchestrator Integration:
# - orchestrator.process_delivery_plan(delivery_plan_df, simulation_date)
#   - Generates in-transit inventory records
#   - Generates delivery_gr records  
#   - Offsets open_deployment records
#
# Execution Pattern: Daily processing following Module4/5 pattern
# Module Execution Order: Module1 â†’ Module4 â†’ Module5 â†’ Module6 â†’ Module3

import pandas as pd
import numpy as np
import ast
import os
from typing import Tuple, List, Dict
from math import floor
from datetime import datetime

# ---------------------- Safe rule evaluator ----------------------
class SafeExpressionEvaluator:
    def __init__(self, allowed_names):
        self.allowed_names = set(allowed_names)

    def eval(self, expr: str, context: dict) -> bool:
        expr = (expr or '').strip()
        expr = expr.replace('AND','and').replace('OR','or').replace('NOT','not')
        if not expr:
            return False
        node = ast.parse(expr, mode='eval')
        return self._eval_node(node.body, context)

    def _eval_node(self, node, context):
        if isinstance(node, ast.BoolOp):
            values = [self._eval_node(v, context) for v in node.values]
            if isinstance(node.op, ast.And):  return all(values)
            if isinstance(node.op, ast.Or):   return any(values)
            raise ValueError(f"Unsupported boolean operator: {type(node.op).__name__}")
        elif isinstance(node, ast.Compare):
            left = self._eval_node(node.left, context)
            results = []
            for op, comparator in zip(node.ops, node.comparators):
                right = self._eval_node(comparator, context)
                if   isinstance(op, ast.Eq):   results.append(left == right)
                elif isinstance(op, ast.NotEq):results.append(left != right)
                elif isinstance(op, ast.Lt):   results.append(left <  right)
                elif isinstance(op, ast.LtE):  results.append(left <= right)
                elif isinstance(op, ast.Gt):   results.append(left >  right)
                elif isinstance(op, ast.GtE):  results.append(left >= right)
                else: raise ValueError(f"Unsupported comparison operator: {type(op).__name__}")
                left = right
            return all(results)
        elif isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.Not):
            return not self._eval_node(node.operand, context)
        elif isinstance(node, ast.Name):
            if node.id not in self.allowed_names:
                raise ValueError(f"Variable '{node.id}' is not allowed")
            if node.id not in context:
                raise ValueError(f"Variable '{node.id}' not found in context")
            return context[node.id]
        elif isinstance(node, ast.Constant):
            return node.value
        else:
            raise ValueError(f"Unsupported syntax: {ast.dump(node)}")

def load_standalone_config(input_excel: str) -> dict:
    """
    åŠ è½½ç‹¬ç«‹æ¨¡å¼çš„é…ç½®æ•°æ®ï¼ˆä» Excel æ–‡ä»¶ï¼‰
    """
    print(f"ğŸ“ æ­£åœ¨ä» '{input_excel}' è¯»å–è¾“å…¥æ•°æ®...")
    try:
        config = {
            'DeploymentPlan': pd.read_excel(input_excel, sheet_name='DeploymentPlan'),
            'TruckReleaseCon': pd.read_excel(input_excel, sheet_name='TruckReleaseCon'),
            'TruckCapacityPlan': pd.read_excel(input_excel, sheet_name='TruckCapacityPlan'),
            'TruckTypeSpecs': pd.read_excel(input_excel, sheet_name='TruckTypeSpecs'),
            'MaterialMD': pd.read_excel(input_excel, sheet_name='MaterialMD'),
            'DemandPriority': pd.read_excel(input_excel, sheet_name='DemandPriority'),
            'LeadTime': pd.read_excel(input_excel, sheet_name='LeadTime'),
            'DeliveryDelayDistribution': pd.read_excel(input_excel, sheet_name='DeliveryDelayDistribution'),
            'MDQBypassRules': pd.read_excel(input_excel, sheet_name='MDQBypassRules')
        }
        
        # Optional seed in file
        xl = pd.ExcelFile(input_excel)
        if 'RandomSeed' in xl.sheet_names:
            rs = pd.read_excel(input_excel, sheet_name='RandomSeed')
            if 'random_seed' in rs.columns and not rs.empty and pd.notna(rs.iloc[0]['random_seed']):
                file_seed = int(rs.iloc[0]['random_seed'])
                np.random.seed(file_seed)
                print(f"ğŸŒ± å·²ä»æ–‡ä»¶è®¾ç½®éšæœºç§å­: {file_seed}")
        
        return config
        
    except Exception as e:
        print(f"âŒ è¯»å–è¾“å…¥å¤±è´¥: {e}")
        raise

def load_integrated_config(
    config_dict: dict,
    orchestrator: object,
    current_date: pd.Timestamp
) -> dict:
    """
    åŠ è½½é›†æˆé…ç½®æ•°æ®ï¼Œæ›¿ä»£åŸæ¥çš„Excelæ–‡ä»¶è¾“å…¥
    
    Args:
        config_dict: é…ç½®æ•°æ®å­—å…¸
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        dict: é›†æˆé…ç½®æ•°æ®
    """
    config = {}
    validation_log = []
    
    try:
        # 1. ä»OrchestratoråŠ è½½OpenDeploymentï¼ˆæ›¿ä»£DeploymentPlanï¼‰
        open_deployment = orchestrator.get_open_deployment(current_date)
        print(f"  ğŸ” ä»Orchestratorè·å–open_deployment: {len(open_deployment) if open_deployment is not None else 0} æ¡")
        
        if open_deployment is None or open_deployment.empty:
            print(f"[WARN] No open deployment found for {current_date.strftime('%Y-%m-%d')}")
            open_deployment = pd.DataFrame(columns=[
                'material', 'sending', 'receiving', 'planned_deployment_date', 
                'deployed_qty', 'demand_element', 'ori_deployment_uid'
            ])
        else:
            # æŒ‰è·¯çº¿ç±»å‹ç»Ÿè®¡
            open_deployment['route_type'] = open_deployment.apply(
                lambda row: 'self_loop' if row['sending'] == row['receiving'] else 'cross_node', axis=1
            )
            route_stats = open_deployment['route_type'].value_counts()
            print(f"  ğŸ“Š è·¯çº¿ç»Ÿè®¡: {route_stats.to_dict()}")
            
            # æ˜¾ç¤ºè·¨èŠ‚ç‚¹è·¯çº¿è¯¦æƒ…
            cross_node = open_deployment[open_deployment['route_type'] == 'cross_node']
            if len(cross_node) > 0:
                cross_routes = cross_node.groupby(['sending', 'receiving']).size().reset_index(name='count')
                print(f"  ğŸšš è·¨èŠ‚ç‚¹è·¯çº¿è¯¦æƒ…:")
                for _, row in cross_routes.iterrows():
                    print(f"    {row['sending']} -> {row['receiving']}: {row['count']} é¡¹")
            else:
                print(f"  âš ï¸  æ— è·¨èŠ‚ç‚¹è·¯çº¿æ•°æ®")
        
        # ç¡®ä¿æ—¥æœŸå­—æ®µæ­£ç¡®æ ¼å¼åŒ–
        if not open_deployment.empty and 'planned_deployment_date' in open_deployment.columns:
            open_deployment['planned_deployment_date'] = pd.to_datetime(open_deployment['planned_deployment_date'])
        
        config['DeploymentPlan'] = open_deployment
        
        # 2. ä»é…ç½®è¡¨åŠ è½½M6_å¼€å¤´çš„é…ç½®æ•°æ®
        m6_configs = {
            'TruckReleaseCon': 'M6_TruckReleaseCon',
            'TruckCapacityPlan': 'M6_TruckCapacityPlan', 
            'TruckTypeSpecs': 'M6_TruckTypeSpecs',
            'MaterialMD': 'M6_MaterialMD',
            'DeliveryDelayDistribution': 'M6_DeliveryDelayDistribution',
            'MDQBypassRules': 'M6_MDQBypassRules'
        }
        
        for config_key, sheet_name in m6_configs.items():
            if sheet_name in config_dict:
                config[config_key] = config_dict[sheet_name].copy()
            else:
                validation_log.append({
                    'sheet': sheet_name, 'row': '', 
                    'issue': f'Missing required configuration sheet: {sheet_name}'
                })
                config[config_key] = pd.DataFrame()
        
        # 3. ä»é…ç½®è¡¨åŠ è½½Global_å¼€å¤´çš„å…±äº«é…ç½®æ•°æ®
        global_configs = {
            'DemandPriority': 'Global_DemandPriority',
            'LeadTime': 'Global_LeadTime'
        }
        
        for config_key, sheet_name in global_configs.items():
            if sheet_name in config_dict:
                config[config_key] = config_dict[sheet_name].copy()
            else:
                validation_log.append({
                    'sheet': sheet_name, 'row': '',
                    'issue': f'Missing required global configuration sheet: {sheet_name}'
                })
                config[config_key] = pd.DataFrame()
        
        # 4. æ—¥æœŸå­—æ®µå¤„ç†
        date_fields = {
            'DeploymentPlan': ['planned_deployment_date'],
            'TruckCapacityPlan': ['date', 'eff_from', 'eff_to'],
            'DeliveryDelayDistribution': ['date'] if 'date' in config.get('DeliveryDelayDistribution', pd.DataFrame()).columns else []
        }
        
        for sheet, fields in date_fields.items():
            if sheet in config and not config[sheet].empty:
                for field in fields:
                    if field in config[sheet].columns:
                        config[sheet][field] = pd.to_datetime(config[sheet][field], errors='coerce')
        
        config['ValidationLog'] = validation_log
        print(f"âœ… Integrated config loaded: {len(config['DeploymentPlan'])} deployment plans, {len(validation_log)} validation issues")
        
    except Exception as e:
        print(f"âŒ Error loading integrated config: {str(e)}")
        validation_log.append({'sheet': 'General', 'row': '', 'issue': f'Config loading error: {str(e)}'})
        config['ValidationLog'] = validation_log
        # æä¾›é»˜è®¤ç©ºé…ç½®ä»¥é˜²æ­¢ç¨‹åºå´©æºƒ
        for key in ['DeploymentPlan', 'TruckReleaseCon', 'TruckCapacityPlan', 'TruckTypeSpecs', 
                   'MaterialMD', 'DeliveryDelayDistribution', 'MDQBypassRules', 'DemandPriority', 'LeadTime']:
            if key not in config:
                config[key] = pd.DataFrame()
    
    return config

# ---------------------- Helpers ----------------------
def _generate_validation_report(validation_log: List[Dict], output_file: str):
    """
    ç”Ÿæˆvalidation.txtæŠ¥å‘Š
    
    Args:
        validation_log: éªŒè¯æ—¥å¿—åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç”Ÿæˆvalidation.txtæ–‡ä»¶è·¯å¾„
    output_dir = os.path.dirname(output_file)
    base_name = os.path.splitext(os.path.basename(output_file))[0]
    validation_file = os.path.join(output_dir, f"{base_name}_validation.txt")
    
    from datetime import datetime
    
    # ç»Ÿè®¡validationé—®é¢˜
    errors = [log for log in validation_log if log.get('severity') == 'ERROR']
    warnings = [log for log in validation_log if log.get('severity') != 'ERROR']
    
    with open(validation_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("MODULE6 VALIDATION REPORT\n")
        f.write("=" * 80 + "\n")
        f.write(f"Generated Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total Issues: {len(validation_log)}\n")
        f.write(f"Errors: {len(errors)}\n")
        f.write(f"Warnings: {len(warnings)}\n")
        f.write("\n")
        
        if errors:
            f.write("âŒ CRITICAL ERRORS FOUND\n")
            f.write("Following issues may cause data loss or incorrect processing:\n")
            f.write("-" * 60 + "\n")
            for i, error in enumerate(errors, 1):
                f.write(f"{i}. {error.get('issue', 'Unknown error')}\n")
                if 'impact' in error:
                    f.write(f"   Impact: {error['impact']}\n")
                if 'missing_element' in error:
                    f.write(f"   Missing Element: {error['missing_element']}\n")
                if 'affected_records' in error:
                    f.write(f"   Affected Records: {error['affected_records']}\n")
                if 'route_breakdown' in error:
                    f.write(f"   Route Breakdown: {error['route_breakdown']}\n")
                f.write("\n")
        
        if warnings:
            f.write("âš ï¸  WARNINGS\n")
            f.write("Following issues should be reviewed but may not block processing:\n")
            f.write("-" * 60 + "\n")
            for i, warning in enumerate(warnings, 1):
                f.write(f"{i}. {warning.get('issue', 'Unknown warning')}\n")
                f.write(f"   Sheet: {warning.get('sheet', 'Unknown')}\n")
                f.write("\n")
        
        if not errors and not warnings:
            f.write("âœ… ALL VALIDATIONS PASSED\n")
            f.write("No configuration issues detected.\n")
            f.write("All demand_element types are properly configured.\n")
            f.write("All material metadata is available.\n")
            f.write("All truck configurations are valid.\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write("RECOMMENDATIONS:\n")
        f.write("=" * 80 + "\n")
        
        if errors:
            f.write("1. Fix all ERROR-level issues before proceeding\n")
            f.write("2. Add missing demand_element configurations to Global_DemandPriority\n")
            f.write("3. Ensure all material metadata is defined in M6_MaterialMD\n")
        elif warnings:
            f.write("1. Review WARNING-level issues for optimization opportunities\n")
            f.write("2. Check truck capacity and configuration alignment\n")
        else:
            f.write("1. Configuration is optimal for current deployment plans\n")
            f.write("2. Monitor validation reports in future runs\n")
            f.write("3. Consider adding bypass rules if delivery performance is suboptimal\n")
        
        f.write("\n")
        f.write("For detailed information, check the ValidationLog sheet in the Excel output.\n")
        f.write("=" * 80 + "\n")
    
    if validation_log:
        status = "with issues" if (errors or warnings) else "clean"
        print(f"ğŸ“„ ValidationæŠ¥å‘Šå·²ç”Ÿæˆ ({status}): {validation_file}")
    else:
        print(f"ğŸ“„ ValidationæŠ¥å‘Šå·²ç”Ÿæˆ (no issues): {validation_file}")


def should_bypass_mdq(
    context: dict,
    rules: pd.DataFrame,
    evaluator: SafeExpressionEvaluator
) -> Tuple[bool, str]:
    for _, rule in rules.iterrows():
        matched = True
        for col in ['sending','receiving','truck_type','demand_element']:
            rule_val = str(rule.get(col, 'ALL'))
            if rule_val != 'ALL' and str(context.get(col)) != rule_val:
                matched = False
                break
        if not matched:
            continue
        try:
            expr = rule['condition_logic']
            if evaluator.eval(expr, context):
                print(f"  ğŸ”¹ è§„åˆ™å‘½ä¸­: RuleID={rule.get('rule_id')}, Condition='{expr}'")
                return True, rule.get('rule_id')
        except Exception as e:
            print(f"  âš ï¸  è§„åˆ™è¡¨è¾¾å¼è¯„ä¼°å¤±è´¥ (RuleID={rule.get('rule_id')}): {e}")
            continue
    return False, None

def _normalize_capacity_plan(truck_cap_df: pd.DataFrame,
                             sim_start: pd.Timestamp,
                             sim_end: pd.Timestamp) -> pd.DataFrame:
    """
    å…¼å®¹ä¸¤ç§è¾“å…¥ï¼šé€æ—¥(date) æˆ– åŒºé—´(eff_from, eff_to)ï¼›å±•å¼€å¹¶èšåˆä¸ºæ—¥ç²’åº¦ï¼ˆé‡å æ±‚å’Œï¼‰
    """
    df = truck_cap_df.copy()
    has_range = {'eff_from','eff_to'}.issubset(set(df.columns))
    has_daily = 'date' in df.columns
    parts = []

    if has_daily:
        d = df[['date','sending','receiving','truck_type','truck_number']].copy()
        d['date'] = pd.to_datetime(d['date'])
        d = d[(d['date'] >= sim_start) & (d['date'] <= sim_end)]
        parts.append(d)

    if has_range:
        r = df[['eff_from','eff_to','sending','receiving','truck_type','truck_number']].copy()
        r['eff_from'] = pd.to_datetime(r['eff_from'])
        r['eff_to']   = pd.to_datetime(r['eff_to'])
        r['from_clip'] = r['eff_from'].clip(lower=sim_start)
        r['to_clip']   = r['eff_to'].clip(upper=sim_end)
        r = r[r['from_clip'] <= r['to_clip']]
        exploded = []
        for _, row in r.iterrows():
            dates = pd.date_range(row['from_clip'], row['to_clip'], freq='D')
            exploded.append(pd.DataFrame({
                'date': dates,
                'sending': row['sending'],
                'receiving': row['receiving'],
                'truck_type': row['truck_type'],
                'truck_number': row['truck_number'],
            }))
        if exploded:
            parts.append(pd.concat(exploded, ignore_index=True))

    if not parts:
        return pd.DataFrame(columns=['date','sending','receiving','truck_type','truck_number'])

    cap_daily = pd.concat(parts, ignore_index=True)
    cap_daily = cap_daily.groupby(['date','sending','receiving','truck_type'], as_index=False)['truck_number'].sum()
    return cap_daily

def sample_delivery_delay(sending, receiving, dist_df: pd.DataFrame) -> int:
    """
    é‡‡æ ·å»¶è¿Ÿï¼š
    1) ç²¾ç¡®è·¯çº¿ï¼›2) å…œåº•è¡Œ sending=ALL & receiving=ALLï¼›3) é»˜è®¤ 0 å¤©
    """
    if dist_df is None or dist_df.empty:
        return 0
    exact = dist_df[(dist_df['sending']==sending) & (dist_df['receiving']==receiving)]
    if not exact.empty:
        delays = exact['delay_days'].to_numpy()
        probs  = exact['probability'].to_numpy()
    else:
        global_mask = (dist_df['sending'].astype(str).str.upper()=='ALL') & \
                      (dist_df['receiving'].astype(str).str.upper()=='ALL')
        global_rows = dist_df[global_mask]
        if not global_rows.empty:
            delays = global_rows['delay_days'].to_numpy()
            probs  = global_rows['probability'].to_numpy()
        else:
            return 0
    probs = np.array(probs, dtype=float)
    if probs.sum() <= 0:
        return 0
    probs = probs / probs.sum()
    return int(np.random.choice(delays, p=probs))

def run_daily_physical_flow(
    config_dict: dict,
    orchestrator: object,
    current_date: pd.Timestamp,
    output_dir: str,
    max_wait_days: int = 7,
    random_seed: int = None
) -> dict:
    """
    æ¯æ—¥ç‰©æµæ‰§è¡Œå‡½æ•°ï¼Œå¤„ç†å½“æ—¥çš„éƒ¨ç½²è®¡åˆ’
    
    Args:
        config_dict: é…ç½®æ•°æ®å­—å…¸
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰ä»¿çœŸæ—¥æœŸ
        output_dir: è¾“å‡ºç›®å½•
        max_wait_days: æœ€å¤§ç­‰å¾…å¤©æ•°
        random_seed: éšæœºç§å­
        
    Returns:
        dict: åŒ…å«è¾“å‡ºç»“æœçš„å­—å…¸
    """
    print(f"ğŸ“Š Module6 Daily Physical Flow - {current_date.strftime('%Y-%m-%d')}")
    
    # åŠ è½½é›†æˆé…ç½®
    config = load_integrated_config(config_dict, orchestrator, current_date)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    daily_output_file = f"{output_dir}/Module6Output_{current_date.strftime('%Y%m%d')}.xlsx"
    
    # è°ƒç”¨ä¸»é€»è¾‘ï¼ˆé›†æˆæ¨¡å¼ï¼‰
    result = main(
        # é›†æˆæ¨¡å¼å‚æ•°
        config_dict=config_dict,
        orchestrator=orchestrator,
        current_date=current_date.strftime('%Y-%m-%d'),
        output_path=daily_output_file,
        # å…¬å…±å‚æ•°
        max_wait_days=max_wait_days,
        random_seed=random_seed
    )
    
    return {
        'daily_output_file': daily_output_file,
        'delivery_plan': result.get('delivery_plan', pd.DataFrame()),
        'vehicle_log': result.get('vehicle_log', pd.DataFrame()),
        'validation_log': result.get('validation_log', []),
        'statistics': result.get('statistics', {})
    }

# ---------------------- Core: packing & shipping ----------------------
def calculate_physical_inventory(
    orchestrator: object,
    current_date: pd.Timestamp
) -> Dict[Tuple[str, str], float]:
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„å®ç‰©åº“å­˜ï¼ˆåˆ›å»ºå‰¯æœ¬ï¼Œé¿å…ç›´æ¥ä¿®æ”¹Orchestratoråº“å­˜ï¼‰
    
    ç”±äºOrchestratoråœ¨Module6æ‰§è¡Œæ—¶ï¼Œå…¶unrestricted_inventoryå·²ç»æŒ‰ç…§ä»¥ä¸‹å…¬å¼æ›´æ–°ï¼š
    å®ç‰©åº“å­˜ = æœŸåˆunrestricted inventory + å½“æ—¥production GR + å½“æ—¥delivery GR - å½“æ—¥shipment
    
    æ³¨æ„ï¼šåˆ›å»ºåº“å­˜å‰¯æœ¬ï¼Œé¿å…Module6ç›´æ¥ä¿®æ”¹Orchestratorçš„å®é™…åº“å­˜ã€‚
    
    Args:
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        Dict: å®ç‰©åº“å­˜å­—å…¸å‰¯æœ¬ {(material, location): physical_quantity}
    """
    try:
        # åˆ›å»ºOrchestratoråº“å­˜çš„å‰¯æœ¬ï¼Œé¿å…ç›´æ¥ä¿®æ”¹åŸå§‹åº“å­˜
        physical_inventory = {}
        for key, qty in orchestrator.unrestricted_inventory.items():
            physical_inventory[key] = float(qty)
        
        print(f"  ğŸ“Š å®ç‰©åº“å­˜ç»Ÿè®¡: {len(physical_inventory)} ä¸ªSKU-åœ°ç‚¹ç»„åˆ")
        if physical_inventory:
            total_items = sum(1 for qty in physical_inventory.values() if qty > 0)
            positive_qty = sum(qty for qty in physical_inventory.values() if qty > 0)
            print(f"  âœ… æœ‰åº“å­˜SKU: {total_items}/{len(physical_inventory)}, æ€»é‡: {positive_qty:.1f}")
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºå‰5ä¸ªå®ç‰©åº“å­˜æ˜ç»†
            for i, (key, qty) in enumerate(list(physical_inventory.items())[:5]):
                material, location = key
                print(f"    å®ç‰©åº“å­˜: {material}@{location}: {qty:.1f}")
        
        return physical_inventory
        
    except Exception as e:
        print(f"  âš ï¸  è·å–å®ç‰©åº“å­˜å¤±è´¥: {e}")
        return {}

def run_physical_flow_module(
    # Standalone mode parameters
    input_excel: str = None,
    simulation_start: str = None,
    simulation_end: str = None,
    output_excel: str = None,
    # Integrated mode parameters
    config_dict: dict = None,
    orchestrator: object = None,
    current_date: str = None,
    output_path: str = None,
    # Common parameters
    max_wait_days: int = 7,
    random_seed: int = None
):
    # åˆ¤æ–­è¿è¡Œæ¨¡å¼
    if config_dict is not None:
        # é›†æˆæ¨¡å¼
        print("ğŸ”„ Module6 è¿è¡Œäºé›†æˆæ¨¡å¼")
        sim_date = pd.to_datetime(current_date)
        sim_dates = pd.DatetimeIndex([sim_date])  # å•æ—¥å¤„ç†ï¼Œä½¿ç”¨DatetimeIndex
        output_file = output_path
        # åŠ è½½é›†æˆé…ç½®
        config = load_integrated_config(config_dict, orchestrator, sim_date)
    else:
        # ç‹¬ç«‹æ¨¡å¼ - ä¿æŒå‘åå…¼å®¹
        print("ğŸ“œ Module6 è¿è¡Œäºç‹¬ç«‹æ¨¡å¼")
        config = load_standalone_config(input_excel)
        sim_start = pd.to_datetime(simulation_start)
        sim_end = pd.to_datetime(simulation_end)
        output_file = output_excel
        
        # è®¡ç®—ä»¿çœŸæ—¥æœŸèŒƒå›´
        dp = config['DeploymentPlan']
        if not dp.empty:
            sim_dates = pd.date_range(
                max(sim_start, dp['planned_deployment_date'].min()),
                min(sim_end, dp['planned_deployment_date'].max() + pd.Timedelta(days=max_wait_days))
            )
        else:
            sim_dates = pd.date_range(sim_start, sim_end)

    # Seed
    if random_seed is not None:
        np.random.seed(random_seed)
        print(f"ğŸŒ± éšæœºç§å­å·²è®¾ç½®ä¸º: {random_seed}")

    print(f"ğŸ“… ä»¿çœŸæ—¶é—´èŒƒå›´: {sim_dates.min().date()} åˆ° {sim_dates.max().date()}")

    # æ•°æ®éªŒè¯
    validation_log = list(config.get('ValidationLog', []))
    print("ğŸ“Š å¼€å§‹æ•°æ®æ ¡éªŒ...")

    # è·å–æ•°æ®
    dp = config['DeploymentPlan']
    truck_con = config['TruckReleaseCon']
    truck_cap = config['TruckCapacityPlan']
    truck_specs = config['TruckTypeSpecs']
    material_md = config['MaterialMD']
    demand_prio = config['DemandPriority']
    lead_time = config['LeadTime']
    delay_dist = config['DeliveryDelayDistribution']
    bypass_rules = config['MDQBypassRules']

    prio_map = demand_prio.set_index('demand_element')['priority'].to_dict()
    missing_prio = dp[~dp['demand_element'].isin(prio_map.keys())]
    if not missing_prio.empty:
        # è®°å½•ç¼ºå¤±çš„demand_elementè¯¦ç»†ä¿¡æ¯
        missing_elements = missing_prio['demand_element'].unique()
        for val in missing_elements:
            missing_records = missing_prio[missing_prio['demand_element'] == val]
            # ç»Ÿè®¡è·¯çº¿ç±»å‹
            route_stats = missing_records.apply(
                lambda row: 'self_loop' if row['sending'] == row['receiving'] else 'cross_node', axis=1
            ).value_counts()
            route_info = ', '.join([f"{k}: {v}" for k, v in route_stats.items()])
            
            validation_log.append({
                'sheet': 'Global_DemandPriority',
                'row': '',
                'issue': f'Missing priority configuration for demand_element "{val}" '
                        f'(affects {len(missing_records)} records: {route_info}). '
                        f'Records will be filtered out and not processed.',
                'severity': 'ERROR',
                'impact': f'Data Loss - {len(missing_records)} deployment plans excluded',
                'missing_element': val,
                'affected_records': len(missing_records),
                'route_breakdown': route_info
            })
        
        print(f"  âš ï¸  å‘ç° {len(missing_elements)} ä¸ªç¼ºå¤±çš„demand_elementé…ç½®ï¼Œå°†è¿‡æ»¤ {len(missing_prio)} æ¡è®°å½•")
        for val in missing_elements:
            missing_count = len(missing_prio[missing_prio['demand_element'] == val])
            print(f"    - '{val}': {missing_count} æ¡è®°å½•")
        
        # è¿‡æ»¤æ‰ç¼ºå¤±priorityçš„è®°å½•
        dp = dp[dp['demand_element'].isin(prio_map.keys())]
        print(f"  ğŸ“Š è¿‡æ»¤åä¿ç•™: {len(dp)} æ¡è®°å½•")

    mat_map = material_md.set_index('material')[['demand_unit_to_weight','demand_unit_to_volume']].to_dict('index')
    missing_mat = dp[~dp['material'].isin(mat_map.keys())]
    if not missing_mat.empty:
        missing_materials = missing_mat['material'].unique()
        for val in missing_materials:
            missing_records = missing_mat[missing_mat['material'] == val]
            validation_log.append({
                'sheet': 'M6_MaterialMD',
                'row': '',
                'issue': f'Missing material metadata for "{val}" '
                        f'(affects {len(missing_records)} records). '
                        f'Default unit conversion factors (1.0) will be used.',
                'severity': 'WARNING',
                'impact': f'Default Values Used - {len(missing_records)} records use defaults',
                'missing_material': val,
                'affected_records': len(missing_records)
            })
        
        print(f"  âš ï¸  å‘ç° {len(missing_materials)} ä¸ªç¼ºå¤±çš„materialé…ç½®ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
        # ä½¿ç”¨é»˜è®¤å€¼å¤„ç†
        dp['demand_unit_to_weight'] = dp['material'].map(lambda x: mat_map.get(x, {}).get('demand_unit_to_weight', 1.0))
        dp['demand_unit_to_volume'] = dp['material'].map(lambda x: mat_map.get(x, {}).get('demand_unit_to_volume', 1.0))
    else:
        dp = dp.merge(material_md, on='material', how='left')
        dp['demand_unit_to_weight'] = dp['demand_unit_to_weight'].fillna(1.0)
        dp['demand_unit_to_volume'] = dp['demand_unit_to_volume'].fillna(1.0)

    spec_map = truck_specs.set_index('truck_type').to_dict('index')

    # --- é˜ˆå€¼>1.0 çš„é…ç½®å‘Šè­¦ï¼ˆä»å…è®¸ï¼Œä½†ä¸ä¼šé é˜ˆå€¼è§¦å‘ï¼‰ ---
    bad_th = truck_con[(truck_con['WFR'] > 1.0) | (truck_con['VFR'] > 1.0)]
    for _, r in bad_th.iterrows():
        validation_log.append({
            'sheet': 'M6_TruckReleaseCon', 
            'row': '',
            'issue': f"Threshold > 1.0 for route {r['sending']}->{r['receiving']} type {r['truck_type']} "
                     f"(WFR={r['WFR']}, VFR={r['VFR']}). Will never trigger by threshold; bypass only.",
            'severity': 'WARNING',
            'impact': 'Configuration Issue - Route can only be triggered by bypass rules',
            'route': f"{r['sending']}->{r['receiving']}",
            'truck_type': r['truck_type'],
            'wfr_threshold': r['WFR'],
            'vfr_threshold': r['VFR']
        })

    # è½¦å‹è§„æ ¼ç¼ºå¤±
    missing_specs = set(truck_con['truck_type'].unique()) - set(spec_map.keys())
    for val in missing_specs:
        affected_routes = truck_con[truck_con['truck_type'] == val]
        validation_log.append({
            'sheet': 'M6_TruckTypeSpecs',
            'row': '',
            'issue': f'Missing truck type specification for "{val}" '
                    f'(affects {len(affected_routes)} route configurations). '
                    f'Routes using this truck type will be skipped.',
            'severity': 'ERROR',
            'impact': f'Data Loss - {len(affected_routes)} route configurations unavailable',
            'missing_truck_type': val,
            'affected_routes': len(affected_routes)
        })

    # Normalize fields
    dp['planned_deployment_date'] = pd.to_datetime(dp['planned_deployment_date'])
    lead_time[['PDT','GR']] = lead_time[['PDT','GR']].astype(int)
    delay_dist['delay_days'] = delay_dist['delay_days'].astype(int)

    # UIDs & priority - ä½¿ç”¨Orchestratoræä¾›çš„åŸå§‹UID
    dp = dp.reset_index(drop=True)
    # å¦‚æœæ²¡æœ‰ori_deployment_uidåˆ—ï¼Œæ‰é‡æ–°ç”Ÿæˆ
    if 'ori_deployment_uid' not in dp.columns or dp['ori_deployment_uid'].isnull().any():
        print(f"  âš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±UIDï¼Œé‡æ–°ç”Ÿæˆ")
        dp['ori_deployment_uid'] = [f'UID{i:06d}' for i in dp.index]
    else:
        print(f"  âœ… ä½¿ç”¨Orchestratoræä¾›çš„åŸUID")
    
    dp['priority'] = dp['demand_element'].map(prio_map)
    dp['waiting_days'] = 0
    dp['simulation_date'] = dp['planned_deployment_date']
    
    print(f"  ğŸ” å¤„ç†åçš„éƒ¨ç½²è®¡åˆ’æ•°é‡: {len(dp)}")
    # æŒ‰è·¯çº¿ç±»å‹ç»Ÿè®¡
    dp['route_type_debug'] = dp.apply(lambda row: 'self_loop' if row['sending'] == row['receiving'] else 'cross_node', axis=1)
    route_debug_stats = dp['route_type_debug'].value_counts()
    print(f"  ğŸ“Š éƒ¨ç½²è®¡åˆ’è·¯çº¿ç»Ÿè®¡: {route_debug_stats.to_dict()}")

    # Dict index
    dp_dict = dp.set_index('ori_deployment_uid').to_dict('index')
    # Pending state
    agg_status = {
        uid: {'qty': row['deployed_qty'], 'waiting': 1, 'planned': row['planned_deployment_date']}
        for uid, row in dp_dict.items()
    }

    # Dates & capacity map
    if config_dict is not None:
        # é›†æˆæ¨¡å¼: ä½¿ç”¨å•æ—¥èŒƒå›´
        sim_start = sim_dates.min()
        sim_end = sim_dates.max()
    else:
        # ç‹¬ç«‹æ¨¡å¼: ä½¿ç”¨åŸå§‹æ—¥æœŸèŒƒå›´
        sim_start = pd.to_datetime(simulation_start)
        sim_end   = pd.to_datetime(simulation_end)
        sim_dates = pd.date_range(
            max(sim_start, dp['planned_deployment_date'].min()),
            min(sim_end,   dp['planned_deployment_date'].max() + pd.Timedelta(days=max_wait_days))
        )
    print(f"ğŸ“… æ¨¡æ‹Ÿæ—¶é—´èŒƒå›´: {sim_dates.min().date()} åˆ° {sim_dates.max().date()}")

    cap_daily = _normalize_capacity_plan(truck_cap.copy(), sim_start, sim_end)
    cap_map   = cap_daily.set_index(['date','sending','receiving','truck_type'])['truck_number'].to_dict()
    print(f"[Capacity] normalized rows: {len(cap_daily)}")

    # Outputs
    delivery_plan, unsat_log, bypass_log = [], [], []
    vehicle_log = []          # é€è½¦ä¸€è¡Œ
    evaluator = SafeExpressionEvaluator(
        ['waiting_days','deployed_qty_ratio','exception_MDQ','sending','receiving','truck_type','demand_element']
    )

    # ---------------------- Main simulation loop ----------------------
    # é›†æˆæ¨¡å¼ï¼šè·å–å½“æ—¥å¯ç”¨åº“å­˜ç”¨äºåº“å­˜æ£€æŸ¥
    available_inventory = {}
    inventory_check_enabled = config_dict is not None and orchestrator is not None
    
    for sim_date in sim_dates:
        print(f"\nğŸ“† æ¨¡æ‹Ÿæ—¥æœŸ: {sim_date.date()}")
        
        # é›†æˆæ¨¡å¼ï¼šè·å–å½“æ—¥å®ç‰©åº“å­˜
        if inventory_check_enabled:
            available_inventory = calculate_physical_inventory(orchestrator, sim_date)
            print(f"    ğŸ’° åº“å­˜æ£€æŸ¥å·²å¯ç”¨ï¼ˆä½¿ç”¨å®ç‰©åº“å­˜ï¼‰")
        else:
            print(f"    âš ï¸  ç‹¬ç«‹æ¨¡å¼ï¼šè·³è¿‡åº“å­˜æ£€æŸ¥")
        
        print(f"    ğŸ—’ éƒ¨ç½²è®¡åˆ’çŠ¶æ€æ£€æŸ¥:")
        active_plans = {uid: st for uid, st in agg_status.items() if st['qty'] > 0}
        print(f"    ğŸ“ˆ æœ‰æ•ˆè®¡åˆ’æ•°: {len(active_plans)}/{len(agg_status)}")
        
        if active_plans:
            print(f"    ğŸ” å‰5ä¸ªæœ‰æ•ˆè®¡åˆ’:")
            for i, (uid, st) in enumerate(list(active_plans.items())[:5]):
                print(f"      {i+1}. {uid}: qty={st['qty']}, planned={st['planned']}, waiting={st['waiting']}")
        
        pending_rows = []
        # collect todays pendings
        for uid, st in agg_status.items():
            if st['qty'] <= 0:
                continue
            planned_date = pd.to_datetime(st['planned'])
            waiting_days = (sim_date - planned_date).days + 1
            if waiting_days > max_wait_days:
                continue
            
            # é›†æˆæ¨¡å¼ï¼šç›´æ¥å¤„ç†æ‰€æœ‰open deploymentæ•°æ®ï¼Œæ— éœ€æ—¶é—´è¿‡æ»¤
            # å› ä¸ºOrchestratorå·²ç»ç®¡ç†äº†éƒ¨ç½²è®¡åˆ’çš„ç”Ÿå‘½å‘¨æœŸï¼Œç¡®ä¿æ•°æ®çš„åˆç†æ€§
            if config_dict is not None:
                # é›†æˆæ¨¡å¼ï¼šä¿¡ä»»Orchestratorçš„æ•°æ®ï¼Œç›´æ¥å¤„ç†
                pass  # æ— éœ€é¢å¤–æ—¶é—´è¿‡æ»¤
            else:
                # ç‹¬ç«‹æ¨¡å¼ï¼šä¿æŒåŸé€»è¾‘ï¼Œåªå¤„ç†å½“å¤©åŠè¿‡å»çš„è®¡åˆ’
                if planned_date > sim_date:
                    continue
                    
            full = dp_dict[uid]
            
            # è°ƒè¯•ï¼šè®°å½•è·¨èŠ‚ç‚¹è®¡åˆ’çš„è¯¦ç»†ä¿¡æ¯
            route_type = "è‡ªå¾ªç¯" if full['sending'] == full['receiving'] else "è·¨èŠ‚ç‚¹"
            if route_type == "è·¨èŠ‚ç‚¹":
                print(f"    ğŸ” è·¨èŠ‚ç‚¹è®¡åˆ’ {uid}: {full['sending']}->{full['receiving']}, planned={planned_date.date()}, waiting={waiting_days}å¤©, qty={st['qty']}")
            
            pending_rows.append({
                'ori_deployment_uid': uid,
                'material': full['material'],
                'sending': full['sending'],
                'receiving': full['receiving'],
                'planned_deployment_date': planned_date,
                'deployed_qty': st['qty'],
                'demand_element': full['demand_element'],
                'demand_unit_to_weight': full['demand_unit_to_weight'],
                'demand_unit_to_volume': full['demand_unit_to_volume'],
                'priority': full['priority'],
                'waiting_days': waiting_days,
            })

        if not pending_rows:
            print("  âœ… æ— å¾…å¤„ç†éœ€æ±‚")
            continue

        pendf = pd.DataFrame(pending_rows)
        print(f"  ğŸ“¦ å‘ç° {len(pending_rows)} ä¸ªå¾…å¤„ç†éœ€æ±‚")
        
        # è°ƒè¯•ï¼šæŒ‰è·¯çº¿ç±»å‹ç»Ÿè®¡
        route_stats = pendf.groupby(['sending', 'receiving']).size().reset_index(name='count')
        print(f"  ğŸ“Š è·¯çº¿ç»Ÿè®¡:")
        for _, row in route_stats.iterrows():
            route_type = "è‡ªå¾ªç¯" if row['sending'] == row['receiving'] else "è·¨èŠ‚ç‚¹"
            print(f"    {row['sending']} -> {row['receiving']}: {row['count']} é¡¹ ({route_type})")
        
        # è¿‡æ»¤å‡ºè·¨èŠ‚ç‚¹è·¯çº¿ï¼ˆå¿½ç•¥è‡ªå¾ªç¯ï¼‰
        cross_node_df = pendf[pendf['sending'] != pendf['receiving']].copy()
        if cross_node_df.empty:
            print("  âœ… æ— è·¨èŠ‚ç‚¹éœ€æ±‚éœ€è¦å¤„ç†")
            continue
            
        # å…¨å±€ä¼˜å…ˆçº§æ’åºï¼šæŒ‰ä¼˜å…ˆçº§(asc) + è®¡åˆ’æ—¥æœŸ(asc) + è·¯çº¿æ’åºï¼Œç¡®ä¿é«˜ä¼˜å…ˆçº§éœ€æ±‚ä¼˜å…ˆè·å¾—åº“å­˜
        cross_node_df_sorted = cross_node_df.sort_values([
            'priority',  # 1=æœ€é«˜ä¼˜å…ˆçº§
            'planned_deployment_date',  # è¶Šæ—©è¶Šä¼˜å…ˆ
            'sending',  # ç¨³å®šæ’åº
            'receiving'  # ç¨³å®šæ’åº
        ]).reset_index(drop=True)
        
        print(f"  ğŸ¯ è·¨èŠ‚ç‚¹éœ€æ±‚å·²æŒ‰å…¨å±€ä¼˜å…ˆçº§æ’åº: {len(cross_node_df_sorted)} é¡¹")
        if len(cross_node_df_sorted) > 0:
            print(f"  ğŸ“‹ å‰5ä¸ªé«˜ä¼˜å…ˆçº§éœ€æ±‚:")
            for i, (_, row) in enumerate(cross_node_df_sorted.head().iterrows()):
                print(f"    {i+1}. {row['material']}@{row['sending']}->{row['receiving']}: qty={row['deployed_qty']}, priority={row['priority']}, date={row['planned_deployment_date'].date()}")
        
        # æ–°çš„å¤„ç†é€»è¾‘ï¼šæŒ‰å…¨å±€ä¼˜å…ˆçº§é€æ¡å¤„ç†éœ€æ±‚ï¼Œåœ¨è·¯çº¿çº§åˆ«è¿›è¡Œè½¦è¾†åˆ†é…
        processed_routes = set()  # è®°å½•å·²å¤„ç†çš„è·¯çº¿
        
        for _, row in cross_node_df_sorted.iterrows():
            sending = row['sending']
            receiving = row['receiving']
            route_key = (sending, receiving)
            
            # å¦‚æœè¯¥è·¯çº¿å·²ç»è¢«å¤„ç†è¿‡ï¼Œè·³è¿‡
            if route_key in processed_routes:
                continue
                
            processed_routes.add(route_key)
            
            # è·å–è¯¥è·¯çº¿çš„æ‰€æœ‰éœ€æ±‚ï¼ˆæŒ‰å…¨å±€ä¼˜å…ˆçº§æ’åºï¼‰
            route_demands = cross_node_df_sorted[
                (cross_node_df_sorted['sending'] == sending) & 
                (cross_node_df_sorted['receiving'] == receiving)
            ].copy()
            
            print(f"    ğŸšš å¤„ç†è·¨èŠ‚ç‚¹è·¯çº¿: {sending} -> {receiving} ({len(route_demands)} é¡¹éœ€æ±‚)")

            
            truck_cfgs = truck_con[(truck_con['sending']==sending) & (truck_con['receiving']==receiving)]
            if truck_cfgs.empty:
                print("      âš ï¸  è¯¥è·¯çº¿æ— å¯ç”¨å¡è½¦é…ç½®")
                continue

            # truck_type order by optimal first
            optimal_types = truck_cfgs[truck_cfgs['optimal_type']=='Y']['truck_type'].tolist()
            all_types = truck_cfgs['truck_type'].tolist()
            type_seq = optimal_types + [x for x in all_types if x not in optimal_types]
            print(f"      ğŸš› è½¦å‹åºåˆ—: {type_seq}")

            # ä½¿ç”¨å…¨å±€æ’åºåçš„éœ€æ±‚åˆ—è¡¨
            remaining_demands = route_demands.copy()
            # ä»…ç”¨äº UnsatisfiedMDQLog å±•ç¤º
            route_mdq = truck_cfgs['MDQ'].min() if not truck_cfgs.empty else np.nan
            
            # try each truck type
            for truck_type in type_seq:
                if remaining_demands.empty:
                    break

                n_truck_total = int(cap_map.get((sim_date, sending, receiving, truck_type), 0))
                if n_truck_total == 0:
                    print(f"      ğŸš« {truck_type}: ä»Šæ—¥æ— å¯ç”¨è½¦è¾†")
                    continue

                conf = truck_cfgs[truck_cfgs['truck_type']==truck_type].iloc[0]
                spec = spec_map.get(truck_type)
                if not spec:
                    print(f"      âš ï¸  {truck_type}: è½¦å‹è§„æ ¼ç¼ºå¤±ï¼Œè·³è¿‡")
                    continue

                wfr_th, vfr_th = float(conf['WFR']), float(conf['VFR'])
                mdq = float(conf['MDQ']) if pd.notna(conf['MDQ']) else 0.0
                cap_w = float(spec['capacity_qty_in_weight'])
                cap_v = float(spec['capacity_qty_in_volume'])
                print(f"      ğŸš› å°è¯•è½¦å‹: {truck_type} (å¯ç”¨: {n_truck_total} è¾†) é˜ˆå€¼ WFR={wfr_th}, VFR={vfr_th}")

                used = 0  # å·²ç”¨è½¦è¾†æ•°ï¼ˆè¯¥è½¦å‹ï¼‰
                while used < n_truck_total and not remaining_demands.empty:
                    # ä¸€è¾†è½¦çš„â€œæ‰“åŒ…å™¨â€ï¼šä¸¥æ ¼ä¸è¶…å®¹é‡ï¼›å…è®¸å¯¹æœ€åä¸€ä¸ªéœ€æ±‚â€œéƒ¨åˆ†è£…è½½â€
                    load_records = []   # {idx, load_qty}
                    q_units = 0.0
                    w_sum = 0.0
                    v_sum = 0.0
                    # åº“å­˜æ£€æŸ¥ï¼šè·Ÿè¸ªæŒ‰ç‰©æ–™çš„ç´¯è®¡è£…è½½é‡
                    material_loaded = {}  # {material: total_loaded_qty}

                    # â€”â€” åˆæ¬¡æ‰«æï¼šå°½é‡è£…å…¥ï¼Œä½†ä¸è¶…è¿‡ capï¼ˆæ—  âˆï¼‰â€”â€”
                    for idx, demand_row in remaining_demands.iterrows():
                        qty_pending = float(demand_row['deployed_qty'])
                        if qty_pending <= 0:
                            continue
                        uw = float(demand_row['demand_unit_to_weight'])
                        uv = float(demand_row['demand_unit_to_volume'])
                        material = demand_row['material']

                        cap_w_rem = max(0.0, cap_w - w_sum)
                        cap_v_rem = max(0.0, cap_v - v_sum)

                        limits = [qty_pending]            # è®¢å•å‰©ä½™é‡ç¡¬ä¸Šé™
                        if uw > 0: limits.append(floor(cap_w_rem / uw))
                        if uv > 0: limits.append(floor(cap_v_rem / uv))
                        
                        # åº“å­˜æ£€æŸ¥ï¼šé™åˆ¶å‘è´§é‡ä¸è¶…è¿‡å¯ç”¨åº“å­˜
                        if inventory_check_enabled:
                            inv_key = (material, sending)
                            available_qty = available_inventory.get(inv_key, 0)
                            already_loaded = material_loaded.get(material, 0)
                            inventory_limit = max(0, available_qty - already_loaded)
                            limits.append(inventory_limit)
                            
                            if inventory_limit <= 0:
                                # åº“å­˜ä¸è¶³ï¼Œè·³è¿‡è¯¥ç‰©æ–™
                                print(f"        ğŸš« åº“å­˜ä¸è¶³: {material}@{sending}, å¯ç”¨={available_qty}, å·²è£…={already_loaded}")
                                continue

                        addable = int(max(0, min(limits)))
                        if addable <= 0:
                            continue

                        load_records.append({'idx': idx, 'load_qty': addable, 'demand_row': demand_row})
                        q_units += addable
                        w_sum  += addable * uw
                        v_sum  += addable * uv
                        
                        # æ›´æ–°ç‰©æ–™çš„ç´¯è®¡è£…è½½é‡
                        material_loaded[material] = material_loaded.get(material, 0) + addable

                        if w_sum >= cap_w or v_sum >= cap_v:
                            break

                    # å½“å‰è½¦è£…è½½æ¯”ä¾‹
                    wfr = (w_sum / cap_w) if cap_w > 0 else 0.0
                    vfr = (v_sum / cap_v) if cap_v > 0 else 0.0

                    # ä»£è¡¨æ€§ä¸Šä¸‹æ–‡
                    if load_records:
                        # ä½¿ç”¨å·²è£…è½½è®°å½•ä¸­ä¼˜å…ˆçº§æœ€é«˜çš„éœ€æ±‚
                        highest_record = min(
                            load_records,
                            key=lambda r: (r['demand_row']['priority'], r['demand_row']['planned_deployment_date'])
                        )
                        repr_type = highest_record['demand_row']['demand_element']
                        repr_wait = highest_record['demand_row']['waiting_days']
                    else:
                        repr_type, repr_wait = None, 0

                    context = {
                        'sending': sending, 'receiving': receiving, 'truck_type': truck_type,
                        'demand_element': repr_type, 'waiting_days': repr_wait,
                        'deployed_qty_ratio': (q_units/mdq) if mdq > 0 else 0.0,
                        'exception_MDQ': 1 if mdq == 0 else 0
                    }
                    bypass, rule_id = should_bypass_mdq(context, bypass_rules, evaluator)

                    trigger_cause = None
                    if load_records and (wfr >= wfr_th or vfr >= vfr_th):
                        trigger_cause = 'threshold'
                    elif load_records and bypass:
                        trigger_cause = 'bypass'

                    if trigger_cause:
                        # â€”â€” è§¦å‘åå†å°½é‡è´´è¿‘ 1.0ï¼ˆä»ä¸è¶…ï¼‰â€”â€”
                        taken = {r['idx'] for r in load_records}
                        for idx, demand_row in remaining_demands.iterrows():
                            if idx in taken:
                                continue
                            qty_pending = float(demand_row['deployed_qty'])
                            if qty_pending <= 0:
                                continue
                            uw = float(demand_row['demand_unit_to_weight'])
                            uv = float(demand_row['demand_unit_to_volume'])
                            material = demand_row['material']

                            cap_w_rem = max(0.0, cap_w - w_sum)
                            cap_v_rem = max(0.0, cap_v - v_sum)

                            limits = [qty_pending]
                            if uw > 0: limits.append(floor(cap_w_rem / uw))
                            if uv > 0: limits.append(floor(cap_v_rem / uv))
                            
                            # åº“å­˜æ£€æŸ¥ï¼šé™åˆ¶å‘è´§é‡ä¸è¶…è¿‡å¯ç”¨åº“å­˜
                            if inventory_check_enabled:
                                inv_key = (material, sending)
                                available_qty = available_inventory.get(inv_key, 0)
                                already_loaded = material_loaded.get(material, 0)
                                inventory_limit = max(0, available_qty - already_loaded)
                                limits.append(inventory_limit)
                                
                                if inventory_limit <= 0:
                                    continue

                            addable = int(max(0, min(limits)))
                            if addable <= 0:
                                continue

                            load_records.append({'idx': idx, 'load_qty': addable, 'demand_row': demand_row})
                            q_units += addable
                            w_sum  += addable * uw
                            v_sum  += addable * uv
                            
                            # æ›´æ–°ç‰©æ–™çš„ç´¯è®¡è£…è½½é‡
                            material_loaded[material] = material_loaded.get(material, 0) + addable
                            
                            if w_sum >= cap_w or v_sum >= cap_v:
                                break

                        # æ›´æ–°æ¯”ä¾‹ï¼ˆæœ€ç»ˆå€¼ï¼‰
                        wfr = (w_sum / cap_w) if cap_w > 0 else 0.0
                        vfr = (v_sum / cap_v) if cap_v > 0 else 0.0

                        # === è½¦è¾†çº§æ—¥å¿—ï¼ˆé€è½¦ä¸€æ¡ï¼‰ ===
                        vehicle_no = used + 1
                        vehicle_uid = f"{sim_date:%Y%m%d}-{sending}-{receiving}-{truck_type}-#{vehicle_no}"
                        vehicle_log.append({
                            'date': sim_date,
                            'sending': sending, 'receiving': receiving, 'truck_type': truck_type,
                            'vehicle_no': vehicle_no,
                            'vehicle_uid': vehicle_uid,
                            'total_units': int(q_units),
                            'total_weight': w_sum,
                            'total_volume': v_sum,
                            'WFR': min(wfr, 1.0),
                            'VFR': min(vfr, 1.0),
                            'trigger': trigger_cause
                        })

                        # === æ˜ç»†å‘è¿è®°å½•ç”Ÿæˆ ===
                        for rec in load_records:
                            # ä½¿ç”¨demand_rowè€Œä¸æ˜¯group.loc
                            sub = rec['demand_row']
                            uid = sub['ori_deployment_uid']
                            lt  = lead_time[(lead_time['sending']==sending) & (lead_time['receiving']==receiving)]
                            PDT = int(lt['PDT'].iloc[0]) if not lt.empty else 0
                            GR  = int(lt['GR'].iloc[0])  if not lt.empty else 0
                            delay = sample_delivery_delay(sending, receiving, delay_dist)
                            #actual_delivery_date çš„æ–°å®šä¹‰æ˜¯ actual_ship_date + OTD + GR + delayï¼›
                            #PDT çš„è§’è‰²ä»â€œå®šä¹‰â€é™çº§ä¸ºâ€œè®¡åˆ’ç”¨ä¼°è®¡å€¼â€ï¼ˆä»…ç”¨äº M5 å€’æ’ã€æ’ç¨‹é¢„ä¼°ï¼Œè€Œéå®é™…ç‰©æµå…¥åº“æ—¶æ•ˆï¼‰ã€‚
                            lt = lead_time[(lead_time['sending']==sending) & (lead_time['receiving']==receiving)]
                            if lt.empty:
                                raise ValueError(f"ç¼ºå°‘è·¯çº¿ {sending}->{receiving} çš„ LeadTime è¡Œ")

                            OTD = int(pd.to_numeric(lt['OTD'].iloc[0])) if 'OTD' in lt.columns else 0
                            GR  = int(pd.to_numeric(lt['GR'].iloc[0]))  if 'GR'  in lt.columns else 0

                            OTD = max(0, OTD)
                            GR  = max(0, GR)

                            ship_date = sim_date  # å‘è¿æ—¥å®šä¹‰
                            eta = ship_date + pd.Timedelta(days=OTD + GR + delay)

                            delivery_plan.append({
                                'vehicle_uid': vehicle_uid,
                                'ori_deployment_uid': uid,
                                'material': sub['material'],
                                'sending': sending, 'receiving': receiving,
                                'planned_deployment_date': sub['planned_deployment_date'],
                                'actual_ship_date': ship_date,
                                'actual_delivery_date': eta,
                                'delivery_qty': rec['load_qty'], 'truck_type': truck_type,
                                'truck_load_pct': min(max(wfr, vfr), 1.0),
                                'WFR': min(wfr, 1.0), 'VFR': min(vfr, 1.0)
                            })
                            
                            # æ³¨æ„: åº“å­˜æ‰£å‡ç”±Orchestratorç»Ÿä¸€å¤„ç†ï¼ŒModule6åªè´Ÿè´£ç”Ÿæˆdelivery plan
                            # æ›´æ–°å¯ç”¨åº“å­˜ï¼ˆä»…ç”¨äºModule6å†…éƒ¨çš„åç»­è§„åˆ’è®¡ç®—ï¼‰
                            if inventory_check_enabled:
                                inv_key = (sub['material'], sending)
                                if inv_key in available_inventory:
                                    available_inventory[inv_key] -= rec['load_qty']
                                    available_inventory[inv_key] = max(0, available_inventory[inv_key])  # é¿å…è´Ÿæ•°
                            
                            # æ›´æ–°agg_statuså’ŒåŸå§‹æ•°æ®
                            agg_status[uid]['qty'] = max(0, agg_status[uid]['qty'] - rec['load_qty'])
                            # æ›´æ–°remaining_demandsä¸­çš„æ•°é‡
                            remaining_demands.at[rec['idx'], 'deployed_qty'] = max(0, sub['deployed_qty'] - rec['load_qty'])

                        # åªåœ¨â€œbypass è§¦å‘â€æ—¶è®°å½•å‘½ä¸­ï¼ˆé˜ˆå€¼è§¦å‘ä¸è®°ï¼‰
                        if trigger_cause == 'bypass':
                            for rec in load_records:
                                sub = rec['demand_row']
                                bypass_log.append({
                                    'ori_deployment_uid': sub['ori_deployment_uid'],
                                    'rule_id': rule_id, 'simulation_date': sim_date,
                                    'context_snapshot': str(context),
                                    'vehicle_uid': vehicle_uid
                                })

                        # æ›´æ–°remaining_demandsï¼Œç§»é™¤å·²å¤„ç†å®Œæ¯•çš„éœ€æ±‚
                        remaining_demands = remaining_demands[remaining_demands['deployed_qty'] > 0].copy()
                        used += 1
                    else:
                        print(f"        ğŸš« {truck_type}: èšåˆæœªè§¦å‘å‘è¿ï¼Œå‰©ä½™ {len(remaining_demands)} é¡¹")
                        break  # æ¢è½¦å‹

            # è·¯çº¿çº§æ”¶å°¾ï¼šå¤„ç†è¯¥è·¯çº¿å‰©ä½™çš„æœªå‘å‡ºéœ€æ±‚
            route_remaining = route_demands[route_demands['deployed_qty'] > 0]
            for _, row in route_remaining.iterrows():
                uid = row['ori_deployment_uid']
                if agg_status[uid]['qty'] <= 0:
                    continue
                agg_status[uid]['waiting'] += 1
                if agg_status[uid]['waiting'] > max_wait_days:
                    unsat_log.append({
                        'ori_deployment_uid': uid, 'material': row['material'],
                        'sending': sending, 'receiving': receiving, 'demand_element': row['demand_element'],
                        'planned_deployment_date': row['planned_deployment_date'],
                        'simulation_date': sim_date, 'waiting_days': agg_status[uid]['waiting'],
                        'accumulated_qty': agg_status[uid]['qty'], 'min_MDQ': route_mdq,
                        'reason': 'waited_too_long'
                    })
                    agg_status[uid]['qty'] = 0

    # ---------------------- Usage summary ----------------------
    if vehicle_log:
        vehicle_df = pd.DataFrame(vehicle_log)
        usage = (vehicle_df
                 .groupby(['date','sending','receiving','truck_type'], as_index=False)
                 .agg(truck_used=('vehicle_uid','nunique')))
    else:
        vehicle_df = pd.DataFrame(columns=['date','sending','receiving','truck_type','vehicle_no','vehicle_uid',
                                           'total_units','total_weight','total_volume','WFR','VFR','trigger'])
        usage = pd.DataFrame(columns=['date','sending','receiving','truck_type','truck_used'])

    # ---------------------- Write outputs ----------------------
    delivery_plan_df = pd.DataFrame(delivery_plan)
    vehicle_df_final = vehicle_df if vehicle_log else pd.DataFrame(columns=[
        'date','sending','receiving','truck_type','vehicle_no','vehicle_uid',
        'total_units','total_weight','total_volume','WFR','VFR','trigger'
    ])
    usage_df = usage if vehicle_log else pd.DataFrame(columns=['date','sending','receiving','truck_type','truck_used'])
    unsat_df = pd.DataFrame(unsat_log)
    validation_df = pd.DataFrame(validation_log)
    bypass_df = pd.DataFrame(bypass_log)
    
    # Excel è¾“å‡º
    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:
        delivery_plan_df.to_excel(writer, sheet_name='DeliveryPlan', index=False)
        vehicle_df_final.to_excel(writer, sheet_name='VehicleLog', index=False)
        usage_df.to_excel(writer, sheet_name='TruckUsageLog', index=False)
        unsat_df.to_excel(writer, sheet_name='UnsatisfiedMDQLog', index=False)
        validation_df.to_excel(writer, sheet_name='ValidationLog', index=False)
        bypass_df.to_excel(writer, sheet_name='BypassRuleHitLog', index=False)
    
    # ç”Ÿæˆvalidation.txtæŠ¥å‘Š
    _generate_validation_report(validation_log, output_file)

    # æ³¨æ„ï¼šOrchestratorçŠ¶æ€æ›´æ–°ç”±main_integration.pyç»Ÿä¸€å¤„ç†
    # é¿å…é‡å¤è°ƒç”¨å¯¼è‡´åŒé‡åº“å­˜æ‰£å‡
    if config_dict is not None and orchestrator is not None and not delivery_plan_df.empty:
        print(f"âœ… Processed {len(delivery_plan_df)} M6 delivery plans for {current_date}")
        print(f"âœ… Orchestrator çŠ¶æ€å°†ç”±main_integrationç»Ÿä¸€æ›´æ–°")
        # ç§»é™¤ç›´æ¥è°ƒç”¨ï¼Œé¿å…ä¸main_integration.pyä¸­çš„process_module6_deliveryé‡å¤

    statistics = {
        'delivery_count': len(delivery_plan),
        'vehicle_count': usage_df['truck_used'].sum() if not usage_df.empty else 0,
        'unsatisfied_count': len(unsat_log),
        'bypass_count': len(bypass_log)
    }
    
    print(f"\nğŸ‰ ä»¿çœŸå®Œæˆ! è¾“å‡ºå·²ä¿å­˜è‡³: {output_file}")
    try:
        print(f"ğŸ“Š ç»Ÿè®¡: å‘è¿ {statistics['delivery_count']} æ˜ç»†ï¼Œè½¦è¾† {statistics['vehicle_count']} è½¦ï¼Œæœªæ»¡è¶³ {statistics['unsatisfied_count']} é¡¹ï¼Œbypass å‘½ä¸­ {statistics['bypass_count']} æ¬¡")
    except Exception as e:
        print(f"[WARN] printing statistics failed: {e}")
    # è¿”å›ç»“æœç”¨äºé›†æˆæ¨¡å¼
    return {
        'delivery_plan': delivery_plan_df,
        'vehicle_log': vehicle_df_final,
        'truck_usage': usage_df,
        'unsatisfied_log': unsat_df,
        'validation_log': validation_df,
        'bypass_log': bypass_df,
        'statistics': statistics
    }

# ä¸»å‡½æ•°åˆ«åï¼ˆä¿æŒä¸Module4/5ä¸€è‡´ï¼‰
main = run_physical_flow_module


# ======================== Example ========================
if __name__ == "__main__":
    # Standalone mode example
    run_physical_flow_module(
        input_excel='Module_6_1_1/config_SC.xlsx',   
        simulation_start='2025-08-01',
        simulation_end='2025-08-03',
        output_excel='Module_6_1_1/output_SC.xlsx',
        random_seed=42
    )
