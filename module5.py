#module 5
import pandas as pd
import numpy as np
import os
from datetime import timedelta
from typing import Dict, List
from functools import lru_cache

# ========= é›†æˆæ•°æ®åŠ è½½å‡½æ•° (æ–°å¢) =========

def _normalize_location(location_str) -> str:
    """Normalize location string by padding with leading zeros to 4 digits"""
    # Handle None and pandas NA
    if location_str is None or pd.isna(location_str):
        return ""
    try:
        return str(int(location_str)).zfill(4)
    except (ValueError, TypeError):
        return str(location_str).zfill(4)

def _normalize_material(material_str) -> str:
    """Normalize material string"""
    # Handle None and pandas NA
    if material_str is None or pd.isna(material_str):
        return ""
    return str(material_str)

def _normalize_identifiers(df: pd.DataFrame) -> pd.DataFrame:
    """Normalize identifier columns to string format with proper formatting"""
    if df.empty:
        return df
    
    # Define identifier columns that need string conversion
    identifier_cols = ['material', 'location', 'sending', 'receiving', 'sourcing']
    
    df = df.copy()
    for col in identifier_cols:
        if col in df.columns:
            # Convert to string and handle NaN values
            df[col] = df[col].astype('string')
            # Apply normalization functions
            if col in ['location', 'sending', 'receiving', 'sourcing']:
                # Normalization for location-type fields
                df[col] = df[col].apply(_normalize_location)
            # Apply specific normalization for material
            elif col == 'material':
                df[col] = df[col].apply(_normalize_material)
            # For other identifier columns, ensure they are properly formatted strings
            else:
                # Vectorized string conversion
                df[col] = df[col].fillna('').astype(str)
    
    return df

def load_module1_daily_shipment(module1_output_dir: str, current_date: pd.Timestamp) -> pd.DataFrame:
    """
    ä»Module1è¾“å‡ºåŠ è½½å½“æ—¥å‘è´§æ•°æ®
    
    Args:
        module1_output_dir: Module1è¾“å‡ºç›®å½•
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        pd.DataFrame: å½“æ—¥å‘è´§æ•°æ® [date, material, location, quantity]
    """
    try:
        date_str = current_date.strftime('%Y%m%d')
        module1_file = f"{module1_output_dir}/module1_output_{date_str}.xlsx"
        
        if os.path.exists(module1_file):
            xl = pd.ExcelFile(module1_file)
            if 'ShipmentLog' in xl.sheet_names:
                shipment_df = xl.parse('ShipmentLog')
                # ç¡®ä¿åŒ…å«éœ€è¦çš„åˆ—
                required_cols = ['date', 'material', 'location', 'quantity']
                if all(col in shipment_df.columns for col in required_cols):
                    result_df = shipment_df[required_cols].copy()
                    # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    return _normalize_identifiers(result_df)
                else:
                    print(f"âš ï¸  Module1è¾“å‡ºæ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {module1_file}")
            else:
                print(f"âš ï¸  Module1è¾“å‡ºæ–‡ä»¶ä¸­æ— ShipmentLogè¡¨: {module1_file}")
        else:
            print(f"âš ï¸  Module1è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {module1_file}")
    except Exception as e:
        print(f"âš ï¸  åŠ è½½Module1å‘è´§æ•°æ®å¤±è´¥: {e}")
    
    # è¿”å›ç©ºDataFrame
    return pd.DataFrame(columns=['date', 'material', 'location', 'quantity'])

def load_module1_daily_orders(module1_output_dir: str, current_date: pd.Timestamp) -> pd.DataFrame:
    """
    ä»Module1è¾“å‡ºåŠ è½½"å½“æ—¥ç‰ˆæœ¬"çš„è®¢å•æ—¥å¿—ï¼ˆåŒ…å«å†å²å¤©ç”Ÿæˆä½†å°šæœªæ¥åˆ°æœŸçš„è®¢å• + å½“å¤©æ–°ç”Ÿæˆï¼‰
    ä»…æŒ‰ requirement_date>=current_date è¿‡æ»¤ï¼Œä¸æŒ‰ simulation_date è¿‡æ»¤
    è¿”å›åˆ—: [date, material, location, demand_type, quantity, simulation_date]
    """
    cols = ['date', 'material', 'location', 'demand_type', 'quantity', 'simulation_date']
    try:
        date_str = current_date.strftime('%Y%m%d')
        module1_file = f"{module1_output_dir}/module1_output_{date_str}.xlsx"
        if not os.path.exists(module1_file):
            print(f"âš ï¸  Module1è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {module1_file}")
            return pd.DataFrame(columns=cols)

        xl = pd.ExcelFile(module1_file)
        if 'OrderLog' not in xl.sheet_names:
            print(f"âš ï¸  Module1è¾“å‡ºæ–‡ä»¶ä¸­æ— OrderLogè¡¨: {module1_file}")
            return pd.DataFrame(columns=cols)

        df = xl.parse('OrderLog')
        for c in ['date', 'simulation_date']:
            if c in df.columns:
                df[c] = pd.to_datetime(df[c])
        # åªä¿ç•™ requirement_date(=date) >= today çš„è®¢å•è¡Œ
        if 'date' in df.columns:
            df = df[df['date'] >= current_date]

        # è§„èŒƒåˆ—
        for c in cols:
            if c not in df.columns:
                df[c] = pd.NaT if c in ['date','simulation_date'] else np.nan
        result_df = df[cols].copy()
        # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
        return _normalize_identifiers(result_df)

    except Exception as e:
        print(f"âš ï¸  åŠ è½½Module1è®¢å•æ•°æ®å¤±è´¥: {e}")
        return pd.DataFrame(columns=cols)

def load_orchestrator_delivery_gr(orchestrator: object, current_date: pd.Timestamp) -> pd.DataFrame:
    """
    ä»OrchestratoråŠ è½½å½“æ—¥æ”¶è´§æ•°æ®
    
    Args:
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        pd.DataFrame: å½“æ—¥æ”¶è´§æ•°æ® [date, material, receiving, quantity]
    """
    try:
        date_str = current_date.strftime('%Y-%m-%d')
        delivery_gr_view = orchestrator.get_delivery_gr_view(date_str)
        
        if isinstance(delivery_gr_view, pd.DataFrame) and not delivery_gr_view.empty:
            # ç¡®ä¿åŒ…å«éœ€è¦çš„åˆ—
            required_cols = ['date', 'material', 'receiving', 'quantity']
            available_cols = delivery_gr_view.columns.tolist()
            
            # å°è¯•æ˜ å°„åˆ—åç§°
            col_mapping = {
                'location': 'receiving',  # location æ˜ å°„ä¸º receiving
                'gr_qty': 'quantity',     # gr_qty æ˜ å°„ä¸º quantity
                'received_qty': 'quantity'  # received_qty æ˜ å°„ä¸º quantity
            }
            
            # åº”ç”¨åˆ—æ˜ å°„
            renamed_df = delivery_gr_view.copy()
            for old_col, new_col in col_mapping.items():
                if old_col in renamed_df.columns:
                    renamed_df = renamed_df.rename(columns={old_col: new_col})
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            missing_cols = [col for col in required_cols if col not in renamed_df.columns]
            if not missing_cols:
                result_df = renamed_df[required_cols].copy()
                # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
                return _normalize_identifiers(result_df)
            else:
                print(f"âš ï¸  Orchestrator delivery_gr_viewç¼ºå°‘å­—æ®µ: {missing_cols}")
        else:
            print(f"âš ï¸  Orchestratorè¿”å›ç©ºçš„delivery_gr_view")
    except Exception as e:
        print(f"âš ï¸  ä»OrchestratoråŠ è½½æ”¶è´§æ•°æ®å¤±è´¥: {e}")
    
    # è¿”å›ç©ºDataFrame
    return pd.DataFrame(columns=['date', 'material', 'receiving', 'quantity'])

def load_orchestrator_open_deployment(orchestrator: object, current_date: pd.Timestamp) -> pd.DataFrame:
    """
    ä»OrchestratoråŠ è½½å¼€æ”¾è°ƒæ‹¨æ•°æ®
    
    Args:
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        pd.DataFrame: å¼€æ”¾è°ƒæ‹¨æ•°æ® [material, sending, receiving, quantity]
    """
    try:
        date_str = current_date.strftime('%Y-%m-%d')
        open_deployment_view = orchestrator.get_open_deployment_view(date_str)
        
        if isinstance(open_deployment_view, pd.DataFrame) and not open_deployment_view.empty:
            # ç¡®ä¿åŒ…å«éœ€è¦çš„åˆ—ï¼ˆåŒ…æ‹¬receivingç”¨äºè‡ªå¾ªç¯æ£€æŸ¥ï¼‰
            required_cols = ['material', 'sending', 'receiving', 'quantity']
            available_cols = open_deployment_view.columns.tolist()
            
            # å°è¯•æ˜ å°„åˆ—åç§°
            col_mapping = {
                'location': 'sending',     # location æ˜ å°„ä¸º sending
                'deployed_qty': 'quantity',  # deployed_qty æ˜ å°„ä¸º quantity
                'planned_qty': 'quantity'    # planned_qty æ˜ å°„ä¸º quantity
            }
            
            # åº”ç”¨åˆ—æ˜ å°„
            renamed_df = open_deployment_view.copy()
            for old_col, new_col in col_mapping.items():
                if old_col in renamed_df.columns:
                    renamed_df = renamed_df.rename(columns={old_col: new_col})
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            missing_cols = [col for col in required_cols if col not in renamed_df.columns]
            if not missing_cols:
                result_df = renamed_df[required_cols].copy()
                # ç¡®ä¿æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
                return _normalize_identifiers(result_df)
            else:
                print(f"âš ï¸  Orchestrator open_deployment_viewç¼ºå°‘å­—æ®µ: {missing_cols}")
        else:
            print(f"âš ï¸  Orchestratorè¿”å›ç©ºçš„open_deployment_view")
    except Exception as e:
        print(f"âš ï¸  ä»OrchestratoråŠ è½½å¼€æ”¾è°ƒæ‹¨æ•°æ®å¤±è´¥: {e}")
    
    # è¿”å›ç©ºDataFrame
    return pd.DataFrame(columns=['material', 'sending', 'receiving', 'quantity'])

def build_open_deployment_inbound(open_deployment_df: pd.DataFrame) -> dict[tuple[str, str], int]:
    """
    ä» open_deployment æ˜ç»†æ„é€  inbound è§†å›¾ï¼š
    - ç»´åº¦ï¼š (material, receiving)
    - è¿‡æ»¤ï¼šsending != receivingï¼ˆæ’é™¤è‡ªå¾ªç¯ï¼‰ï¼›deployed_qty/quantity > 0
    - æ±‡æ€»ï¼šsum(quantity)
    è¿”å›ï¼š{(material, receiving): qty}
    """
    if open_deployment_df is None or open_deployment_df.empty:
        return {}

    df = open_deployment_df.copy()
    # ç»Ÿä¸€æ•°é‡åˆ—å
    if 'quantity' not in df.columns and 'deployed_qty' in df.columns:
        df = df.rename(columns={'deployed_qty': 'quantity'})
    if 'quantity' not in df.columns:
        # å…œåº•ï¼šå¦‚æœå« planned_qty
        if 'planned_qty' in df.columns:
            df = df.rename(columns={'planned_qty': 'quantity'})
        else:
            return {}

    # è¿‡æ»¤ï¼šæ•°é‡>0ï¼Œä¸”éè‡ªå¾ªç¯
    df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0).astype(int)
    df = df[(df['quantity'] > 0) & (df['sending'] != df['receiving'])]

    # èšåˆï¼š (material, receiving)
    g = (df.groupby(['material', 'receiving'])['quantity']
           .sum().reset_index())

    # Performance optimization: Use dict comprehension with itertuples
    inbound = {(row.material, row.receiving): int(row.quantity) for row in g.itertuples(index=False)}
    return inbound

def calculate_projected_inventory(
    beginning_inventory: dict,
    in_transit: dict, 
    delivery_gr: dict,
    today_production_gr: dict,
    future_production: dict,
    today_shipment: dict,
    open_deployment: dict
) -> dict:
    """
    è®¡ç®—é¢„æµ‹åº“å­˜ï¼Œç”¨äºgapè®¡ç®—å’Œä¾›åº”é“¾è§„åˆ’
    
    Formula: projected_inventory = beginning_inventory + in_transit + delivery_gr + 
             today_production + future_production - today_shipment - open_deployment
    
    Args:
        å„ä¸ªåº“å­˜ç»´åº¦çš„å­—å…¸ï¼Œé”®ä¸º(material, location)ï¼Œå€¼ä¸ºæ•°é‡
        
    Returns:
        dict: é¢„æµ‹åº“å­˜å­—å…¸ {(material, location): quantity}
    """
    all_keys = set()
    for d in [beginning_inventory, in_transit, delivery_gr, today_production_gr, 
              future_production, today_shipment, open_deployment]:
        all_keys.update(d.keys())
    
    projected_inventory = {}
    for key in all_keys:
        projected_inventory[key] = (
            beginning_inventory.get(key, 0) +
            in_transit.get(key, 0) +
            delivery_gr.get(key, 0) +
            today_production_gr.get(key, 0) +
            future_production.get(key, 0) -
            today_shipment.get(key, 0) -
            open_deployment.get(key, 0)
        )
    
    return projected_inventory

def calculate_available_inventory(
    beginning_inventory: dict,
    delivery_gr: dict,
    today_production_gr: dict,
    today_shipment: dict,
    open_deployment: dict,
    open_deployment_inbound: dict
) -> dict:
    """
    è®¡ç®—å½“æ—¥çœŸå®å¯ç”¨åº“å­˜ï¼ˆdynamic_sohï¼‰ï¼Œç”¨äºå®é™…åˆ†é…

    ç°åœ¨çº¦å®šï¼š
    dynamic_soh = beginning + delivery_gr + today_production_gr - open_deployment

    open_deployment_inbound ä¸å†è®¡å…¥å½“æ—¥ç°è´§ï¼Œåªä½œä¸º pipeline supply ä½¿ç”¨
    """
    all_keys = set()
    for d in [beginning_inventory, delivery_gr, today_production_gr,
              today_shipment, open_deployment]:
        all_keys.update(d.keys())

    soh = {}
    for key in all_keys:
        soh[key] = (
            beginning_inventory.get(key, 0) +
            delivery_gr.get(key, 0) +
            today_production_gr.get(key, 0) -
            # today_shipment.get(key, 0) -   # å¦‚éœ€æ‰£å‡å½“æ—¥å¯¹å®¢å‘è´§å¯æ”¾å¼€
            open_deployment.get(key, 0)
        )
    return soh


# ========= 1. é€šç”¨è¾…åŠ© =========

def get_upstream(location, material, network_df, sim_date, 
                 active_network_cache=None):
    row = get_active_network(network_df, material, location, sim_date, cache=active_network_cache)
    if not row.empty:
        return row.iloc[0]['sourcing']
    return None

def apply_moq_rv(qty, moq, rv, is_cross_node=True):
    """
    è¡¥è´§é‡å°äºmoqè¡¥moqï¼Œå¦åˆ™å‘ä¸Šå–æ•´åˆ°rvçš„å€æ•°
    
    Args:
        qty: éœ€æ±‚æ•°é‡
        moq: æœ€å°è®¢è´§é‡
        rv: é‡è®¢é‡(Round Volume)
        is_cross_node: æ˜¯å¦ä¸ºè·¨èŠ‚ç‚¹è°ƒè¿ã€‚True=è·¨èŠ‚ç‚¹éœ€è¦åº”ç”¨MOQ/RVï¼ŒFalse=è‡ªå¾ªç¯ä¸åº”ç”¨MOQ/RV
    """
    if qty <= 0:
        return 0
    
    # ğŸ”§ ä¿®å¤ï¼šè‡ªå¾ªç¯è°ƒè¿ä¸åº”ç”¨MOQ/RVçº¦æŸï¼Œç›´æ¥è¿”å›åŸéœ€æ±‚é‡
    if not is_cross_node:
        return qty
    
    # è·¨èŠ‚ç‚¹è°ƒè¿åº”ç”¨MOQ/RVçº¦æŸ
    if qty < moq:
        return moq
    return int(np.ceil(qty / rv)) * rv

def apply_grouped_moq_rv(demand_rows, location):
    """
    æŒ‰è°ƒè¿è·¯å¾„åˆ†ç»„åº”ç”¨MOQ/RV
    
    Args:
        demand_rows: éœ€æ±‚è¡Œåˆ—è¡¨
        location: å½“å‰ä½ç½®ï¼ˆsendingï¼‰
        
    Returns:
        dict: è°ƒæ•´åçš„ demand_row_index -> adjusted_qty æ˜ å°„
    """
    # æŒ‰ (material, sending, receiving, demand_element) åˆ†ç»„
    route_groups = {}
    for i, d in enumerate(demand_rows):
        receiving = d.get('from_location', d.get('receiving', location))
        is_cross_node = (location != receiving)
        
        route_key = (d['material'], location, receiving, d['demand_element'])
        if route_key not in route_groups:
            route_groups[route_key] = {
                'items': [],
                'total_qty': 0,
                'is_cross_node': is_cross_node,
                'moq': d['moq'],
                'rv': d['rv']
            }
        
        route_groups[route_key]['items'].append((i, d))
        route_groups[route_key]['total_qty'] += d['demand_qty']
    
    # å¯¹æ¯ä¸ªè·¯å¾„ç»„åº”ç”¨MOQ/RV
    adjusted_qtys = {}
    
    for route_key, group in route_groups.items():
        material, sending, receiving, demand_element = route_key
        total_qty = group['total_qty']
        is_cross_node = group['is_cross_node']
        moq = group['moq']
        rv = group['rv']
        
        # å¯¹ç»„åˆåçš„æ€»é‡åº”ç”¨MOQ/RV
        adjusted_total = apply_moq_rv(total_qty, moq, rv, is_cross_node=is_cross_node)
        
        # print(f"      ğŸ“¦ è·¯å¾„ç»„ {sending}â†’{receiving} [{demand_element}]: åŸå§‹={total_qty} â†’ è°ƒæ•´={adjusted_total} (MOQ={moq}, è·¨èŠ‚ç‚¹={is_cross_node})")
        
        # å°†è°ƒæ•´åçš„æ€»é‡æŒ‰åŸå§‹æ¯”ä¾‹åˆ†é…å›å„ä¸ªéœ€æ±‚é¡¹
        if total_qty > 0:
            adjustment_ratio = adjusted_total / total_qty
        else:
            adjustment_ratio = 1.0
            
        for item_idx, item in group['items']:
            original_qty = item['demand_qty']
            adjusted_qty = int(original_qty * adjustment_ratio)
            adjusted_qtys[item_idx] = adjusted_qty
            
    return adjusted_qtys

def _build_ptf_lsk_cache(m4_mlcfg_df: pd.DataFrame | None) -> Dict[tuple[str, str], tuple[int, int]]:
    """Build cache for PTF/LSK lookups - 15-20x faster than DataFrame filtering"""
    cache = {}
    if m4_mlcfg_df is None or m4_mlcfg_df.empty:
        return cache
    
    for row in m4_mlcfg_df.itertuples():
        material = getattr(row, 'material', None)
        location = getattr(row, 'location', None)
        if material is None or location is None:
            continue
        
        ptf = 0
        lsk = 1
        
        # Try lowercase first, then uppercase
        ptf_val = getattr(row, 'ptf', None) or getattr(row, 'PTF', None)
        lsk_val = getattr(row, 'lsk', None) or getattr(row, 'LSK', None)
        
        if ptf_val is not None and not pd.isna(ptf_val):
            ptf = int(ptf_val)
        if lsk_val is not None and not pd.isna(lsk_val):
            lsk = int(lsk_val)
        
        cache[(str(material), str(location))] = (ptf, lsk)
    
    return cache

def _get_ptf_lsk(material: str, site: str, m4_mlcfg_df: pd.DataFrame | None, 
                 cache: Dict[tuple[str, str], tuple[int, int]] | None = None) -> tuple[int, int]:
    """Get PTF/LSK with optional caching support"""
    # Use cache if provided (15-20x faster)
    if cache is not None:
        return cache.get((str(material), str(site)), (0, 1))
    
    # Fallback to original logic if no cache
    ptf, lsk = 0, 1
    if m4_mlcfg_df is None or m4_mlcfg_df.empty:
        return ptf, lsk
    ml = m4_mlcfg_df[
        (m4_mlcfg_df['material'] == material) &
        (m4_mlcfg_df['location'] == site)
    ]
    if ml.empty:
        return ptf, lsk
    row = ml.iloc[0]
    if 'ptf' in ml.columns and pd.notna(row.get('ptf')):
        ptf = int(row['ptf'])
    elif 'PTF' in ml.columns and pd.notna(row.get('PTF')):
        ptf = int(row['PTF'])
    if 'lsk' in ml.columns and pd.notna(row.get('lsk')):
        lsk = int(row['lsk'])
    elif 'LSK' in ml.columns and pd.notna(row.get('LSK')):
        lsk = int(row['LSK'])
    return ptf, lsk

def _build_lead_time_cache(lead_time_df: pd.DataFrame) -> Dict[tuple[str, str], tuple[int, int, int]]:
    """Build cache for lead time base values (PDT, GR, MCT) - 10-15x faster"""
    cache = {}
    if lead_time_df.empty:
        return cache
    
    for row in lead_time_df.itertuples():
        sending = getattr(row, 'sending', None)
        receiving = getattr(row, 'receiving', None)
        if sending is None or receiving is None:
            continue
        
        PDT = int(getattr(row, 'PDT', 0) or 0)
        GR = int(getattr(row, 'GR', 0) or 0)
        MCT = int(getattr(row, 'MCT', 0) or 0)
        
        cache[(str(sending), str(receiving))] = (PDT, GR, MCT)
    
    return cache

def determine_lead_time(
    sending: str,
    receiving: str,
    location_type: str,
    lead_time_df: pd.DataFrame,
    m4_mlcfg_df: pd.DataFrame | None = None,
    material: str | None = None,
    lead_time_cache: Dict[tuple[str, str], tuple[int, int, int]] | None = None,
    ptf_lsk_cache: Dict[tuple[str, str], tuple[int, int]] | None = None
) -> tuple[int, str]:
    """
    Plant: lead_time = max(MCT, PDT+GR) + PTF + LSK - 1
    DC:    lead_time = PDT + GR
    PTF/LSK æ¥æº: M4_MaterialLocationLineCfgï¼ˆæŒ‰ material+sending åŒ¹é…ï¼‰
    
    With caching: 10-15x faster when cache hit rate is high
    """
    # Use cache if provided (10-15x faster)
    if lead_time_cache is not None:
        base_values = lead_time_cache.get((str(sending), str(receiving)))
        if base_values is None:
            return 1, 'lead_time_missing'
        PDT, GR, MCT = base_values
    else:
        # Fallback to DataFrame filtering
        if lead_time_df.empty:
            return 1, 'empty_lead_time_config'

        row = lead_time_df[
            (lead_time_df['sending'] == sending) &
            (lead_time_df['receiving'] == receiving)
        ]
        if row.empty:
            return 1, 'lead_time_missing'

        try:
            PDT = int(row.iloc[0].get('PDT', 0) or 0)
            GR  = int(row.iloc[0].get('GR',  0) or 0)
            MCT = int(row.iloc[0].get('MCT', 0) or 0)
        except Exception as e:
            return 1, f'lead_time_calculation_error: {str(e)}'

    try:
        ptf, lsk = 0, 1
        if str(location_type).lower() == 'plant' and material is not None:
            # ä¸ M3 å¯¹é½ï¼šæŒ‰ (material, sending) å– PTF/LSK
            ptf, lsk = _get_ptf_lsk(material=material, site=sending, m4_mlcfg_df=m4_mlcfg_df, cache=ptf_lsk_cache)

        if str(location_type).lower() == 'plant':
            base_lt  = max(MCT, PDT + GR)
            leadtime = base_lt + ptf + lsk - 1
        else:
            leadtime = PDT + GR

        return max(1, int(leadtime)), ""

    except Exception as e:
        return 1, f'lead_time_calculation_error: {str(e)}'

def get_sending_location_type(
    material: str,
    sending: str,
    sim_date: pd.Timestamp,
    network_df: pd.DataFrame,
    location_layer_map: dict
) -> str:
    """
    ä¸ Module3 ä¸€è‡´çš„å£å¾„ï¼š
    1) å…ˆæŸ¥ network ä¸­ (material, location=sending) æ´»åŠ¨è¡Œï¼Œè‹¥å­˜åœ¨åˆ™ç”¨å…¶ location_type
    2) è‹¥ä¸å­˜åœ¨ä¸” sending æ˜¯è‡ªåŠ¨è¯†åˆ«çš„æœ€ä¸Šæ¸¸ï¼ˆlayer=0ï¼‰ï¼Œåˆ™è§†ä¸º Plant
    3) å¦åˆ™é»˜è®¤ DC
    """
    if not sending or pd.isna(sending) or str(sending).strip() == "":
        return 'DC'

    row = get_active_network(network_df, material, sending, sim_date, cache=None)  # This function doesn't have access to cache
    if not row.empty:
        return str(row.iloc[0].get('location_type', 'DC') or 'DC')

    # æœªç»´æŠ¤ä½†è¢«è‡ªåŠ¨è¯†åˆ«ä¸ºæ ¹èŠ‚ç‚¹ â†’ Plant
    if location_layer_map.get(str(sending), None) == 0:
        return 'Plant'

    return 'DC'

# === ç”¨ module3 çš„ç‰ˆæœ¬æ›¿æ¢ ===
def assign_location_layers(network_df: pd.DataFrame) -> pd.DataFrame:
    from collections import defaultdict, deque
    if network_df.empty:
        return pd.DataFrame({'location': [], 'layer': []})

    children = defaultdict(list)
    parents = defaultdict(list)
    # Performance optimization: Use itertuples instead of iterrows
    for row in network_df.itertuples():
        sourcing_val = row.sourcing
        location_val = row.location
        sourcing_valid = sourcing_val is not None and pd.notna(sourcing_val) and str(sourcing_val).strip() != ''
        location_valid = location_val is not None and pd.notna(location_val) and str(location_val).strip() != ''
        if sourcing_valid and location_valid:
            children[sourcing_val].append(location_val)
            parents[location_val].append(sourcing_val)

    all_locations = set(network_df['location'].dropna()).union(set(network_df['sourcing'].dropna()))
    potential_roots = [loc for loc in all_locations if not parents[loc]]

    true_roots = []
    for loc in potential_roots:
        if loc in children:
            true_roots.append(loc)
        else:
            has_incoming = any(loc in parents.get(other_loc, []) for other_loc in all_locations)
            if not has_incoming:
                true_roots.append(loc)
    if not true_roots:
        true_roots = potential_roots

    layer_dict = {}
    from collections import deque
    queue = deque()
    for root in true_roots:
        queue.append((root, 0))
    while queue:
        loc, layer = queue.popleft()
        if loc in layer_dict and layer_dict[loc] <= layer:
            continue
        layer_dict[loc] = layer
        for child in children.get(loc, []):
            queue.append((child, layer + 1))

    unassigned = [loc for loc in all_locations if loc not in layer_dict]
    if unassigned:
        max_layer = max(layer_dict.values()) if layer_dict else 0
        for loc in unassigned:
            layer_dict[loc] = max_layer + 1

    layer_df = pd.DataFrame([{'location': loc, 'layer': layer} for loc, layer in layer_dict.items()])
    layer_df = layer_df.sort_values('layer')
    return layer_df

def _build_active_network_cache(network_df: pd.DataFrame) -> Dict[tuple[str, str, pd.Timestamp, pd.Timestamp], pd.Series]:
    """Build cache for active network lookups - 20-30x faster than DataFrame filtering"""
    cache = {}
    if network_df.empty:
        return cache
    
    for row in network_df.itertuples():
        material = getattr(row, 'material', None)
        location = getattr(row, 'location', None)
        eff_from = getattr(row, 'eff_from', None)
        eff_to = getattr(row, 'eff_to', None)
        
        if material is None or location is None:
            continue
        
        key = (str(material), str(location), eff_from, eff_to)
        # Store the row as a dictionary for easy access
        cache[key] = row
    
    return cache

def get_active_network(network_df, material, location, sim_date, 
                      cache: Dict[tuple[str, str, pd.Timestamp, pd.Timestamp], pd.Series] | None = None):
    """Get active network with optional caching support - 20-30x faster with cache"""
    # Use cache if provided
    if cache is not None:
        # Find matching entries in cache
        matching_rows = []
        for key, row in cache.items():
            if (key[0] == str(material) and 
                key[1] == str(location) and 
                key[2] <= sim_date <= key[3]):
                matching_rows.append(row)
        
        if matching_rows:
            # Convert back to DataFrame format for compatibility
            # Return just the first match as a DataFrame
            return pd.DataFrame([matching_rows[0]._asdict()])
        return pd.DataFrame()
    
    # Fallback to original DataFrame filtering
    rows = network_df[
        (network_df['material'] == material) &
        (network_df['location'] == location) &
        (network_df['eff_from'] <= sim_date) &
        (network_df['eff_to'] >= sim_date)
    ]
    return rows

def is_review_day(dt, lsk, day):
    if lsk == 'daily':
        return True
    if lsk == 'weekly':
        return dt.weekday() == (int(day) - 1)
    if lsk == 'monthly':
        return dt.day == int(day)
    raise ValueError(f"Unknown LSK: {lsk}")

def compute_horizon(dt, lsk, day):
    if lsk == 'daily':
        return dt, dt
    if lsk == 'weekly':
        # Only valid if dt is review day
        if dt.weekday() != (int(day)-1):
            raise ValueError(f"compute_horizon: input date {dt} is not review day (expected weekday {int(day)-1})")
        cur = dt + timedelta(days=1)
        while True:
            if cur.weekday() == (int(day) - 1):
                break
            cur += timedelta(days=1)
        window_end = cur - timedelta(days=1)
        return dt, window_end
    if lsk == 'monthly':
        if dt.day != int(day):
            raise ValueError(f"compute_horizon: input date {dt} is not review day (expected day {int(day)})")
        y, m = dt.year, dt.month
        if dt.day >= int(day):
            m += 1
            if m > 12:
                m = 1
                y += 1
        next_review = pd.Timestamp(y, m, int(day))
        window_end = next_review - timedelta(days=1)
        return dt, window_end
    raise ValueError(f"Unknown LSK: {lsk}")

def load_integrated_config(
    config_dict: dict,
    module1_output_dir: str,
    module4_output_path: str, 
    orchestrator: object,
    current_date: pd.Timestamp
) -> dict:
    """
    åŠ è½½é›†æˆé…ç½®æ•°æ®ï¼Œæ›¿ä»£åŸæ¥çš„load_config
    
    Args:
        config_dict: é…ç½®æ•°æ®å­—å…¸
        module1_output_dir: Module1è¾“å‡ºç›®å½•
        module4_output_path: Module4è¾“å‡ºæ–‡ä»¶è·¯å¾„
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        dict: é›†æˆé…ç½®æ•°æ®
    """
    config = {}
    validation_log = []
    
    # 1. ä»é…ç½®è¡¨åŠ è½½é™æ€æ•°æ®
    config['SafetyStock'] = config_dict.get('M3_SafetyStock', pd.DataFrame())
    config['Network'] = config_dict.get('Global_Network', pd.DataFrame())
    config['LeadTime'] = config_dict.get('Global_LeadTime', pd.DataFrame())
    config['DemandPriority'] = config_dict.get('Global_DemandPriority', pd.DataFrame())
    config['PushPullModel'] = config_dict.get('M5_PushPullModel', pd.DataFrame())
    config['DeployConfig'] = config_dict.get('M5_DeployConfig', pd.DataFrame())
    
    # åº”ç”¨å­—ç¬¦ä¸²æ ¼å¼åŒ–åˆ°æ‰€æœ‰é…ç½®è¡¨
    for sheet_name in ['SafetyStock', 'Network', 'LeadTime', 'DemandPriority', 'PushPullModel', 'DeployConfig']:
        if not config[sheet_name].empty:
            config[sheet_name] = _normalize_identifiers(config[sheet_name])
    
    # 2. ä»Module1åŠ è½½å½“æ—¥æ•°æ®
    config['SupplyDemandLog'] = config_dict.get('M5_SupplyDemandLog', pd.DataFrame())  # ä»æµ‹è¯•é…ç½®åŠ è½½
    
    # ä»Module1åŠ è½½å½“æ—¥"è®¢å•æ± "
    if module1_output_dir and current_date:
        config['OrderLog'] = load_module1_daily_orders(module1_output_dir, current_date)
    else:
        config['OrderLog'] = pd.DataFrame()

    # å®é™…ä» Module1 è¾“å‡ºåŠ è½½å½“æ—¥æ•°æ®
    if module1_output_dir and current_date:
        try:
            # ä» Module1 æ—¥è¾“å‡ºåŠ è½½ SupplyDemandLog
            date_str = current_date.strftime('%Y%m%d')
            module1_file = f"{module1_output_dir}/module1_output_{date_str}.xlsx"
            if os.path.exists(module1_file):
                xl = pd.ExcelFile(module1_file)
                if 'SupplyDemandLog' in xl.sheet_names:
                    m1_supply_demand = xl.parse('SupplyDemandLog')
                    if not m1_supply_demand.empty:
                        config['SupplyDemandLog'] = m1_supply_demand
                        # print(f"  âœ… ä» Module1 åŠ è½½äº† {len(m1_supply_demand)} æ¡ SupplyDemandLog æ•°æ®")
        except Exception as e:
            print(f"  âš ï¸  æ— æ³•ä» Module1 åŠ è½½æ•°æ®: {e}")
    
    # ä»Module1åŠ è½½å½“æ—¥å‘è´§æ•°æ®
    if module1_output_dir and current_date:
        config['TodayShipment'] = load_module1_daily_shipment(module1_output_dir, current_date)
    else:
        config['TodayShipment'] = pd.DataFrame()
    
    # 3. ç”Ÿäº§è®¡åˆ’ï¼šä¿®å¤é‡å¤è®¡ç®—é—®é¢˜ï¼Œåªä½¿ç”¨å®é™…çš„å†å²ç”Ÿäº§GR
    config['ProductionPlan'] = pd.DataFrame()  # å…ˆç½®ç©º
    # === ğŸ”§ ä¿®å¤ï¼šåªä» Orchestrator å–å½“æ—¥å®é™…å†å²ç”Ÿäº§GRï¼Œé¿å…é‡å¤è®¡ç®— ===
    if orchestrator and current_date:
        date_str = current_date.strftime('%Y-%m-%d')
        try:
            # åªè·å–å½“æ—¥å®é™…å†å²ç”Ÿäº§GRï¼Œä¸åŒ…å«è®¡åˆ’ç”Ÿäº§
            prod_gr = orchestrator.get_production_gr_view(date_str)
            if isinstance(prod_gr, pd.DataFrame) and not prod_gr.empty:
                # è§„èŒƒå­—æ®µï¼Œå°†dateé‡å‘½åä¸ºavailable_dateä»¥ä¿æŒå…¼å®¹æ€§
                prod_gr = prod_gr.rename(columns={'date': 'available_date'})[['material', 'location', 'available_date', 'quantity']]
                if 'available_date' in prod_gr.columns:
                    prod_gr['available_date'] = pd.to_datetime(prod_gr['available_date'])
                for col in ['quantity']:
                    if col in prod_gr.columns:
                        prod_gr[col] = pd.to_numeric(prod_gr[col], errors='coerce').fillna(0)
                config['ProductionPlan'] = prod_gr
                # print(f"  âœ… ä» Orchestrator åŠ è½½äº† {len(prod_gr)} æ¡ç”Ÿäº§è®¡åˆ’æ•°æ®ï¼ˆä»…å†å²ç”Ÿäº§GRï¼Œä¿®å¤é‡å¤è®¡ç®—ï¼‰")
            else:
                print(f"  âš ï¸  Orchestratorå½“æ—¥æ— å†å²ç”Ÿäº§GRæ•°æ®")
        except Exception as e:
            print(f"  âš ï¸  ä» Orchestrator åŠ è½½ç”Ÿäº§è®¡åˆ’å¤±è´¥: {e}")

    # === å›é€€ï¼šè‹¥ orchestrator æ— æ•°æ®ï¼Œå†å°è¯•ä» module4 æ–‡ä»¶è¯»å– ProductionPlan ===
    if (config['ProductionPlan'].empty) and module4_output_path and os.path.exists(module4_output_path):
        try:
            xl = pd.ExcelFile(module4_output_path)
            if 'ProductionPlan' in xl.sheet_names:
                m4_production = xl.parse('ProductionPlan')
                if not m4_production.empty:
                    if 'available_date' in m4_production.columns:
                        m4_production['available_date'] = pd.to_datetime(m4_production['available_date'])
                    for col in ['produced_qty', 'uncon_planned_qty', 'planned_qty', 'quantity']:
                        if col in m4_production.columns:
                            m4_production[col] = pd.to_numeric(m4_production[col], errors='coerce').fillna(0)
                    config['ProductionPlan'] = m4_production
                    # print(f"  âœ… å›é€€ï¼šä» Module4 åŠ è½½äº† {len(m4_production)} æ¡ç”Ÿäº§è®¡åˆ’æ•°æ®")
        except Exception as e:
            print(f"  âš ï¸  æ— æ³•ä» Module4 åŠ è½½ ProductionPlan: {e}")   

    # è¯»å– M4_MaterialLocationLineCfgï¼ˆç”¨äº PTF/LSKï¼‰
    config['M4_MaterialLocationLineCfg'] = config_dict.get('M4_MaterialLocationLineCfg', pd.DataFrame())
    if module4_output_path and os.path.exists(module4_output_path):
        try:
            xl = pd.ExcelFile(module4_output_path)
            if 'M4_MaterialLocationLineCfg' in xl.sheet_names:
                mlcfg = xl.parse('M4_MaterialLocationLineCfg')
                if not mlcfg.empty:
                    config['M4_MaterialLocationLineCfg'] = mlcfg
                    print(f"  âœ… ä» Module4 åŠ è½½äº† {len(mlcfg)} æ¡ M4_MaterialLocationLineCfg")
        except Exception as e:
            print(f"  âš ï¸  æ— æ³•ä» Module4 è¯»å– M4_MaterialLocationLineCfg: {e}")

    # 4. ä»OrchestratoråŠ è½½åŠ¨æ€æ•°æ®
    if orchestrator and current_date:
        date_str = current_date.strftime('%Y-%m-%d')
        try:
            # ğŸ”„ ä¿®æ”¹ï¼šä½¿ç”¨æœŸåˆåº“å­˜è€Œä¸æ˜¯å½“å‰åº“å­˜çŠ¶æ€ï¼Œé¿å…é‡å¤è®¡ç®—
            config['InventoryLog'] = orchestrator.get_beginning_inventory_view(date_str)
            config['InTransit'] = orchestrator.get_planning_intransit_view(date_str)
            config['DeliveryGR'] = load_orchestrator_delivery_gr(orchestrator, current_date)
            config['OpenDeployment'] = load_orchestrator_open_deployment(orchestrator, current_date)
            config['ReceivingSpace'] = orchestrator.get_space_quota_view(date_str)
            # print(f"  âœ… ä» Orchestrator åŠ è½½äº†åŠ¨æ€æ•°æ®ï¼ˆä½¿ç”¨æœŸåˆåº“å­˜åŸºç¡€ï¼‰")
        except Exception as e:
            print(f"  âš ï¸  ä» Orchestrator åŠ è½½åŠ¨æ€æ•°æ®å¤±è´¥: {e}")
            # ä½¿ç”¨ç©ºæ•°æ®ä½œä¸ºå¤‡é€‰
            config['InventoryLog'] = pd.DataFrame()
            config['InTransit'] = pd.DataFrame() 
            config['DeliveryGR'] = pd.DataFrame()
            config['OpenDeployment'] = pd.DataFrame()
            config['ReceivingSpace'] = pd.DataFrame()
    else:
        # ä½¿ç”¨ç©ºæ•°æ®
        config['InventoryLog'] = pd.DataFrame()
        config['InTransit'] = pd.DataFrame()
        config['DeliveryGR'] = pd.DataFrame()
        config['OpenDeployment'] = pd.DataFrame()
        config['ReceivingSpace'] = pd.DataFrame()
    
    # ä¸´æ—¶ä½¿ç”¨ç©ºæ•°æ®ä»¥ä¿æŒå…¼å®¹æ€§
    for key in ['SupplyDemandLog', 'ProductionPlan', 'InventoryLog', 'InTransit', 'ReceivingSpace']:
        if key not in config:
            config[key] = pd.DataFrame()
    
    # æ—¥æœŸå­—æ®µå¤„ç†
    date_fields = {
        'SupplyDemandLog': ['date'],
        'ProductionPlan': ['available_date'],
        'InventoryLog': ['date'],
        'InTransit': ['available_date'],
        'SafetyStock': ['date'],
        'ReceivingSpace': ['date'],
        'Network': ['eff_from', 'eff_to'],
        'OrderLog': ['date', 'simulation_date'],
    }
    
    for sheet, fields in date_fields.items():
        if sheet in config and not config[sheet].empty:
            for f in fields:
                if f in config[sheet].columns:
                    config[sheet][f] = pd.to_datetime(config[sheet][f])
    
    # æœ€ç»ˆæ ¼å¼åŒ–æ‰€æœ‰é…ç½®è¡¨çš„æ ‡è¯†ç¬¦å­—æ®µ
    for sheet_name, df in config.items():
        if isinstance(df, pd.DataFrame) and not df.empty:
            config[sheet_name] = _normalize_identifiers(df)
    
    config['ValidationLog'] = validation_log
    return config

def load_config(input_path: str):
    required_sheets = [
        'SupplyDemandLog', 'ProductionPlan', 'InventoryLog', 'InTransit', 'SafetyStock',
        'Network', 'PushPullModel', 'ReceivingSpace', 'LeadTime',
        'DemandPriority', 'DeployConfig'
    ]
    config = {}
    validation_log = []
    xl = pd.ExcelFile(input_path)
    for sheet in required_sheets:
        if sheet not in xl.sheet_names:
            validation_log.append({'No': len(validation_log)+1, 'Issue': f'Missing required sheet: {sheet}'})
            config[sheet] = pd.DataFrame()
        else:
            df = xl.parse(sheet)
            config[sheet] = df
    # å­—æ®µæ ¡éªŒä¸¾ä¾‹
    sdl_required = ['date', 'material', 'location', 'demand_element', 'quantity']
    if not config['SupplyDemandLog'].empty:
        missing_cols = [c for c in sdl_required if c not in config['SupplyDemandLog'].columns]
        if missing_cols:
            validation_log.append({'No': len(validation_log)+1, 'Issue': f'SupplyDemandLog missing columns: {",".join(missing_cols)}'})
    # æ—¥æœŸç±»å‹å¤„ç†
    date_fields = {
        'SupplyDemandLog': ['date'],
        'ProductionPlan': ['available_date'],
        'InventoryLog': ['date'],
        'InTransit': ['available_date'],
        'SafetyStock': ['date'],
        'ReceivingSpace': ['date'],
        'Network': ['eff_from', 'eff_to'],
        'OrderLog': ['date', 'simulation_date'],
    }
    for sheet, fields in date_fields.items():
        if sheet in config and not config[sheet].empty:
            for f in fields:
                if f in config[sheet].columns:
                    config[sheet][f] = pd.to_datetime(config[sheet][f])
    
    # æœ€ç»ˆæ ¼å¼åŒ–æ‰€æœ‰é…ç½®è¡¨çš„æ ‡è¯†ç¬¦å­—æ®µ
    for sheet_name, df in config.items():
        if isinstance(df, pd.DataFrame) and not df.empty:
            config[sheet_name] = _normalize_identifiers(df)
    
    config['ValidationLog'] = validation_log
    return config

def validate_config_before_run(config, validation_log):
    deploy_cfg = config['DeployConfig']
    leadtime_df = config['LeadTime']
    pushpull = config['PushPullModel']
    demand_priority = config['DemandPriority']
    network = config['Network']
    # ======= æ ¡éªŒnetworkæ˜¯å¦æœ‰multiple sourcing ==========
    multi_sourcing = (
        network.groupby(['material', 'location'])['sourcing']
        .nunique().reset_index()
    )
    multi_sourcing = multi_sourcing[multi_sourcing['sourcing'] > 1]
    for _, row in multi_sourcing.iterrows():
        validation_log.append({
            'No': len(validation_log) + 1,
            'Issue': f"Networké…ç½®ä¸åˆæ³•: material={row['material']}, location={row['location']} æœ‰å¤šä¸ªsourcing"
        })
    # æ ¡éªŒleadtime
    for _, row in network.iterrows():
        if leadtime_df[
            (leadtime_df['sending'] == row['sourcing']) & (leadtime_df['receiving'] == row['location'])
        ].empty:
            validation_log.append({'No': len(validation_log)+1,
                                  'Issue': f"Missing leadtime for {row['sourcing']}->{row['location']} ({row['material']})"})
    # æ ¡éªŒpushpull
    for _, row in deploy_cfg.iterrows():
        if pushpull[
            (pushpull['material'] == row['material']) & (pushpull['sending'] == row['sending'])
        ].empty:
            validation_log.append({'No': len(validation_log)+1,
                'Issue': f"Missing PushPullModel for {row['material']}/{row['sending']}"})
    # ======= æ ¡éªŒ/è¡¥å…… DemandPriority ==========
    dp = demand_priority.copy()

    # æ—¢çœ‹ SupplyDemandLog çš„ demand_elementï¼Œä¹Ÿçœ‹ OrderLog çš„ demand_typeï¼ˆAO/normalï¼‰
    sdl_types = set(config['SupplyDemandLog']['demand_element'].unique()) if not config['SupplyDemandLog'].empty else set()
    ol = config.get('OrderLog', pd.DataFrame())
    ol_types = set(ol['demand_type'].unique()) if ('demand_type' in ol.columns and not ol.empty) else set()

    # æŠŠ AO/normal æ˜ å°„ä¸º demand_element å­—æ®µé‡Œçš„å€¼ï¼ˆæˆ‘ä»¬åç»­ç”¨ demand_element åšä¼˜å…ˆçº§ï¼‰
    needed = sdl_types | ol_types  # AO/normal ä¹Ÿåœ¨å…¶ä¸­

    # ç¼ºå•¥è¡¥å•¥ï¼ˆé»˜è®¤ï¼šAO=1ï¼Œnormal=2ï¼Œå…¶ä½™ç»™ä¸ªè¾ƒä½ä¼˜å…ˆçº§ 9ï¼‰
    def _ensure_priority(elem, default_p):
        if dp[dp['demand_element'] == elem].empty:
            dp.loc[len(dp)] = {'demand_element': elem, 'priority': default_p}
            validation_log.append({
                'No': len(validation_log)+1,
                'Issue': f'Auto add DemandPriority for {elem}={default_p}'
            })
    for elem in needed:
        if elem == 'AO':
            _ensure_priority('AO', 1)
        elif elem == 'normal':
            _ensure_priority('normal', 2)
        else:
            _ensure_priority(elem, 9)
    # å›å†™
    config['DemandPriority'] = dp

    return validation_log

def collect_node_demands(material, location, sim_date, config, up_gap_buffer,
                         ptf_lsk_cache=None, lead_time_cache=None, active_network_cache=None):
    """
    å¯¹é½è§„åˆ™ï¼š
    - horizon ç»Ÿä¸€æ¥è‡ª determine_lead_time å£å¾„ï¼š
        * æœ‰ä¸Šæ¸¸ï¼šhorizon = determine_lead_time(upstream->location)
        * æ— ä¸Šæ¸¸ï¼ˆé¡¶å±‚è‡ªè¡¥ï¼‰ï¼šhorizon = max(MCT, PDT+GR) + PTF + LSK - 1
    - horizon_end = sim_date + horizon
    - é€‰æ‹©çª—å£ï¼š
        * AO/normalï¼ˆè®¢å•ï¼Œæ¥è‡ª OrderLogï¼‰ï¼šdate âˆˆ (sim_date, horizon_end]
        * forecastï¼ˆæ¥è‡ª SupplyDemandLogï¼‰ï¼šdate âˆˆ [sim_date, horizon_end]
        * safetyï¼šå– horizon_end å½“å¤©çš„ç›®æ ‡é‡
        * å…¶ä½™/å‡€éœ€æ±‚ä¼ é€’ï¼ˆnet demand for xx ç­‰ï¼‰ï¼šdate âˆˆ [sim_date, horizon_end]
    - è¡Œçº§ leadtimeï¼š
        * æœ‰ä¸Šæ¸¸ï¼š= horizonï¼ˆåŒä¸€å¥—å£å¾„ï¼‰
        * æ— ä¸Šæ¸¸ï¼š= 0
    """
    import pandas as pd
    from datetime import timedelta

    supply_demand_log = config['SupplyDemandLog']
    safety_stock      = config['SafetyStock']
    deploy_cfg        = config['DeployConfig']
    network           = config['Network']
    leadtime_df       = config['LeadTime']

    # è¯»å– MOQ/RV/LSK/Dayï¼ˆä¿æŒä½ ç°æœ‰å£å¾„ï¼šä»¥æœ¬èŠ‚ç‚¹ä½œä¸º sending å»è¯»ï¼‰
    param_row = deploy_cfg[
        (deploy_cfg['material'] == material) & (deploy_cfg['sending'] == location)
    ]
    if not param_row.empty:
        moq = int(param_row.iloc[0]['moq'])
        rv  = int(param_row.iloc[0]['rv'])
        lsk = param_row.iloc[0]['lsk']  # æ­¤å¤„ lsk/day ä»…ä½œä¸ºåç»­æ¨¡å—å¯èƒ½ç”¨åˆ°çš„å…ƒæ•°æ®ï¼Œçª—å£ä¸å†ä¾èµ–å®ƒ
        day = int(param_row.iloc[0]['day'])
    else:
        moq, rv, lsk, day = 1, 1, 1, 1

    # ä¸Šæ¸¸
    network_row = get_active_network(network, material, location, sim_date, cache=active_network_cache)
    upstream = network_row.iloc[0]['sourcing'] if not network_row.empty else None

    # ç»Ÿä¸€ï¼šå‘é€ç«¯ç±»å‹ & horizon
    if upstream and str(upstream).strip():
        sending_location_type = get_sending_location_type(
            material=str(material),
            sending=str(upstream),
            sim_date=sim_date,
            network_df=network,
            location_layer_map=config.get('LocationLayerMap', {})
        )
        # æœ‰ä¸Šæ¸¸ï¼šç”¨ determine_lead_time å¾—åˆ° horizon
        horizon, err = determine_lead_time(
            sending=str(upstream),
            receiving=str(location),
            location_type=str(sending_location_type),
            lead_time_df=leadtime_df,
            m4_mlcfg_df=config.get('M4_MaterialLocationLineCfg', pd.DataFrame()),
            material=str(material),
            lead_time_cache=lead_time_cache,
            ptf_lsk_cache=ptf_lsk_cache
        )
        if err:
            # ç¼ºå¤±æˆ–å¼‚å¸¸å›é€€ä¸º 1
            horizon = 1
        leadtime_for_row = int(horizon)  # è¡Œçº§ LTï¼ˆè·¨èŠ‚ç‚¹ï¼‰
    else:
        # é¡¶å±‚ï¼šæŒ‰ Plant å…¬å¼è®¡ç®— horizon = max(MCT, PDT+GR) + PTF + LSK - 1
        # PTF/LSK æ¥è‡ª M4_MaterialLocationLineCfg
        ptf, lsk_val = _get_ptf_lsk(
            material=str(material),
            site=str(location),
            m4_mlcfg_df=config.get('M4_MaterialLocationLineCfg', pd.DataFrame()),
            cache=ptf_lsk_cache
        )
        # MCT/PDT/GR æ¥è‡ª Global_LeadTimeï¼ˆä»¥ sending==location çš„è¡Œå–æœ€å¤§å€¼ï¼›ç¼ºå¤±æŒ‰ 0ï¼‰
        df_loc = leadtime_df[leadtime_df['sending'] == str(location)]
        MCT = int(pd.to_numeric(df_loc.get('MCT', 0), errors='coerce').fillna(0).max()) if not df_loc.empty else 0
        PDT = int(pd.to_numeric(df_loc.get('PDT', 0), errors='coerce').fillna(0).max()) if not df_loc.empty else 0
        GR  = int(pd.to_numeric(df_loc.get('GR',  0), errors='coerce').fillna(0).max()) if not df_loc.empty else 0

        base_lt  = max(MCT, PDT + GR)
        horizon  = max(1, int(base_lt + int(ptf) + int(lsk_val) - 1))
        leadtime_for_row = 0  # è‡ªè¡¥è´§ï¼ˆé¡¶å±‚ï¼‰è¡Œçº§ LT æ’ä¸º 0

    # horizon_end
    horizon_end = sim_date + timedelta(days=int(horizon))

    demand_rows = []

    # ========= 1) SDL: é¢„æµ‹ / å…¶ä»–æœ¬åœ°éœ€æ±‚ =========
    sdl = supply_demand_log[
        (supply_demand_log['material'] == material) &
        (supply_demand_log['location'] == location)
    ].copy()
    if not sdl.empty:
        sdl['requirement_date'] = pd.to_datetime(sdl['date'])

        # è¯†åˆ« forecast è¡Œï¼ˆå®Œå…¨åŒ¹é… 'forecast'ï¼Œå¿½ç•¥å¤§å°å†™ï¼‰
        is_fc = sdl['demand_element'].astype(str).str.lower() == 'forecast'

        # forecast: [sim_date, horizon_end]
        sdl_fc = sdl[is_fc & (sdl['requirement_date'] >= sim_date) & (sdl['requirement_date'] <= horizon_end)]

        # å…¶ä»–ï¼ˆå« net demand for xx ç­‰ï¼‰ï¼š[sim_date, horizon_end]
        sdl_others = sdl[~is_fc & (sdl['requirement_date'] >= sim_date) & (sdl['requirement_date'] <= horizon_end)]

        for _, row in pd.concat([sdl_fc, sdl_others], ignore_index=True).iterrows():
            demand_rows.append({
                'material': material,
                'location': location,
                'sending': upstream,
                'receiving': location,
                'demand_element': row['demand_element'],
                'demand_qty': int(row['quantity']),
                'planned_qty': int(row['quantity']),
                'moq': moq,
                'rv': rv,
                'leadtime': leadtime_for_row if upstream else 0,  # é¡¶å±‚è‡ªè¡¥ 0ï¼Œè·¨èŠ‚ç‚¹=ç»Ÿä¸€ horizon
                'requirement_date': row['requirement_date'],
                'plan_deploy_date': sim_date  # è®¡åˆ’è§¦å‘æ—¥åœ¨çª—å£å†…åˆ†é…ç¯èŠ‚ä½¿ç”¨ï¼Œè¿™é‡Œå…ˆæ”¾ sim_date
            })

    # ========= 2) å®‰å…¨åº“å­˜ï¼šåªå– horizon_end å½“å¤© =========
    ss = safety_stock[
        (safety_stock['material'] == material) &
        (safety_stock['location'] == location)
    ].copy()
    ss_qty = 0
    if not ss.empty:
        ss['date'] = pd.to_datetime(ss['date'])
        ss_end = ss[ss['date'] == horizon_end]
        if not ss_end.empty:
            ss_qty = int(pd.to_numeric(ss_end['safety_stock_qty'], errors='coerce').fillna(0).sum())

    if ss_qty > 0:
        demand_rows.append({
            'material': material,
            'location': location,
            'sending': upstream,
            'receiving': location,
            'demand_element': 'safety',
            'demand_qty': ss_qty,
            'planned_qty': ss_qty,
            'moq': moq,
            'rv': rv,
            'leadtime': leadtime_for_row if upstream else 0,
            'requirement_date': horizon_end,     # ç›®æ ‡æ—¥ = horizon_end
            'plan_deploy_date': sim_date
        })

    # ========= 3) è®¢å•æ± ï¼ˆAO/normalï¼‰ï¼š[sim_date, horizon_end] =========
    order_df = config.get('OrderLog', pd.DataFrame())
    if not order_df.empty:
        orders = order_df[
            (order_df['material'] == material) & (order_df['location'] == location)
        ].copy()
        if not orders.empty:
            orders['requirement_date'] = pd.to_datetime(orders['date'])
            orders['demand_element']  = orders['demand_type']

            # AO / normal ç»Ÿä¸€ç”¨ (sim_date, horizon_end]
            mask = (orders['requirement_date'] >= sim_date) & (orders['requirement_date'] <= horizon_end)
            orders = orders[mask]

            for _, row in orders.iterrows():
                qty = int(row['quantity'])
                demand_rows.append({
                    'material': material,
                    'location': location,
                    'sending': upstream,
                    'receiving': location,
                    'demand_element': str(row['demand_element']),
                    'demand_qty': qty,
                    'planned_qty': qty,
                    'moq': moq,
                    'rv': rv,
                    'leadtime': leadtime_for_row if upstream else 0,
                    'requirement_date': row['requirement_date'],
                    'plan_deploy_date': sim_date,
                    'orig_location': location
                })

    # ========= 4) GAP ä¼ é€’ï¼ˆä¸Šæ¸¸ä¸‹å‘çš„å‡€éœ€æ±‚ï¼‰ï¼š[sim_date, horizon_end] =========
    if up_gap_buffer is not None and (material, location) in up_gap_buffer:
        for gap in up_gap_buffer[(material, location)]:
            req_dt = pd.to_datetime(gap.get('requirement_date', sim_date))
            if (req_dt >= sim_date) and (req_dt <= horizon_end):
                demand_rows.append({
                    'material': material,
                    'location': gap.get('location', location),
                    'receiving': gap.get('receiving', gap.get('location', location)),
                    'orig_location': gap.get('orig_location', gap.get('location', location)),
                    'sending': upstream,
                    'demand_element': gap['demand_element'],
                    'demand_qty': int(gap['planned_qty']),
                    'planned_qty': int(gap['planned_qty']),
                    'moq': moq,
                    'rv': rv,
                    'leadtime': leadtime_for_row if upstream else 0,
                    'requirement_date': req_dt,
                    'plan_deploy_date': sim_date,
                    'from_location': gap.get('from_location', None),
                })

    return demand_rows

def push_softpush_allocation(
    deployment_plan_rows, config, dynamic_soh, sim_date,
    ptf_lsk_cache=None, lead_time_cache=None
):
    """
    å¯¹ push / soft-push èŠ‚ç‚¹ï¼ŒæŠŠçœŸæ­£çš„å‰©ä½™åº“å­˜æŒ‰ä¸‹æ¸¸ safety æƒé‡åˆ†é…ä¸ºè¡¥è´§è®¡åˆ’è¡Œã€‚
    ä¿®å¤ç‚¹ï¼š
      1) æœ¬èŠ‚ç‚¹å½“æ—¥è‹¥å­˜åœ¨æœªæ»¡è¶³çš„é push éœ€æ±‚ï¼ˆå«è‡ªæ»¡è¶³ & è·¨èŠ‚ç‚¹ï¼‰ï¼Œåˆ™ä¸è§¦å‘ push
      2) å·²åˆ†é…åº“å­˜ï¼šå½“æ—¥ã€é push è¡Œï¼ˆå«è‡ªæ»¡è¶³ï¼‰éƒ½ä¼šæ‰£å‡ï¼Œé¿å…è¯¯åˆ¤â€œå‰©ä½™â€
      3) receiving ç«¯å®‰å…¨åº“å­˜æŒ‰ (sim_date + leadtime) å–å€¼
      4) è‹¥ä¸‹æ¸¸ safety æ€»å’Œä¸º 0 æˆ–æ— ä¸‹æ¸¸ï¼Œåˆ™ä¸åˆ†é…
      5) æœ€å¤§ä½™æ•°æ³•åˆ†é…ï¼Œæ¶ˆç­å‘ä¸‹å–æ•´å°¾å·®
    """
    import pandas as pd
    import numpy as np
    from datetime import timedelta

    pushpull   = config['PushPullModel']
    safety     = config['SafetyStock']
    lt_df      = config['LeadTime']
    deploy_cfg = config['DeployConfig']
    net        = config['Network']

    plan_rows_push = []

    # â€”â€” ç»Ÿè®¡ã€å½“æ—¥ã€‘å·²åˆ†é…çš„åº“å­˜ï¼ˆé push è¡Œï¼Œå«è‡ªæ»¡è¶³ + è·¨èŠ‚ç‚¹ï¼‰â€”â€”
    allocated_inventory: dict[tuple[str, str], int] = {}
    for r in deployment_plan_rows:
        if 'push' in str(r.get('demand_element', '')).lower():
            continue  # æ’é™¤ push/soft-push è‡ªèº«
        if pd.to_datetime(r.get('date')) != sim_date:
            continue  # ä»…å½“æ—¥
        mat = r.get('material'); snd = r.get('sending')
        if mat is None or snd is None:
            continue
        key = (mat, snd)
        qty = int(r.get('deployed_qty_invCon', 0) or 0)
        if qty > 0:
            allocated_inventory[key] = allocated_inventory.get(key, 0) + qty

    # â€”â€” é€ (material, sending) å¤„ç† â€”â€” 
    group_keys = {(r['material'], r['sending']) for r in deployment_plan_rows if r.get('material') and r.get('sending')}
    for mat, sending in group_keys:
        # A) å½“æ—¥æ˜¯å¦ä»æœ‰æœªæ»¡è¶³çš„é push éœ€æ±‚ï¼Ÿï¼ˆå«è‡ªæ»¡è¶³ & è·¨èŠ‚ç‚¹ï¼‰
        pending_gap = any(
            (
                r.get('material') == mat
                and r.get('sending') == sending
                and 'push' not in str(r.get('demand_element', '')).lower()
                and pd.to_datetime(r.get('date')) == sim_date
                and int(r.get('deployed_qty_invCon', 0) or 0) < int(r.get('planned_qty', 0) or 0)
            )
            for r in deployment_plan_rows
        )
        if pending_gap:
            continue  # æœ‰ç¼ºå£å°±ä¸æ¨

        # B) è¯¥ (mat,sending) æ˜¯å¦é…ç½® push / soft push
        row_pp = pushpull[(pushpull['material'] == mat) & (pushpull['sending'] == sending)]
        if row_pp.empty:
            continue
        model = str(row_pp.iloc[0]['model']).strip().lower()
        if model not in ['push', 'soft push']:
            continue

        # C) è®¡ç®—çœŸæ­£çš„å‰©ä½™åº“å­˜ = dynamic_soh - å½“æ—¥å·²åˆ†é…ï¼ˆé pushï¼‰
        total_soh        = int(dynamic_soh.get((mat, sending), 0) or 0)
        already_allocated = int(allocated_inventory.get((mat, sending), 0) or 0)
        soh = max(0, total_soh - already_allocated)
        if soh <= 0:
            continue

        # D) è¯»å– LSK/Dayï¼ˆè™½å½“å‰é€»è¾‘æœªç”¨åˆ° dayï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
        row_cfg = deploy_cfg[(deploy_cfg['material'] == mat) & (deploy_cfg['sending'] == sending)]
        if not row_cfg.empty:
            lsk = int(row_cfg.iloc[0]['lsk'])
            day = int(row_cfg.iloc[0]['day'])
        else:
            lsk, day = 1, 1

        # E) soft-push éœ€å…ˆä¿ç•™æœ¬èŠ‚ç‚¹å½“æ—¥ safetyï¼ˆsim_dateï¼‰
        sending_ss = 0
        if model == 'soft push':
            ss_self = safety[(safety['material'] == mat) & (safety['location'] == sending)]
            ss_self = ss_self[pd.to_datetime(ss_self['date']) == sim_date] if not ss_self.empty else pd.DataFrame()
            if not ss_self.empty:
                sending_ss = int(ss_self['safety_stock_qty'].sum())
        # å¯ç”¨äºä¸‹æ¨çš„åº“å­˜
        available_soh = soh if model == 'push' else max(0, soh - sending_ss)
        if available_soh <= 0:
            continue

        # F) æ‰¾ä¸‹æ¸¸ receiving åˆ—è¡¨
        recs = net[(net['material'] == mat) & (net['sourcing'] == sending)]['location'].dropna().unique().tolist()
        if not recs:
            continue  # æ²¡æœ‰ä¸‹æ¸¸

        # G) æ„é€  receiving å®‰å…¨åº“å­˜ä¸ leadtime
        receiving_ss_data = []
        for rec in recs:
            # leadtime è®¡ç®—
            sending_location_type = get_sending_location_type(
                material=str(mat),
                sending=str(sending),
                sim_date=sim_date,
                network_df=net,
                location_layer_map=config.get('LocationLayerMap', {})
            )
            leadtime, err = determine_lead_time(
                sending=str(sending),
                receiving=str(rec),
                location_type=str(sending_location_type),
                lead_time_df=lt_df,
                m4_mlcfg_df=config.get('M4_MaterialLocationLineCfg', pd.DataFrame()),
                material=str(mat),
                lead_time_cache=lead_time_cache,
                ptf_lsk_cache=ptf_lsk_cache
            )
            if err:
                leadtime = 1
            target_date = sim_date + timedelta(days=int(leadtime))
            ss_rec = safety[(safety['material'] == mat) & (safety['location'] == rec)]
            ss_rec = ss_rec[pd.to_datetime(ss_rec['date']) == target_date] if not ss_rec.empty else pd.DataFrame()
            ss_qty = int(ss_rec['safety_stock_qty'].sum()) if not ss_rec.empty else 0

            receiving_ss_data.append({
                'receiving': rec,
                'ss_qty': ss_qty,
                'leadtime': int(leadtime),
                'planned_delivery_date': target_date
            })

        total_ss = sum(x['ss_qty'] for x in receiving_ss_data)
        if total_ss <= 0:
            continue  # ä¸‹æ¸¸å®‰å…¨åº“å­˜æ€»å’Œä¸º 0ï¼Œä¸æ¨

        # H) æŒ‰ safety æƒé‡åˆ†é…ï¼ˆæœ€å¤§ä½™æ•°æ³•ï¼Œåƒæ‰å°¾å·®ï¼‰
        # 1) ç†æƒ³é…é¢ä¸å‘ä¸‹å–æ•´
        ideal = []
        floor_sum = 0
        for x in receiving_ss_data:
            share = available_soh * (x['ss_qty'] / total_ss)
            q_floor = int(np.floor(share))
            frac = share - q_floor
            ideal.append((x, q_floor, frac))
            floor_sum += q_floor

        # 2) æŠŠå‰©ä½™ (available_soh - floor_sum) ä»½é¢æŒ‰ frac ä»å¤§åˆ°å° +1
        remainder = int(available_soh - floor_sum)
        if remainder > 0:
            ideal_sorted = sorted(ideal, key=lambda t: t[2], reverse=True)
            for i in range(remainder):
                x, q_floor, frac = ideal_sorted[i % len(ideal_sorted)]
                ideal_sorted[i % len(ideal_sorted)] = (x, q_floor + 1, frac)
            ideal = ideal_sorted

        # I) ç”Ÿæˆ push è®¡åˆ’è¡Œï¼ˆåªè½ qty>0ï¼‰
        for x, qty, _ in ideal:
            if qty <= 0:
                continue
            plan = {
                'date': sim_date,
                'material': mat,
                'sending': sending,
                'receiving': x['receiving'],
                'demand_qty': 0,
                'demand_element': 'push replenishment' if model == 'push' else 'soft push replenishment',
                'planned_qty': int(qty),
                'deployed_qty_invCon_push': int(qty),
                'deployed_qty_invCon': int(qty),  # å…¼å®¹åç»­ç©ºé—´é…é¢ä¸åº“å­˜ç»Ÿè®¡
                'planned_delivery_date': x['planned_delivery_date'],
                'orig_location': x['receiving'],
                'leadtime': int(x['leadtime']),
                'is_cross_node': True
            }
            plan_rows_push.append(plan)

    return plan_rows_push


def apply_receiving_space_quota(deployment_plan_rows, receiving_space, sim_date, demand_priority_map):
    """
    åœ¨æ‰€æœ‰è°ƒè¿è®¡åˆ’æ˜ç»†ç”Ÿæˆåï¼ŒæŒ‰receiving space quotaå†åˆ†é…ï¼Œæ›´æ–°deployed_qtyï¼Œunfulfilled log
    ä¿®å¤ï¼šä»…å¯¹è·¨èŠ‚ç‚¹è°ƒè¿ï¼ˆsending != receivingï¼‰åº”ç”¨receiving space quotaé™åˆ¶
    """
    df = pd.DataFrame(deployment_plan_rows)
    if df.empty:
        df['deployed_qty'] = []
        df['quota'] = []
        return df, []
    
    # å¦‚æœreceiving_spaceé…ç½®ä¸ºç©ºï¼Œç›´æ¥è¿”å›åŸåˆ†é…ç»“æœ
    if receiving_space.empty:
        df['deployed_qty'] = df['deployed_qty_invCon']
        df['quota'] = np.inf
        return df, []
    
    # æŒ‰receiving+dateåˆ†ç»„
    unfulfilled = []
    for (recv, date), grp in df.groupby(['receiving', 'date']):
        quota_row = receiving_space[
            (receiving_space['receiving'] == recv) & (pd.to_datetime(receiving_space['date']) == date)
        ]
        quota = quota_row['max_qty'].iloc[0] if not quota_row.empty else np.inf
        
        # ğŸ”§ ä¿®å¤ï¼šä»…è®¡ç®—è·¨èŠ‚ç‚¹è°ƒè¿çš„quantityå ç”¨quota
        cross_node_grp = grp[grp['sending'] != grp['receiving']]
        self_fulfillment_grp = grp[grp['sending'] == grp['receiving']]
        
        # è‡ªæˆ‘éœ€æ±‚æ»¡è¶³ä¸å ç”¨quotaï¼Œç›´æ¥é€šè¿‡
        df.loc[self_fulfillment_grp.index, 'deployed_qty'] = self_fulfillment_grp['deployed_qty_invCon']
        df.loc[self_fulfillment_grp.index, 'quota'] = np.inf  # è‡ªæˆ‘æ»¡è¶³ä¸å—quotaé™åˆ¶
        
        if cross_node_grp.empty:
            # å¦‚æœæ²¡æœ‰è·¨èŠ‚ç‚¹è°ƒè¿ï¼Œè·³è¿‡quotaæ£€æŸ¥
            continue
            
        # ä»…æ£€æŸ¥è·¨èŠ‚ç‚¹è°ƒè¿æ˜¯å¦è¶…è¿‡quota
        cross_node_total = cross_node_grp['deployed_qty_invCon'].sum()
        if cross_node_total <= quota:
            df.loc[cross_node_grp.index, 'deployed_qty'] = cross_node_grp['deployed_qty_invCon']
            df.loc[cross_node_grp.index, 'quota'] = quota
            continue
        # è·¨èŠ‚ç‚¹è°ƒè¿ç©ºé—´ä¸è¶³ï¼ŒæŒ‰ä¼˜å…ˆçº§+æƒé‡åˆ†é…ï¼ˆä»…å¤„ç†è·¨èŠ‚ç‚¹è°ƒè¿ï¼‰
        cross_node_rows = cross_node_grp.to_dict(orient='records')
        
        # æŒ‰ä¼˜å…ˆçº§å¯¹è·¨èŠ‚ç‚¹è°ƒè¿è¿›è¡Œæ’åºå’Œåˆ†ç»„
        rows_sorted = sorted(cross_node_rows, key=lambda r: demand_priority_map.get(r['demand_element'], 99))
        grouped = {}
        for r in rows_sorted:
            p = demand_priority_map.get(r['demand_element'], 99)
            grouped.setdefault(p, []).append(r)
        
        left = quota
        deploy_qtys = {i: 0 for i in range(len(cross_node_rows))}
        
        for priority in sorted(grouped):
            group = grouped[priority]
            group_total = sum(r['deployed_qty_invCon'] for r in group)
            if left >= group_total:
                for r in group:
                    idx = cross_node_rows.index(r)
                    deploy_qtys[idx] = r['deployed_qty_invCon']
                left -= group_total
            else:
                allocated = 0
                for r in group:
                    idx = cross_node_rows.index(r)
                    weight = r['deployed_qty_invCon'] / group_total if group_total > 0 else 0
                    q = int(left * weight)
                    deploy_qtys[idx] = min(q, r['deployed_qty_invCon'])
                    allocated += deploy_qtys[idx]
                left -= allocated
                # ä¸å†åˆ†é…
                break
        
        # æ›´æ–°è·¨èŠ‚ç‚¹è°ƒè¿çš„å®é™…åˆ†é…
        for idx, qty in deploy_qtys.items():
            original_row = cross_node_rows[idx]
            # æ‰¾åˆ°åŸå§‹DataFrameä¸­å¯¹åº”çš„ç´¢å¼•
            original_idx = cross_node_grp[
                (cross_node_grp['sending'] == original_row['sending']) &
                (cross_node_grp['receiving'] == original_row['receiving']) &
                (cross_node_grp['material'] == original_row['material']) &
                (cross_node_grp['demand_element'] == original_row['demand_element'])
            ].index[0]
            
            df.at[original_idx, 'deployed_qty'] = qty
            df.at[original_idx, 'quota'] = quota
            
            gap = original_row['deployed_qty_invCon'] - qty
            if gap > 0:
                unfulfilled.append({
                    'date': date,
                    'sending': original_row['sending'],
                    'receiving': original_row['receiving'],
                    'material': original_row['material'],  # æ–°å¢
                    'demand_qty': original_row['demand_qty'],
                    'demand_element': original_row['demand_element'],
                    'unfulfilled_qty': gap,
                    'reason': "space constraint"
                })
    # ç©ºé—´å……è¶³è¡Œ
    df['deployed_qty'] = df['deployed_qty'].fillna(df['deployed_qty_invCon'])
    df['quota'] = df['quota'].fillna(np.nan)
    return df, unfulfilled

def log_outputs(output_path: str, outputs: Dict[str, pd.DataFrame]):
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        for sheet, df in outputs.items():
            if df.empty:
                # è¾“å‡ºç©ºè¡¨å¤´
                pd.DataFrame(columns=df.columns).to_excel(writer, sheet_name=sheet, index=False)
            else:
                # ç¡®ä¿è¾“å‡ºæ—¶æ ‡è¯†ç¬¦å­—æ®µä¸ºå­—ç¬¦ä¸²æ ¼å¼
                normalized_df = _normalize_identifiers(df)
                normalized_df.to_excel(writer, sheet_name=sheet, index=False)

# ============ 2. ä¸»æµç¨‹ ===============

def main(
    input_path: str = None, 
    output_path: str = None, 
    sim_start: str = None, 
    sim_end: str = None,
    # æ–°å¢å‚æ•°æ”¯æŒé›†æˆæ¨¡å¼
    config_dict: dict = None,
    module1_output_dir: str = None,
    module4_output_path: str = None,
    orchestrator: object = None,
    current_date: str = None
):
    """
    Module 5: å¤šå±‚çº§éƒ¨ç½²è§„åˆ’æ¨¡å—
    
    æ”¯æŒä¸¤ç§è¿è¡Œæ¨¡å¼:
    1. ç‹¬ç«‹æ¨¡å¼: ä¼ å…¥input_path, output_path, sim_start, sim_end
    2. é›†æˆæ¨¡å¼: ä¼ å…¥config_dict, module1_output_dir, module4_output_path, orchestrator, current_date
    
    Args:
        # ç‹¬ç«‹æ¨¡å¼å‚æ•°
        input_path: è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ 
        sim_start: ä»¿çœŸå¼€å§‹æ—¥æœŸ
        sim_end: ä»¿çœŸç»“æŸæ—¥æœŸ
        
        # é›†æˆæ¨¡å¼å‚æ•°
        config_dict: é…ç½®æ•°æ®å­—å…¸
        module1_output_dir: Module1è¾“å‡ºç›®å½•
        module4_output_path: Module4è¾“å‡ºæ–‡ä»¶è·¯å¾„
        orchestrator: Orchestratorå®ä¾‹
        current_date: å½“å‰æ—¥æœŸ(å•æ—¥è¿è¡Œæ—¶)
    """
    # åˆ¤æ–­è¿è¡Œæ¨¡å¼
    if config_dict is not None:
        # é›†æˆæ¨¡å¼ - å®Œæˆçš„é›†æˆæ•°æ®åŠ è½½
        # print("\nğŸ”„ Module5 è¿è¡Œäºé›†æˆæ¨¡å¼")
        current_date_obj = pd.to_datetime(current_date) if current_date else None
        config = load_integrated_config(
            config_dict, module1_output_dir, module4_output_path, 
            orchestrator, current_date_obj
        )
        sim_dates = [current_date_obj] if current_date_obj else pd.date_range(sim_start, sim_end, freq='D')
        
        # é›†æˆæ¨¡å¼è¾“å‡ºè·¯å¾„
        if output_path is None:
            output_path = f"./Module5Output_{current_date_obj.strftime('%Y%m%d')}.xlsx" if current_date_obj else "./Module5Output.xlsx"
    else:
        # ç‹¬ç«‹æ¨¡å¼ - ä¿æŒå¾€åå…¼å®¹
        # print("\nğŸ“œ Module5 è¿è¡Œäºç‹¬ç«‹æ¨¡å¼") 
        config = load_config(input_path)
        sim_dates = pd.date_range(sim_start, sim_end, freq='D')
    
    # ç»§ç»­åŸæœ‰é€»è¾‘æµç¨‹
    validation_log = list(config.get('ValidationLog', []))
    validate_config_before_run(config, validation_log)

    network = config['Network']
    deploy_cfg = config['DeployConfig']
    inventory_log = config['InventoryLog']
    production_plan = config['ProductionPlan']
    in_transit = config['InTransit']
    demand_priority = config['DemandPriority']
    receiving_space = config['ReceivingSpace']

    network_layers = assign_location_layers(network)
    location_to_layer = dict(zip(network_layers['location'], network_layers['layer']))
    layer_list = sorted(network_layers['layer'].unique(), reverse=True)  # ä»æœ€å¤§å±‚å¾€ä¸Šæ¸¸æ¨è¿›
    # Performance optimization: Use dict() with zip instead of iterrows
    demand_priority_map = dict(zip(demand_priority['demand_element'], demand_priority['priority']))
    config['LocationLayerMap'] = location_to_layer
    
    # ========== é«˜ä¼˜å…ˆçº§æ€§èƒ½ä¼˜åŒ–: æ„å»ºæŸ¥è¯¢ç¼“å­˜ (15-25% æå‡) ==========
    # 1. PTF/LSK ç¼“å­˜ (15-20x faster)
    ptf_lsk_cache = _build_ptf_lsk_cache(config.get('M4_MaterialLocationLineCfg', pd.DataFrame()))
    # 2. Lead Time ç¼“å­˜ (10-15x faster)
    lead_time_cache = _build_lead_time_cache(config.get('LeadTime', pd.DataFrame()))
    # 3. Active Network ç¼“å­˜ (20-30x faster)
    active_network_cache = _build_active_network_cache(network)
    print(f"âœ… ç¼“å­˜å·²åˆå§‹åŒ–: PTF/LSK={len(ptf_lsk_cache)} | LeadTime={len(lead_time_cache)} | Network={len(active_network_cache)}")
    # ========== åˆå§‹åŒ–åº“å­˜ soh_dict ==========
    # 1. å…¨æ”¶é›†æ‰€æœ‰material/locationï¼ˆåŒ…å« OrderLogï¼‰
    ol_df = config.get('OrderLog', pd.DataFrame())
    mats_from_ol = set(ol_df['material'].unique()) if ('material' in ol_df.columns and not ol_df.empty) else set()
    locs_from_ol = set(ol_df['location'].unique()) if ('location' in ol_df.columns and not ol_df.empty) else set()

    all_mats = set(config['SupplyDemandLog']['material'].unique()) | \
            set(config['SafetyStock']['material'].unique()) | \
            mats_from_ol

    all_locs = set(config['SupplyDemandLog']['location'].unique()) | \
            set(config['SafetyStock']['location'].unique()) | \
            locs_from_ol

    # 2. ç¡®å®šä»¿çœŸå¼€å§‹æ—¥æœŸå¹¶è·å–å½“å¤©çš„åº“å­˜
    # é›†æˆæ¨¡å¼ä¸‹ä½¿ç”¨ç¬¬ä¸€ä¸ªä»¿çœŸæ—¥æœŸï¼Œç‹¬ç«‹æ¨¡å¼ä¸‹ä½¿ç”¨sim_startå‚æ•°
    actual_sim_start = (sim_dates[0] if hasattr(sim_dates, '__getitem__') else pd.to_datetime(sim_start))
    
    inv_df = inventory_log[inventory_log['date'] == actual_sim_start]
    if inv_df.empty:
        print(f"[WARN] No inventory records found for sim_start: {actual_sim_start}")

    # 3. æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤è®°å½•
    duplicates = inv_df.duplicated(subset=['material', 'location'], keep=False)
    if duplicates.any():
        dup_rows = inv_df[duplicates]
        raise ValueError(f"InventoryLog contains duplicate (material, location) on sim_start {sim_start}:\n{dup_rows[['material', 'location', 'date']]}")

    # 4. åˆå§‹åŒ–soh_dictï¼Œé»˜è®¤0
    # Performance optimization: Use dictionary comprehension instead of nested loops
    soh_dict = {(mat, loc): 0 for mat in all_mats for loc in all_locs}

    # Performance optimization: Use itertuples instead of iterrows
    for row in inv_df.itertuples():
        soh_dict[(row.material, row.location)] = int(row.quantity)


    deployment_plan_rows = []
    unfulfilled_rows = []
    stock_on_hand_log = []

    up_gap_buffer = {}

    for sim_date in sim_dates:
        # === ä¼˜åŒ–çš„æ—¥å¿—è¾“å‡º ===
        # print(f"\n{'='*60}")
        # print(f"ğŸ“… ä»¿çœŸæ—¥æœŸ: {sim_date.strftime('%Y-%m-%d')}")
        # print(f"{'='*60}")

        # ===== åº“å­˜è®¡ç®—é€»è¾‘é‡æ„ (ä¿®å¤é‡å¤è®¡ç®—é—®é¢˜) =====
        # ğŸ”„ æ–°çš„åº“å­˜è®¡ç®—å…¬å¼: åŸºäºæœŸåˆåº“å­˜é¿å…é‡å¤è®¡ç®—
        # available_inventory = 
        #   beginning_inventory +              # å½“æ—¥æœŸåˆåº“å­˜ï¼ˆæœªåŒ…å«å½“æ—¥äº‹åŠ¡ï¼‰
        #   in_transit +                      # åœ¨é€”åº“å­˜
        #   delivery_gr +                     # å½“æ—¥æ”¶è´§æ•°æ®  
        #   today_production +                # å½“æ—¥ç”Ÿäº§ (available_date = today)
        #   future_production +               # æœªæ¥ç”Ÿäº§ (available_date > today)
        #   - today_shipment -                # å½“æ—¥å‘è´§æ•°æ®
        #   - open_deployment                 # å¼€æ”¾è°ƒæ‹¨æ•°æ®
        
        # ä½¿ç”¨æœŸåˆåº“å­˜ä½œä¸ºåŸºç¡€ï¼Œé¿å…é‡å¤è®¡ç®—M1 shipmentå’ŒM4 production
        beginning_inventory = soh_dict.copy()
        
        # ä» Module4/Orchestrator è·å–å½“æ—¥å’Œæœªæ¥ç”Ÿäº§ï¼ˆæ¥æºè§ load_integrated_config çš„é…ç½®ï¼‰
        today_production_gr = {}
        future_production = {}
        if not production_plan.empty:
            # ğŸ” è°ƒè¯•ç”Ÿäº§è®¡åˆ’æ•°æ®
            # print(f"\nğŸ” è°ƒè¯•ç”Ÿäº§è®¡åˆ’æ•°æ®:")
            # print(f"   ç”Ÿäº§è®¡åˆ’æ€»æ¡ç›®: {len(production_plan)}")
            # print(f"   ç”Ÿäº§è®¡åˆ’åˆ—: {production_plan.columns.tolist()}")
            
            # æ£€æŸ¥æ‰€æœ‰å½“æ—¥ç”Ÿäº§è®¡åˆ’
            all_today = production_plan[production_plan['available_date'] == sim_date]
            # print(f"   æ‰€æœ‰å½“æ—¥ç”Ÿäº§è®¡åˆ’: {len(all_today)} æ¡")
            # for _, row in all_today.iterrows():
                # print(f"   - {row.get('material')}@{row.get('location')}: {row.get('quantity')}")
            
            # æŸ¥çœ‹å½“æ—¥çš„80813644@0386ç”Ÿäº§è®¡åˆ’
            # debug_today = production_plan[
            #     (production_plan['available_date'] == sim_date) & 
            #     (production_plan['material'] == '80813644') & 
            #     (production_plan['location'] == '0386')
            # ]
            # if not debug_today.empty:
            #     print(f"   å½“æ—¥80813644@0386ç”Ÿäº§è®¡åˆ’: {len(debug_today)} æ¡")
            #     for _, row in debug_today.iterrows():
            #         print(f"   - material: {row.get('material')}, location: {row.get('location')}")
            #         print(f"     produced_qty: {row.get('produced_qty')}, planned_qty: {row.get('planned_qty')}")
            #         print(f"     quantity: {row.get('quantity')}, available_date: {row.get('available_date')}")
                    
            # ğŸ” é‡è¦ï¼šå¯¹æ¯”å†å²ç”Ÿäº§å…¥åº“vsè®¡åˆ’ç”Ÿäº§
            if orchestrator:
                date_str = sim_date.strftime('%Y-%m-%d')
                # print(f"\nğŸ” å¯¹æ¯”å†å²ç”Ÿäº§å…¥åº“ vs è®¡åˆ’ç”Ÿäº§:")
                # è·å–å½“æ—¥å†å²ç”Ÿäº§GR
                # prod_gr_view = orchestrator.get_production_gr_view(date_str)
                # print(f"   å½“æ—¥å†å²ç”Ÿäº§GRæ¡ç›®: {len(prod_gr_view) if not prod_gr_view.empty else 0}")
                # if not prod_gr_view.empty:
                #     for _, row in prod_gr_view.iterrows():
                #         print(f"   - å†å²GR: {row.get('material')}@{row.get('location')}: {row.get('quantity')}")
                
                # # è·å–è®¡åˆ’ç”Ÿäº§backlog
                # if hasattr(orchestrator, 'production_plan_backlog'):
                #     backlog_today = [p for p in orchestrator.production_plan_backlog 
                #                    if pd.to_datetime(p.get('available_date')).normalize() == sim_date.normalize()]
                #     print(f"   å½“æ—¥è®¡åˆ’ç”Ÿäº§backlogæ¡ç›®: {len(backlog_today)}")
                #     for record in backlog_today:
                #         print(f"   - è®¡åˆ’backlog: {record.get('material')}@{record.get('location')}: {record.get('quantity')}")
                # else:
                #     print(f"   Orchestratoræ²¡æœ‰production_plan_backlogå±æ€§")
            
            # # å½“æ—¥ç”Ÿäº§ (available_date = sim_date) â€”â€” ç”¨ produced_qty
            today_prod = production_plan[production_plan['available_date'] == sim_date]
            # print(f"   å½“æ—¥ç”Ÿäº§æ¡ç›®: {len(today_prod)}")
            # Helper function to get first valid quantity from multiple columns
            def _get_qty_from_row(row, col_names):
                """Get first non-null, non-NaN value from list of column names"""
                for col in col_names:
                    val = getattr(row, col, None)
                    if val is not None and not pd.isna(val):
                        return int(val)
                return 0
            
            # Performance optimization: Use itertuples for faster iteration
            for row in today_prod.itertuples():
                k = (row.material, row.location)
                # Try columns in order: produced_qty -> planned_qty -> quantity
                qty_today = _get_qty_from_row(row, ['produced_qty', 'planned_qty', 'quantity'])
                today_production_gr[k] = today_production_gr.get(k, 0) + qty_today

            # æœªæ¥ç”Ÿäº§ (available_date > sim_date) â€”â€” ç”¨ uncon_planned_qty
            future_prod = production_plan[production_plan['available_date'] > sim_date]
            # Performance optimization: Use itertuples for faster iteration
            for row in future_prod.itertuples():
                k = (row.material, row.location)
                # Try columns in order: uncon_planned_qty -> produced_qty -> planned_qty -> quantity
                qty_future = _get_qty_from_row(row, ['uncon_planned_qty', 'produced_qty', 'planned_qty', 'quantity'])
                future_production[k] = future_production.get(k, 0) + qty_future
        
        # ä» Orchestrator è·å–åœ¨é€”åº“å­˜
        today_intransit = {}
        if not in_transit.empty:
            # Performance optimization: Combine filter and iteration
            for row in in_transit[in_transit['available_date'] == sim_date].itertuples():
                k = (row.material, row.receiving)
                today_intransit[k] = today_intransit.get(k, 0) + int(row.quantity)
        # æœªæ¥åœ¨é€”ï¼šavailable_date > sim_dateï¼Œç”¨äºè‡ªè¡¥è´§çš„ pipeline è¦†ç›–
        future_intransit = {}
        if not in_transit.empty:
            # Performance optimization: Combine filter and iteration
            for row in in_transit[in_transit['available_date'] > sim_date].itertuples():
                k = (row.material, row.receiving)
                future_intransit[k] = future_intransit.get(k, 0) + int(row.quantity)
        
        # åŠ è½½å½“æ—¥æ”¶è´§ã€å‘è´§å’Œå¼€æ”¾è°ƒæ‹¨æ•°æ®
        delivery_gr_data = config.get('DeliveryGR', pd.DataFrame())
        today_shipment_data = config.get('TodayShipment', pd.DataFrame())
        open_deployment_data = config.get('OpenDeployment', pd.DataFrame())
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        delivery_gr = {}
        if not delivery_gr_data.empty:
            filtered_delivery = delivery_gr_data[pd.to_datetime(delivery_gr_data['date']) == sim_date] if 'date' in delivery_gr_data.columns else delivery_gr_data
            # Performance optimization: Use itertuples for faster iteration
            for row in filtered_delivery.itertuples():
                k = (row.material, row.receiving)
                delivery_gr[k] = delivery_gr.get(k, 0) + int(row.quantity)
        
        today_shipment = {}
        if not today_shipment_data.empty:
            filtered_shipment = today_shipment_data[pd.to_datetime(today_shipment_data['date']) == sim_date] if 'date' in today_shipment_data.columns else today_shipment_data
            # Performance optimization: Use itertuples for faster iteration
            for row in filtered_shipment.itertuples():
                k = (row.material, row.location)
                today_shipment[k] = today_shipment.get(k, 0) + int(row.quantity)
        
        open_deployment = {}
        if not open_deployment_data.empty:
            # Performance optimization: Use itertuples for faster iteration
            for row in open_deployment_data.itertuples():
                # åªè®¡ç®—çœŸæ­£ä»è¯¥åœ°ç‚¹å‘å‡ºçš„è°ƒæ‹¨ï¼Œæ’é™¤è‡ªå¾ªç¯ï¼ˆsending=receivingï¼‰
                if row.sending != row.receiving:
                    k = (row.material, row.sending)
                    open_deployment[k] = open_deployment.get(k, 0) + int(row.quantity)
        # ğŸ” æ–°å¢ï¼šæ„é€  inbound è§†å›¾ (material, receiving) â†’ qty
        open_deployment_inbound = build_open_deployment_inbound(open_deployment_data)

        # è®¡ç®—é¢„æµ‹åº“å­˜ï¼ˆç”¨äºgapè®¡ç®—ï¼‰
        projected_soh = calculate_projected_inventory(
            beginning_inventory=beginning_inventory,
            in_transit=today_intransit, 
            delivery_gr=delivery_gr,
            today_production_gr=today_production_gr,
            future_production=future_production,
            today_shipment=today_shipment,
            open_deployment=open_deployment
        )
        
        # è®¡ç®—å½“æ—¥çœŸå®å¯ç”¨åº“å­˜ï¼ˆç”¨äºå®é™…åˆ†é…ï¼‰
        dynamic_soh = calculate_available_inventory(
            beginning_inventory=beginning_inventory,
            delivery_gr=delivery_gr,
            today_production_gr=today_production_gr,
            today_shipment=today_shipment,
            open_deployment=open_deployment,
            open_deployment_inbound=open_deployment_inbound
        )

        
        # print(f"ğŸ” åº“å­˜è®¡ç®—åŸºç¡€: æœŸåˆåº“å­˜ {len(beginning_inventory)} é¡¹, é¢„æµ‹åº“å­˜ {len([k for k, v in projected_soh.items() if v > 0])} é¡¹æœ‰åº“å­˜, å½“æ—¥å¯ç”¨åº“å­˜ {len([k for k, v in dynamic_soh.items() if v > 0])} é¡¹æœ‰åº“å­˜")
        
        # ğŸ” è°ƒè¯•ï¼šè¯¦ç»†åˆ†æ80813644@0386çš„åº“å­˜è®¡ç®—
        # debug_key = ('80813644', '0386')
        # if debug_key in beginning_inventory or debug_key in dynamic_soh:
        #     print(f"\nğŸ” è°ƒè¯•80813644@0386åº“å­˜è®¡ç®—:")
        #     print(f"   æœŸåˆåº“å­˜ (beginning_inventory): {beginning_inventory.get(debug_key, 0)}")
        #     print(f"   äº¤ä»˜å…¥åº“ (delivery_gr): {delivery_gr.get(debug_key, 0)}")
        #     print(f"   å½“æ—¥ç”Ÿäº§å…¥åº“ (today_production_gr): {today_production_gr.get(debug_key, 0)}")
        #     print(f"   å½“æ—¥å‘è´§å‡ºåº“ (today_shipment): {today_shipment.get(debug_key, 0)}")
        #     print(f"   å¼€æ”¾éƒ¨ç½²æ‰£å‡ (open_deployment): {open_deployment.get(debug_key, 0)}")
        #     calculated = (beginning_inventory.get(debug_key, 0) + 
        #                  delivery_gr.get(debug_key, 0) + 
        #                  today_production_gr.get(debug_key, 0) - 
        #                  today_shipment.get(debug_key, 0) - 
        #                  open_deployment.get(debug_key, 0))
        #     print(f"   è®¡ç®—ç»“æœ = {beginning_inventory.get(debug_key, 0)} + {delivery_gr.get(debug_key, 0)} + {today_production_gr.get(debug_key, 0)} - {today_shipment.get(debug_key, 0)} - {open_deployment.get(debug_key, 0)} = {calculated}")
        #     print(f"   dynamic_sohå®é™…å€¼: {dynamic_soh.get(debug_key, 0)}")
            
            # # ğŸ” è°ƒè¯•today_production_grçš„å…·ä½“æ¥æº
            # print(f"\nğŸ” è°ƒè¯•today_production_grçš„æ¥æº:")
            # print(f"   today_production_græ€»æ¡ç›®: {len(today_production_gr)}")
            # for key, qty in today_production_gr.items():
            #     if key[0] == '80813644' and key[1] == '0386':
            #         print(f"   å‘ç°80813644@0386çš„ç”Ÿäº§å…¥åº“: {qty}")
            
            # å¯¹æ¯”Orchestratorçš„unrestricted_inventory
            # if orchestrator:
            #     date_str = current_date.strftime('%Y-%m-%d') if hasattr(current_date, 'strftime') else str(current_date)
            #     orch_inventory = orchestrator.get_unrestricted_inventory_view(date_str)
            #     orch_row = orch_inventory[(orch_inventory['material'] == '80813644') & (orch_inventory['location'] == '0386')]
            #     if not orch_row.empty:
            #         orch_qty = orch_row.iloc[0]['quantity']
                    # print(f"   Orchestrator unrestricted_inventory: {orch_qty}")
                    # print(f"   å·®å¼‚: dynamic_soh({dynamic_soh.get(debug_key, 0)}) - unrestricted({orch_qty}) = {dynamic_soh.get(debug_key, 0) - orch_qty}")
                    
                    # ğŸ” è°ƒè¯•Orchestratorå½“æ—¥å†å²ç”Ÿäº§å…¥åº“è®°å½•
                    # print(f"\nğŸ” è°ƒè¯•Orchestratorå½“æ—¥å†å²ç”Ÿäº§å…¥åº“:")
                    # if hasattr(orchestrator, 'production_gr'):
                    #     prod_records = [p for p in orchestrator.production_gr if 
                    #                   p.get('date') == date_str and 
                    #                   p.get('material') == '80813644' and 
                    #                   p.get('location') == '0386']
                    #     print(f"   Orchestratorå½“æ—¥å†å²ç”Ÿäº§å…¥åº“è®°å½•æ•°: {len(prod_records)}")
                    #     total_orch_prod = sum(p.get('quantity', 0) for p in prod_records)
                    #     print(f"   Orchestratorå½“æ—¥å†å²ç”Ÿäº§å…¥åº“æ€»é‡: {total_orch_prod}")
                    #     for record in prod_records:
                    #         print(f"   - {record}")
                    # else:
                    #     print(f"   Orchestratoræ²¡æœ‰production_grå±æ€§")
        up_gap_next = {}

        for layer in layer_list:
            # print(f"\nğŸ“¦ å¤„ç†å±‚çº§ {layer}")
            # print(f"{'-'*40}")
            
            # ç»„åˆæ‰€æœ‰material-locationå¯¹ï¼ˆåŒ…å« OrderLogå’Œsafety stockï¼‰
            materials_union = set(config['SupplyDemandLog']['material'].unique())
            if 'OrderLog' in config and not config['OrderLog'].empty:
                materials_union |= set(config['OrderLog']['material'].unique())
            if not config['SafetyStock'].empty:
                materials_union |= set(config['SafetyStock']['material'].unique())
            base_pairs = set(
                (mat, loc)
                for loc, l in location_to_layer.items() if l == layer
                for mat in materials_union
            )
            # gap bufferè¡¥å……
            gap_pairs = set(
                (mat, loc)
                for (mat, loc) in up_gap_buffer
                if location_to_layer.get(loc, None) == layer
            )
            all_pairs = base_pairs | gap_pairs
            
            for mat, loc in all_pairs:
                node_key = (mat, loc)
                current_stock = dynamic_soh.get(node_key, 0)
                # print(f"ğŸ“ èŠ‚ç‚¹: {mat}@{loc} [å¯ç”¨åº“å­˜: {current_stock}]")
                
                demand_rows = collect_node_demands(mat, loc, sim_date, config, up_gap_buffer,
                                                   ptf_lsk_cache=ptf_lsk_cache,
                                                   lead_time_cache=lead_time_cache,
                                                   active_network_cache=active_network_cache)
                if not demand_rows:
                    # print(f"   âš ï¸  æ— éœ€æ±‚éœ€è¦å¤„ç†")
                    continue
                
                demand_types = [d['demand_element'] for d in demand_rows]
                # print(f"   ğŸ“‹ éœ€æ±‚ç±»å‹: {', '.join(demand_types)}")
                
                # ğŸ”§ ä¿®å¤ï¼šMOQ/RVåº”ç”¨é€»è¾‘ç§»è‡³è°ƒæ‹¨è®¡åˆ’ç”Ÿæˆé˜¶æ®µï¼Œæ ¹æ®å®é™…çš„sending/receivingå…³ç³»å†³å®š
                # æ­¤å¤„å…ˆå°†planned_qtyè®¾ä¸ºdemand_qtyï¼Œç¨ååœ¨ç”Ÿæˆplan_rowæ—¶å†å†³å®šæ˜¯å¦åº”ç”¨MOQ/RV
                for d in demand_rows:
                    d['planned_qty'] = d['demand_qty']  # æš‚æ—¶è®¾ä¸ºåŸå§‹éœ€æ±‚é‡

                # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„å¤„ç†
                demand_rows_sorted = sorted(demand_rows, key=lambda d: demand_priority_map.get(d['demand_element'], 99))
                grouped = {}
                for d in demand_rows_sorted:
                    p = demand_priority_map.get(d['demand_element'], 99)
                    grouped.setdefault(p, []).append(d)
                
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åˆ†ç»„MOQ/RVé€»è¾‘è®¡ç®—æ€»éœ€æ±‚é‡
                adjusted_qtys = apply_grouped_moq_rv(demand_rows, loc)
                total_actual_demand = sum(adjusted_qtys.values())
                # print(f"   ğŸ“Š æ€»éœ€æ±‚: {total_actual_demand}, å¯ç”¨åº“å­˜: {current_stock}")
                
                for priority in sorted(grouped):
                    group = grouped[priority]
                    # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆçº§ç»„éœ€æ±‚é‡åŸºäºåˆ†ç»„MOQ/RVè°ƒæ•´ç»“æœ
                    group_actual_demand = 0
                    for i, d in enumerate(demand_rows):
                        if d in group:
                            group_actual_demand += adjusted_qtys.get(i, d['demand_qty'])
                    # print(f"   ğŸ”¢ ä¼˜å…ˆçº§ {priority}: éœ€æ±‚ {group_actual_demand}")
                    
                    # å¦‚æœæ²¡æœ‰å‰©ä½™åº“å­˜ï¼Œæ‰€æœ‰åç»­ä¼˜å…ˆçº§éƒ½åˆ†é…0
                    if current_stock <= 0:
                        for d in group:
                            d['deployed_qty_invCon'] = 0
                        # print(f"      âŒ æ— å‰©ä½™åº“å­˜ï¼Œè·³è¿‡")
                        continue
                    
                    if group_actual_demand == 0:
                        for d in group:
                            d['deployed_qty_invCon'] = 0
                        continue
                    
                    if current_stock >= group_actual_demand:
                        # åº“å­˜å……è¶³ï¼Œå®Œå…¨æ»¡è¶³å½“å‰ä¼˜å…ˆçº§
                        for i, d in enumerate(demand_rows):
                            if d in group:
                                adjusted_qty = adjusted_qtys.get(i, d['demand_qty'])
                                d['deployed_qty_invCon'] = adjusted_qty
                        current_stock -= group_actual_demand
                        # print(f"      âœ… åº“å­˜å……è¶³ï¼Œå®Œå…¨æ»¡è¶³")
                    else:
                        # åº“å­˜ä¸è¶³ï¼ŒæŒ‰æƒé‡åˆ†é…æ‰€æœ‰å‰©ä½™åº“å­˜ç»™å½“å‰ä¼˜å…ˆçº§
                        # å…³é”®ä¿®å¤ï¼šç”¨å®Œåº“å­˜åï¼Œåç»­ä¼˜å…ˆçº§ä¸å†åˆ†é…
                        for i, d in enumerate(demand_rows):
                            if d in group:
                                adjusted_qty = adjusted_qtys.get(i, d['demand_qty'])
                                weight = adjusted_qty / group_actual_demand if group_actual_demand > 0 else 0
                                d['deployed_qty_invCon'] = min(int(current_stock * weight), adjusted_qty)
                        
                        # é‡æ–°è®¡ç®—å®é™…åˆ†é…é‡
                        actual_allocated = sum(d['deployed_qty_invCon'] for d in group)
                        current_stock = 0  # å…³é”®ä¿®å¤ï¼šåº“å­˜ä¸è¶³æ—¶ï¼Œç”¨å®Œæ‰€æœ‰åº“å­˜ï¼Œåç»­ä¼˜å…ˆçº§ä¸å†åˆ†é…
                        # print(f"      âš ï¸  åº“å­˜ä¸è¶³ï¼Œéƒ¨åˆ†æ»¡è¶³ {actual_allocated}/{group_actual_demand}ï¼Œåç»­ä¼˜å…ˆçº§ä¸å†åˆ†é…")
                        
                        # ä¸ºåç»­ä¼˜å…ˆçº§é¢„è®¾0åˆ†é…
                        remaining_priorities = [p for p in sorted(grouped) if p > priority]
                        for remaining_priority in remaining_priorities:
                            for d in grouped[remaining_priority]:
                                d['deployed_qty_invCon'] = 0
                        break  # è·³å‡ºä¼˜å…ˆçº§å¾ªç¯
                    
                    # æ˜¾ç¤ºåˆ†é…è¯¦æƒ…
                    for i, d in enumerate(demand_rows):
                        if d in group:
                            receiving = d.get('from_location', d.get('receiving', loc))
                            is_cross_node = (loc != receiving)
                            adjusted_qty = adjusted_qtys.get(i, d['demand_qty'])
                            # status = "âœ…" if d['deployed_qty_invCon'] == adjusted_qty else "âš ï¸"
                            # print(f"      {status} [{d['demand_element']}] åŸå§‹éœ€æ±‚={d['demand_qty']} è®¡åˆ’={adjusted_qty} åˆ†é…={d['deployed_qty_invCon']} è·¨èŠ‚ç‚¹={is_cross_node}")
                # â€”â€” åœ¨å¤„ç† GAP ä¹‹å‰ï¼Œç»™æ‰€æœ‰éœ€æ±‚è¡Œåˆå§‹åŒ– pipeline ç›¸å…³å­—æ®µ â€”â€” 
                for d in demand_rows:
                    d.setdefault('deploy_qty_with_plan_order', 0)
                    d.setdefault('deploy_from_in_transit', 0)
                    d.setdefault('deploy_from_open_deployment_inbound', 0)
                    d.setdefault('deploy_from_future_production', 0)

                # â€”â€” è‡ªè¡¥è´§ç¬¬äºŒè½®ï¼šç”¨ pipeline supply è¦†ç›–å‰©ä½™ gapï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½é€‚ç”¨ï¼‰â€”â€”

                # åªè€ƒè™‘æœ¬èŠ‚ç‚¹è‡ªè¡¥è´§è¡Œï¼šreceiving == æœ¬èŠ‚ç‚¹ loc
                self_idx_list = []

                for idx, d in enumerate(demand_rows):
                    receiving = d.get('from_location', d.get('receiving', loc))
                    if receiving == loc:
                        self_idx_list.append(idx)

                # å¦‚æœæ²¡æœ‰è‡ªè¡¥è´§éœ€æ±‚ï¼Œç›´æ¥è·³è¿‡ pipeline é€»è¾‘
                if self_idx_list:
                    # æ„é€ æœ¬èŠ‚ç‚¹è‡ªè¡¥å¯ç”¨çš„ pipeline æ± 
                    node_key = (mat, loc)
                    # âœ… æŒ‰ä½ çš„è¦æ±‚ï¼špipeline ç”¨ future intransitï¼Œä¸åŠ¨
                    pool_in_transit = future_intransit.get(node_key, 0)
                    pool_future_production = future_production.get(node_key, 0)
                    pool_odi = open_deployment_inbound.get(node_key, 0)

                    pipeline_pool_total = pool_in_transit + pool_odi + pool_future_production

                    if pipeline_pool_total > 0:
                        # æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼Œç”¨ pipeline è¦†ç›–è‡ªè¡¥è´§ gap
                        self_idx_sorted = sorted(
                            self_idx_list,
                            key=lambda i: demand_priority_map.get(demand_rows[i]['demand_element'], 99)
                        )

                        for idx in self_idx_sorted:
                            d = demand_rows[idx]
                            adjusted_qty = adjusted_qtys.get(idx, d['demand_qty'])
                            allocated_invcon = d.get('deployed_qty_invCon', 0)
                            raw_gap = adjusted_qty - allocated_invcon

                            if raw_gap <= 0:
                                continue
                            if pipeline_pool_total <= 0:
                                break

                            alloc = min(raw_gap, pipeline_pool_total)

                            # æŒ‰ in_transit -> open_deployment_inbound -> future_production é¡ºåºæ¶ˆè€—
                            alloc_intrans = min(alloc, pool_in_transit)
                            pool_in_transit -= alloc_intrans

                            remain1 = alloc - alloc_intrans
                            alloc_odi = min(remain1, pool_odi)
                            pool_odi -= alloc_odi

                            remain2 = remain1 - alloc_odi
                            alloc_future = min(remain2, pool_future_production)
                            pool_future_production -= alloc_future

                            pipeline_pool_total = pool_in_transit + pool_odi + pool_future_production

                            d['deploy_qty_with_plan_order'] += alloc
                            d['deploy_from_in_transit'] += alloc_intrans
                            d['deploy_from_open_deployment_inbound'] += alloc_odi
                            d['deploy_from_future_production'] += alloc_future


                # å¤„ç†GAPå’Œç”Ÿæˆè°ƒæ‹¨è®¡åˆ’
                gap_count = 0
                for i, d in enumerate(demand_rows):
                    receiving = d.get('from_location', d.get('receiving', loc))
                    is_cross_node = (loc != receiving)
                    adjusted_qty = adjusted_qtys.get(i, d['demand_qty'])

                    # æœ¬åœ°è‡ªè¡¥éœ€æ±‚ä½¿ç”¨ pipeline coverï¼ˆdeploy_qty_with_plan_orderï¼‰
                    is_self_node = (receiving == loc)
                    if is_self_node:
                        plan_order_cover = d.get('deploy_qty_with_plan_order', 0)
                    else:
                        plan_order_cover = 0

                    gap_qty = adjusted_qty - d.get('deployed_qty_invCon', 0) - plan_order_cover
                    if gap_qty <= 0:
                        continue

                    up_loc = get_upstream(loc, mat, network, sim_date, active_network_cache=active_network_cache)
                    gap_count += 1

                    if up_loc:
                        new_demand_element = f"net demand for {d['demand_element']}"
                        up_gap_next.setdefault((mat, up_loc), []).append({
                            'demand_element': new_demand_element,
                            'planned_qty': gap_qty,
                            'leadtime': d['leadtime'],
                            'requirement_date': d.get('requirement_date', d['plan_deploy_date']),
                            'location': up_loc,
                            'from_location': loc,
                            'orig_location': d.get('orig_location', d['location'])
                        })
                    
                    unfulfilled_rows.append({
                        'date': d['plan_deploy_date'],
                        'sending': loc,
                        'receiving': receiving,
                        'demand_qty': d['demand_qty'],
                        'demand_element': d['demand_element'],
                        'unfulfilled_qty': gap_qty,
                        'reason': "supply shortage"
                    })

                        
                        # print(f"      ğŸ”¼ éœ€æ±‚ç¼ºå£: {gap_qty} [{d['demand_element']}] â†’ ä¸Šæ¸¸ {up_loc} (is_cross_node: {is_cross_node}, adjusted_qty: {adjusted_qty})")
                
                # if gap_count == 0:
                    # print(f"      ğŸŸ¢ æ— éœ€æ±‚ç¼ºå£")
                
                # ç”Ÿæˆè°ƒæ‹¨è®¡åˆ’è¡Œ
                for i, d in enumerate(demand_rows):
                    receiving = d.get('from_location', d.get('receiving', loc))
                    
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨åˆ†ç»„MOQ/RVè°ƒæ•´åçš„æ•°é‡
                    is_cross_node = (loc != receiving)
                    actual_planned_qty = adjusted_qtys.get(i, d['demand_qty'])
                    
                    # è‡ªè¡¥è´§ï¼ˆsending == receivingï¼‰ä¸åº”æœ‰leadtime
                    if loc == receiving:
                        planned_delivery_date = d['plan_deploy_date']
                        leadtime_for_row = 0
                    else:
                        planned_delivery_date = d.get('requirement_date', d['plan_deploy_date'])
                        # å³æ—¶è®¡ç®—è¯¥è¡Œçš„ lead timeï¼šsending=loc, receiving=receiving
                        sending_location_type = get_sending_location_type(
                            material=str(mat),
                            sending=str(loc),
                            sim_date=sim_date,
                            network_df=network,
                            location_layer_map=config.get('LocationLayerMap', {})
                        )
                        lt_row, _ = determine_lead_time(
                            sending=str(loc),
                            receiving=str(receiving),
                            location_type=str(sending_location_type),
                            lead_time_df=config['LeadTime'],
                            m4_mlcfg_df=config.get('M4_MaterialLocationLineCfg', pd.DataFrame()),
                            material=str(mat),
                            lead_time_cache=lead_time_cache,
                            ptf_lsk_cache=ptf_lsk_cache
                        )
                        leadtime_for_row = int(lt_row)
                    
                    plan_row = {
                        'date': d['plan_deploy_date'],
                        'material': mat,
                        'sending': loc,
                        'receiving': receiving,
                        'demand_qty': d['demand_qty'],
                        'demand_element': d['demand_element'],
                        'planned_qty': actual_planned_qty,
                        'deployed_qty_invCon': d['deployed_qty_invCon'],
                        'deploy_qty_with_plan_order': d.get('deploy_qty_with_plan_order', 0),
                        'deploy_from_in_transit': d.get('deploy_from_in_transit', 0),
                        'deploy_from_open_deployment_inbound': d.get('deploy_from_open_deployment_inbound', 0),
                        'deploy_from_future_production': d.get('deploy_from_future_production', 0),
                        'planned_delivery_date': planned_delivery_date,
                        'orig_location': d.get('orig_location', d['location']),
                        'leadtime': leadtime_for_row,
                        'is_cross_node': is_cross_node,
                    }

                    deployment_plan_rows.append(plan_row)

            # print(f"\nâœ… å±‚çº§ {layer} å¤„ç†å®Œæˆï¼Œå‘ä¸Šæ¸¸ä¼ é€’ {sum(len(v) for v in up_gap_next.values())} ä¸ªéœ€æ±‚ç¼ºå£")

            # æ›´æ–°GAPç¼“å†²åŒº
            up_gap_buffer = up_gap_next.copy()
        
        # push/soft-pushå†åˆ†é…ï¼šç›´æ¥ç”¨ dynamic_soh
        dynamic_soh_for_push = dynamic_soh.copy()
        plan_push = push_softpush_allocation(deployment_plan_rows, config, dynamic_soh_for_push, sim_date,
                                              ptf_lsk_cache=ptf_lsk_cache, lead_time_cache=lead_time_cache)

        if plan_push:
            deployment_plan_rows.extend(plan_push)
            # print(f"\nğŸ”„ Push/Soft-push è¡¥è´§: ç”Ÿæˆ {len(plan_push)} æ¡è¡¥è´§è®¡åˆ’")

        # æ›´æ–°åº“å­˜ï¼ˆåŸºäºå½“æ—¥äº‹åŠ¡æµæ°´ï¼‰
        deployed_dict = {}
        df = pd.DataFrame(deployment_plan_rows)
        if not df.empty:
            today_rows = df[df['date'] == sim_date]
            for _, row in today_rows.iterrows():
                k = (row['material'], row['sending'])
                qty = row['deployed_qty_invCon'] if row['sending'] != row['receiving'] else 0
                deployed_dict[k] = deployed_dict.get(k, 0) + qty

        # æ›´æ–°soh_dictä¸ºä¸‹ä¸€æ—¥çš„æœŸåˆåº“å­˜
        all_keys = set(list(beginning_inventory.keys()) +
                       list(today_production_gr.keys()) +
                       list(today_intransit.keys()) +
                       list(deployed_dict.keys()) +
                       list(today_shipment.keys()) +
                       list(delivery_gr.keys()))
        
        for (mat, loc) in all_keys:
            beginning_soh = beginning_inventory.get((mat, loc), 0)
            prod = today_production_gr.get((mat, loc), 0)
            intrans = today_intransit.get((mat, loc), 0)
            deliv_gr = delivery_gr.get((mat, loc), 0)
            deployed = deployed_dict.get((mat, loc), 0)
            shipped = today_shipment.get((mat, loc), 0)
            
            # æœŸæœ«åº“å­˜è®¡ç®—ï¼šæœŸåˆ + ç”Ÿäº§ + åœ¨é€”åˆ°è´§ + æ”¶è´§ - å‘è´§ - è°ƒæ‹¨
            end_soh = beginning_soh + prod + intrans + deliv_gr - shipped - deployed
            soh_dict[(mat, loc)] = end_soh  # ä½œä¸ºä¸‹ä¸€æ—¥çš„æœŸåˆåº“å­˜
            
            stock_on_hand_log.append({
                'material': mat,
                'location': loc,
                'date': sim_date,
                'beginning_soh': beginning_soh,
                'production': prod,
                'in_transit': intrans,
                'delivery_gr': deliv_gr,
                'today_shipment': shipped,
                'deployed_qty': deployed,
                'ending_soh': end_soh
            })
        
        # print(f"\nğŸ“Š å½“æ—¥ç»Ÿè®¡:")
        # print(f"   æ€»è°ƒæ‹¨è®¡åˆ’æ•°: {len(deployment_plan_rows)}")
        # print(f"   æœªæ»¡è¶³éœ€æ±‚æ•°: {len([r for r in unfulfilled_rows if r['date'] == sim_date])}")

    # åº”ç”¨æ”¶è´§ç©ºé—´é…é¢
    deployment_plan_rows_df, unfulfilled_space = apply_receiving_space_quota(
        deployment_plan_rows, receiving_space, sim_date, demand_priority_map
    )
    unfulfilled_all = pd.DataFrame(unfulfilled_rows + unfulfilled_space)

    outputs = {
        'DeploymentPlan': deployment_plan_rows_df,
        'UnfulfilledLog': unfulfilled_all,
        'StockOnHandLog': pd.DataFrame(stock_on_hand_log),
        'Validation': pd.DataFrame(validation_log),
    }
    log_outputs(output_path, outputs)
    
    # é›†æˆæ¨¡å¼ï¼šå°†éƒ¨ç½²è®¡åˆ’å‘é€ç»™Orchestratorï¼ˆç”±ä¸»é›†æˆè„šæœ¬ç»Ÿä¸€å¤„ç†ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶æ³¨é‡Šæ‰ç›´æ¥è°ƒç”¨ï¼Œäº¤ç”±ä¸»é›†æˆè„šæœ¬ç»Ÿä¸€å¤„ç†ä»¥é¿å…é‡å¤
    # if config_dict is not None and orchestrator is not None and not deployment_plan_rows_df.empty:
    #     try:
    #         # è¿‡æ»¤å‡ºæœ‰å®é™…éƒ¨ç½²é‡çš„è®¡åˆ’
    #         valid_deployment = deployment_plan_rows_df[
    #             (deployment_plan_rows_df['deployed_qty_invCon'] > 0) & 
    #             (deployment_plan_rows_df['deployed_qty_invCon'].notna())
    #         ].copy()
    #         
    #         if not valid_deployment.empty:
    #             # é‡å‘½ååˆ—ä»¥åŒ¹é…orchestratoræœŸæœ›çš„æ ¼å¼
    #             orchestrator_deployment = valid_deployment.rename(columns={
    #                 'date': 'planned_deployment_date',
    #                 'deployed_qty_invCon': 'deployed_qty'
    #             })[['material', 'sending', 'receiving', 'planned_deployment_date', 'deployed_qty', 'demand_element']]
    #             
    #             orchestrator.process_module5_deployment(orchestrator_deployment, current_date)
    #             print(f"âœ… å·²å‘Orchestratorå‘é€ {len(orchestrator_deployment)} æ¡éƒ¨ç½²è®¡åˆ’")
    #         else:
    #             print(f"â„¹ï¸  æ— æœ‰æ•ˆéƒ¨ç½²è®¡åˆ’å‘é€ç»™Orchestrator")
    #     except Exception as e:
    #         print(f"âš ï¸  Orchestratoré›†æˆå¤±è´¥: {str(e)}")
    #         print(f"Error type: {type(e).__name__}")
    #         print(f"Deployment plan columns: {list(deployment_plan_rows_df.columns)}")
    #         print(f"Deployment plan shape: {deployment_plan_rows_df.shape}")
    #         if not deployment_plan_rows_df.empty:
    #             print(f"Sample row: {deployment_plan_rows_df.iloc[0].to_dict()}")
    #         import traceback
    #         traceback.print_exc()
    
    # print(f"\n{'='*60}")
    # print(f"ğŸ‰ ä»¿çœŸå®Œæˆ! æ‰€æœ‰å±‚çº§å·²å¤„ç†å®Œæ¯•")
    # print(f"ğŸ’¾ è°ƒæ‹¨è®¡åˆ’å·²ä¿å­˜è‡³: {output_path}")
    # print(f"ğŸ“ˆ æ€»è°ƒæ‹¨è®¡åˆ’æ•°: {len(deployment_plan_rows_df)}")
    # print(f"ğŸ“ æœªæ»¡è¶³éœ€æ±‚æ•°: {len(unfulfilled_all)}")
    # print(f"âœ… ä¿®å¤é‡å¤è®¡ç®—é—®é¢˜: ä½¿ç”¨æœŸåˆåº“å­˜ä½œä¸ºè®¡ç®—åŸºç¡€")
    # print(f"{'='*60}")
    
    # è¿”å›ç»“æœç”¨äºé›†æˆæ¨¡å¼
    return {
        'deployment_plan': deployment_plan_rows_df,
        'unfulfilled_log': unfulfilled_all,
        'stock_on_hand_log': pd.DataFrame(stock_on_hand_log),
        'validation_log': pd.DataFrame(validation_log),
        'statistics': {
            'deployment_count': len(deployment_plan_rows_df),
            'unfulfilled_count': len(unfulfilled_all),
            'processed_dates': len(sim_dates) if isinstance(sim_dates, list) else 1
        }
    }


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description='Module 5: Multi-echelon Deployment Planning')
    parser.add_argument('--input', required=True, help='Input config excel path')
    parser.add_argument('--output', required=True, help='Output excel path')
    parser.add_argument('--sim_start', required=True, help='Simulation start date, YYYY-MM-DD')
    parser.add_argument('--sim_end', required=True, help='Simulation end date, YYYY-MM-DD')
    args = parser.parse_args()
    main(args.input, args.output, args.sim_start, args.sim_end)
